{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a556508b-e829-46a4-8c25-4b843a0da80b",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbebdb2-6227-4728-becc-bdabc95ee7b4",
   "metadata": {},
   "source": [
    "# Clasificación binaria en redes neuronales\n",
    "\n",
    "Cuando hablamos de clasificación, muchas personas piensan inmediatamente en separar datos en dos grupos: sí o no, verdadero o falso, positivo o negativo. En efecto, la clasificación binaria consiste en asignar cada observación a una de dos clases posibles.\n",
    "\n",
    "Sin embargo, en deep learning, la clasificación no se reduce a “trazar una frontera”. Lo que realmente aprende el modelo es una función de decisión compleja que separa regiones del espacio de características de acuerdo con patrones estadísticos aprendidos a partir de los datos.\n",
    "\n",
    "La diferencia fundamental con la regresión no está en la arquitectura interna de la red, sino en el tipo de variable objetivo y en la interpretación probabilística de la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6310e-23c8-4b9d-8ef6-985e0e920cbe",
   "metadata": {},
   "source": [
    "## ¿Qué significa clasificar en una red neuronal?\n",
    "\n",
    "En un problema de clasificación binaria, el objetivo no es predecir un valor continuo, sino estimar la probabilidad de que una observación pertenezca a una de dos clases.\n",
    "\n",
    "Formalmente, la red aprende una función $$f(\\boldsymbol{x})=P(y=1|\\boldsymbol{x})$$\n",
    "\n",
    "donde el resultado está acotado entre 0 y 1.\n",
    "\n",
    "Esto se logra utilizando una función de activación **sigmoide en la capa de salida**, que transforma cualquier valor real en una probabilidad interpretable.\n",
    "\n",
    "Aunque el resultado final es un número entre 0 y 1, la complejidad real del modelo ocurre en las capas ocultas: allí se aprenden combinaciones no lineales de las variables de entrada que permiten separar regiones del espacio de características de forma altamente no lineal.\n",
    "\n",
    "## Capa de salida en clasificación binaria\n",
    "\n",
    "En clasificación binaria, la arquitectura de salida es prácticamente estándar:\n",
    "\n",
    "- Una sola neurona\n",
    "\n",
    "- Función de activación sigmoide\n",
    "\n",
    "La salida puede interpretarse como:\n",
    "\n",
    "- $\\hat{y}\\sim 1$ significa alta probabilidad de pertenecer a la clase positiva\n",
    "- $\\hat{y}\\sim 0$ significa alta probabilidad de pertenecer a la clase negativa\n",
    "\n",
    "El **umbral de decisión** puede ajustarse según el problema, por ejemplo, cuando el costo de falsos positivos y falsos negativos no es simétrico.\n",
    "\n",
    "## Función de pérdida en clasificación binaria\n",
    "\n",
    "En clasificación, la métrica central durante el entrenamiento no es una distancia, sino una función de verosimilitud.\n",
    "\n",
    "La función estándar es la entropía cruzada binaria (binary cross-entropy): $$\\mathcal{L}=-(y\\log(\\hat{y})+(1-y)\\log(1-\\hat{y}))$$\n",
    "\n",
    "Esta función penaliza fuertemente las predicciones seguras pero incorrectas, y está directamente alineada con la interpretación probabilística del modelo.\n",
    "\n",
    "Durante el entrenamiento, minimizar esta pérdida equivale a maximizar la verosimilitud de los datos observados bajo el modelo.\n",
    "\n",
    "## Regularización en clasificación binaria\n",
    "\n",
    "Al igual que en regresión, una red neuronal puede sobreajustar fácilmente los datos de entrenamiento, aprendiendo patrones espurios o ruido.\n",
    "\n",
    "Las estrategias de regularización más comunes son:\n",
    "\n",
    "- **Regularización L2:** Penaliza pesos grandes, favoreciendo fronteras de decisión más suaves y estables.\n",
    "\n",
    "- **Dropout:** Desactiva aleatoriamente neuronas durante el entrenamiento, obligando a la red a no depender de rutas específicas de activación.\n",
    "\n",
    "- **Early stopping:** Monitorea el desempeño en un conjunto de validación y detiene el entrenamiento cuando la mejora se estanca, evitando que la red memorice el conjunto de entrenamiento.\n",
    "\n",
    "En clasificación, estas técnicas son especialmente importantes porque pequeñas variaciones en la frontera de decisión pueden cambiar la clase asignada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f493d4-5eaa-4f10-911f-99134897d096",
   "metadata": {},
   "source": [
    "## Métricas de evaluación en clasificación binaria\n",
    "\n",
    "Aunque la función de pérdida guía el entrenamiento, la evaluación del modelo se basa en métricas interpretables:\n",
    "\n",
    "**Accuracy:** proporción de predicciones correctas.\n",
    "\n",
    "**Precision:** qué fracción de las predicciones positivas son correctas.\n",
    "\n",
    "**Recall:** qué fracción de los positivos reales fue detectada.\n",
    "\n",
    "**F1-score:** balance entre precisión y exhaustividad.\n",
    "\n",
    "La elección de la métrica depende del problema: no es lo mismo detectar fraude que filtrar spam o diagnosticar una enfermedad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0050c-26cc-4378-9b4d-3c16ab7db7a5",
   "metadata": {},
   "source": [
    "## Pseudocódigo para diseñar una red para regresión binaria\n",
    "\n",
    "```python\n",
    "# ============================================================\n",
    "# Diseñador de arquitectura para clasificación binaria\n",
    "# (red neuronal con regularización y control de complejidad)\n",
    "# ============================================================\n",
    "\n",
    "k     ← 10            # máximo de parámetros por dato\n",
    "c1    ← 2.0           # factor de expansión inicial\n",
    "r     ← 0.5           # tasa de reducción entre capas\n",
    "n_min ← 8             # tamaño mínimo de capa oculta\n",
    "L_max ← 4             # máximo de capas ocultas\n",
    "\n",
    "ALGORITHM DiseñarRedClasificacion(d, n0)\n",
    "\n",
    "    # 0. Límite adaptativo por tamaño de muestra\n",
    "    n_max ← MIN(1024, MAX(64, floor(0.25 * d)))\n",
    "\n",
    "    # 1. Verificación mínima de viabilidad\n",
    "    IF d < 2 * n0 THEN\n",
    "        RETURN \"Dataset demasiado pequeño para clasificación confiable\"\n",
    "    END IF\n",
    "\n",
    "    # 2. Presupuesto total de parámetros\n",
    "    P_max ← floor(k * d)\n",
    "\n",
    "    # 3. Primera capa oculta\n",
    "    n1_cap ← floor(P_max / (n0 + 1))\n",
    "    n1 ← MIN(floor(c1 * n0), n1_cap, n_max)\n",
    "\n",
    "    IF n1 < n_min THEN\n",
    "        RETURN \"Modelo demasiado pequeño para capturar patrones\"\n",
    "    END IF\n",
    "\n",
    "    capas ← [n1]\n",
    "\n",
    "    # 4. Construcción progresiva de capas\n",
    "    WHILE length(capas) < L_max DO\n",
    "\n",
    "        n_prev ← last(capas)\n",
    "        n_new ← floor(r * n_prev)\n",
    "\n",
    "        IF n_new < n_min THEN\n",
    "            BREAK\n",
    "        END IF\n",
    "\n",
    "        append n_new to capas\n",
    "\n",
    "    END WHILE\n",
    "\n",
    "    # 5. Cálculo de parámetros totales\n",
    "    P ← (n0 + 1) * capas[1]\n",
    "    FOR i FROM 2 TO length(capas) DO\n",
    "        P ← P + (capas[i-1] + 1) * capas[i]\n",
    "    END FOR\n",
    "    P ← P + (last(capas) + 1) * 1   # salida binaria\n",
    "\n",
    "    # 6. Regularización adaptativa\n",
    "    rho ← P / P_max\n",
    "\n",
    "    IF rho >= 0.8 THEN\n",
    "        l2 ← 3e-4\n",
    "        dropout ← 0.4\n",
    "    ELSE IF rho >= 0.4 THEN\n",
    "        l2 ← 1e-4\n",
    "        dropout ← 0.25\n",
    "    ELSE\n",
    "        l2 ← 5e-5\n",
    "        dropout ← 0.15\n",
    "    END IF\n",
    "\n",
    "    # 7. Early stopping\n",
    "    patience ← IF d < 2000 THEN 20 ELSE 10\n",
    "    max_epochs ← IF d < 2000 THEN 300 ELSE 150\n",
    "\n",
    "    RETURN (capas, l2, dropout, patience, max_epochs)\n",
    "\n",
    "END ALGORITHM\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
