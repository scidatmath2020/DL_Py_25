{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599bfdef-375a-4180-887d-f8b4c71d4005",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba68cb-6575-40e2-ab83-48fef75a7005",
   "metadata": {},
   "source": [
    "# Funciones de pérdida\n",
    "\n",
    "Como ya hemos comentado, dependiendo del tipo de problema que se nos presente tendremos una función de activación específica en la capa de salida. Pero eso no es suficiente: una vez que la red produce una salida $\\boldsymbol{\\hat{y}}$ necesitamos cuantificar qué tan equivocada está respecto a la salida real $\\boldsymbol y$.$\n",
    "\n",
    "Esa cuantificación la realiza la función de pérdida $\\mathcal{L}(\\boldsymbol{y},\\boldsymbol{\\hat{y}})$, que mide el error cometido por la red en una predicción.\n",
    "\n",
    "Mientras que la función de activación de salida traduce números internos a algo interpretable (probabilidades, clases, valores reales), la función de pérdida traduce la diferencia entre lo predicho y lo real en un número que la red pueda minimizar durante el entrenamiento.\n",
    "\n",
    "De nuevo, la elección no es arbitraria: la función de pérdida está dictada por el tipo de problema y por la activación usada en la capa de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f3690e-24be-46e0-b723-424afa0284cc",
   "metadata": {},
   "source": [
    "## Regresión\n",
    "\n",
    "En problemas de regresión, la salida es un número real sin restricciones. No estamos hablando de probabilidades ni de clases, sino de valores continuos.\n",
    "\n",
    "La función de pérdida más común es el error cuadrático medio (MSE): $$\\mathcal{L}(y,\\hat{y})=(y-\\hat{y})^2$$ o bien su versión promedio cuando tenemos muchos datos.\n",
    "\n",
    "Esta pérdida penaliza fuertemente los errores grandes y es coherente con una salida lineal $f_{out}(t)=t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5e8ba-76e0-48ab-86d6-0001ab2a5478",
   "metadata": {},
   "source": [
    "## Clasificación binaria\n",
    "\n",
    "Aquí la salida de la red es una probabilidad $$\\hat{y}=P(y=1|\\boldsymbol{x})$$ obtenida típicamente mediante una sigmoide.\n",
    "\n",
    "En este contexto, usar una pérdida cuadrática es conceptualmente incorrecto: no estamos midiendo distancias entre números reales, sino discrepancias entre probabilidades y eventos binarios.\n",
    "\n",
    "La función adecuada es la entropía cruzada binaria:\n",
    "$$\\mathcal{L}(y,\\hat{y})=-(y\\log(\\hat{y})+(1-y)\\log(1-\\hat{y}))$$\n",
    "\n",
    "Esta función castiga con mucha severidad las predicciones “seguras pero equivocadas”, lo cual es exactamente lo que queremos en clasificación.\n",
    "\n",
    "### ¿Qué significa que la pérdida “castiga fuertemente”?\n",
    "\n",
    "Cuando decimos que la función de pérdida en clasificación binaria *castiga fuertemente*, **no hablamos de fórmulas**, sino del comportamiento lógico del error.\n",
    "\n",
    "En clasificación binaria, la red **no solo decide**, también **expresa qué tan segura está** de su decisión. La entropía cruzada binaria no solo pregunta:\n",
    "\n",
    "> “¿Te equivocaste o no?”\n",
    "\n",
    "sino también:\n",
    "\n",
    "> “¿Qué tan convencido estabas de tu respuesta?”\n",
    "\n",
    "---\n",
    "\n",
    "#### Caso 1: la red se equivoca, pero duda\n",
    "\n",
    "El valor real es $y=1$ (por ejemplo, el paciente **sí** tiene la enfermedad). La red dice algo como:\n",
    "> “Creo que sí, pero no estoy muy seguro”.\n",
    "\n",
    "Es decir, asigna una probabilidad intermedia a $y=1$. Aquí la pérdida **aumenta**, pero de forma moderada. La red se equivocó, pero **no estaba convencida** de su error.\n",
    "\n",
    "---\n",
    "\n",
    "#### Caso 2: la red se equivoca y está muy segura\n",
    "\n",
    "El valor real es $y=1$. La red dice:\n",
    "> “Estoy casi seguro de que **no**”.\n",
    "\n",
    "Aquí el problema es mayor:\n",
    "- La predicción es incorrecta.\n",
    "- Además, la red estaba **muy confiada** en esa respuesta falsa.\n",
    "\n",
    "En este caso la función de pérdida **crece mucho** y se penaliza severamente el error seguro.\n",
    "\n",
    "---\n",
    "\n",
    "#### Caso 3: la red acierta, pero con poca seguridad\n",
    "\n",
    "El valor real es $y=1$. La red dice:\n",
    "> “Creo que sí, pero apenas”.\n",
    "\n",
    "La pérdida es pequeña, pero **no cero**. La red acertó, pero todavía debe aprender a estar más segura.\n",
    "\n",
    "---\n",
    "\n",
    "#### Caso 4: la red acierta y está muy segura\n",
    "\n",
    "El valor real es $y=1$. La red dice:\n",
    "> “Estoy completamente seguro de que sí”.\n",
    "\n",
    "Aquí la pérdida es **casi nula**.  No hay nada relevante que corregir.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e25cdf-9cc8-4e6c-a2dd-9a62e03d1f30",
   "metadata": {},
   "source": [
    "## Clasificación multiclase (una sola clase correcta)\n",
    "\n",
    "En este caso la red produce un **vector de probabilidades**, una por clase, mediante una softmax:\n",
    "\n",
    "$$\n",
    "\\hat{y}_j = \\frac{e^{z_{out}^{(j)}}}{\\sum_k e^{z_{out}^{(k)}}}.\n",
    "$$\n",
    "\n",
    "La función de pérdida adecuada es la **entropía cruzada categórica**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{y},\\boldsymbol{\\hat{y}}) = -\\sum_j y_j \\log(\\hat{y}_j),\n",
    "$$\n",
    "\n",
    "donde $\\boldsymbol{y}$ es un vector one-hot que indica la clase correcta.\n",
    "\n",
    "Esta pérdida mide qué tan bien la red asigna probabilidad a la clase verdadera frente a las demás, reflejando la competencia implícita del softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d3178-0b67-432d-b74c-aa71099f97df",
   "metadata": {},
   "source": [
    "## Clasificación multietiqueta\n",
    "\n",
    "Aquí no hay competencia entre salidas: cada neurona responde de forma independiente si cierta etiqueta está presente o no.\n",
    "\n",
    "Por ello, aunque hay varias neuronas de salida, **cada una usa una sigmoide**, y la pérdida es la suma de entropías cruzadas binarias:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = -\\sum_j \\left( y_j \\log(\\hat{y}_j) + (1 - y_j) \\log(1 - \\hat{y}_j) \\right).\n",
    "$$\n",
    "\n",
    "Cada etiqueta se evalúa como un problema binario independiente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
