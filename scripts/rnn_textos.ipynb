{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwct8IiS_UW0"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# NOTEBOOK: CLASIFICACI√ìN DE TEXTO (RNN) ‚Äî robusto estilo tu CNN\n",
        "# ENTRADA:\n",
        "#   - EXACTAMENTE 1 ZIP en /content (al inicio)\n",
        "#   - Estructuras soportadas dentro del ZIP:\n",
        "#       (A) Carpetas por clase con .txt:\n",
        "#             /\n",
        "#               clase_0/  *.txt\n",
        "#               clase_1/  *.txt\n",
        "#               ...\n",
        "#       (B) Un CSV con texto+label (autodetecta columnas):\n",
        "#             - 1 o 2 columnas de texto (p.ej. title + description)\n",
        "#             - 1 columna label (string o num√©rica)\n",
        "# SALIDAS:\n",
        "#   - /content/resultados_texto.zip:\n",
        "#       model.keras\n",
        "#       weights.best.keras (si existe)\n",
        "#       metadata.json\n",
        "#       vocab.txt\n",
        "#   - descarga autom√°tica del zip (Colab)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CELDA 0 ‚Äî CONFIG GLOBAL + ZIP ‚Üí WORKDIR + autodetecci√≥n DATA MODE (texto)\n",
        "# =========================\n",
        "import os, glob, zipfile, shutil, random, time, json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "WORKDIR = \"/content/dataset_text\"\n",
        "CLEAN_WORKDIR = True\n",
        "\n",
        "SEED = 123\n",
        "TRAIN_FRAC = 0.70\n",
        "VAL_FRAC   = 0.15\n",
        "TEST_FRAC  = 0.15\n",
        "\n",
        "# -------- GPU T4 ----------\n",
        "USE_MIXED_PRECISION = True\n",
        "if USE_MIXED_PRECISION:\n",
        "    try:\n",
        "        from tensorflow.keras import mixed_precision\n",
        "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "        print(\"Mixed precision activada:\", mixed_precision.global_policy())\n",
        "    except Exception as e:\n",
        "        print(\"No se pudo activar mixed precision:\", e)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# -------- EXACTAMENTE 1 ZIP al inicio ----------\n",
        "zips = sorted(glob.glob(\"/content/*.zip\"), key=os.path.getmtime)\n",
        "assert len(zips) == 1, (\n",
        "    f\"Al INICIO se esperaba EXACTAMENTE 1 ZIP (el de datos) en /content, \"\n",
        "    f\"pero encontr√© {len(zips)}.\\n\"\n",
        "    \"ZIPs encontrados:\\n\" + \"\\n\".join([f\" - {os.path.basename(z)}\" for z in zips]) + \"\\n\\n\"\n",
        "    \"Deja SOLO el ZIP de datos antes de correr la Celda 0.\"\n",
        ")\n",
        "zip_name = zips[0]\n",
        "ZIP_DATOS_BASENAME = os.path.basename(zip_name)\n",
        "\n",
        "print(\"ZIP detectado (√∫nico al inicio):\", ZIP_DATOS_BASENAME)\n",
        "print(\"√öltima modificaci√≥n:\", time.ctime(os.path.getmtime(zip_name)))\n",
        "\n",
        "# -------- descomprimir ----------\n",
        "if CLEAN_WORKDIR and os.path.isdir(WORKDIR):\n",
        "    shutil.rmtree(WORKDIR)\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_name, \"r\") as z:\n",
        "    z.extractall(WORKDIR)\n",
        "\n",
        "print(\"Dataset extra√≠do en:\", WORKDIR)\n",
        "!ls -lah \"{WORKDIR}\"\n",
        "\n",
        "# -------- autodetecci√≥n modo texto ----------\n",
        "import pandas as pd\n",
        "\n",
        "MODE = None\n",
        "DATA_DIR = None\n",
        "CSV_TEXT_PATH = None\n",
        "TEXT_COLS = None\n",
        "LABEL_COL = None\n",
        "\n",
        "def walk_files(root):\n",
        "    for dirpath, _, filenames in os.walk(root):\n",
        "        for fn in filenames:\n",
        "            yield os.path.join(dirpath, fn)\n",
        "\n",
        "def find_text_folders_root(workdir):\n",
        "    candidates = [workdir]\n",
        "    candidates += [os.path.join(workdir, d) for d in os.listdir(workdir) if os.path.isdir(os.path.join(workdir, d))]\n",
        "\n",
        "    def score_dir(d):\n",
        "        if not os.path.isdir(d):\n",
        "            return (-1, -1, -1)\n",
        "        subdirs = [os.path.join(d, s) for s in os.listdir(d) if os.path.isdir(os.path.join(d, s))]\n",
        "        if len(subdirs) < 2:\n",
        "            return (-1, -1, -1)\n",
        "        good, total = 0, 0\n",
        "        for sd in subdirs:\n",
        "            n = len(glob.glob(os.path.join(sd, \"*.txt\")))\n",
        "            if n > 0:\n",
        "                good += 1\n",
        "                total += n\n",
        "        return (good, total, len(subdirs))\n",
        "\n",
        "    best, best_sc = None, (-1, -1, -1)\n",
        "    for c in candidates:\n",
        "        sc = score_dir(c)\n",
        "        if sc > best_sc:\n",
        "            best_sc = sc\n",
        "            best = c\n",
        "\n",
        "    if best is None or best_sc[0] < 2 or best_sc[1] == 0:\n",
        "        return None, best_sc\n",
        "    return best, best_sc\n",
        "\n",
        "def find_csv_files(workdir):\n",
        "    return sorted([p for p in walk_files(workdir) if p.lower().endswith(\".csv\")])\n",
        "\n",
        "def detect_text_label_columns_textonly(df):\n",
        "    \"\"\"\n",
        "    Texto:\n",
        "      - 1 o 2 columnas de texto (object/string) con longitud promedio >=5\n",
        "    Label:\n",
        "      - pocos √∫nicos (categor√≠a real), puede ser num√©rica o string\n",
        "    \"\"\"\n",
        "    df = df.dropna(axis=1, how=\"all\")\n",
        "    n = int(df.shape[0])\n",
        "    if df.shape[1] < 2 or n < 20:\n",
        "        return None\n",
        "\n",
        "    text_cands = [c for c in df.columns if (df[c].dtype == \"object\" or str(df[c].dtype).startswith(\"string\"))]\n",
        "    if len(text_cands) == 0:\n",
        "        return None\n",
        "\n",
        "    def text_score(col):\n",
        "        s = df[col].dropna().astype(str)\n",
        "        if len(s) == 0:\n",
        "            return -1.0\n",
        "        L = s.str.len().clip(0, 10000)\n",
        "        return float(L.mean())\n",
        "\n",
        "    text_cands = sorted(text_cands, key=text_score, reverse=True)\n",
        "    if text_score(text_cands[0]) < 5:\n",
        "        return None\n",
        "\n",
        "    def label_ok(col, text_cols):\n",
        "        if col in text_cols:\n",
        "            return False\n",
        "        s = df[col].dropna()\n",
        "        if len(s) == 0:\n",
        "            return False\n",
        "        u = int(s.astype(str).nunique())\n",
        "        if u < 2:\n",
        "            return False\n",
        "        # categor√≠a: pocos √∫nicos\n",
        "        if u > 100:\n",
        "            return False\n",
        "        if u > max(50, int(0.01 * n)):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    # 1 texto\n",
        "    t1 = [text_cands[0]]\n",
        "    label_cands = [c for c in df.columns if label_ok(c, t1)]\n",
        "    if len(label_cands) > 0:\n",
        "        lcol = min(label_cands, key=lambda c: df[c].dropna().astype(str).nunique())\n",
        "        return (t1, lcol)\n",
        "\n",
        "    # 2 textos\n",
        "    if len(text_cands) >= 2:\n",
        "        t2 = [text_cands[0], text_cands[1]]\n",
        "        label_cands = [c for c in df.columns if label_ok(c, t2)]\n",
        "        if len(label_cands) > 0:\n",
        "            lcol = min(label_cands, key=lambda c: df[c].dropna().astype(str).nunique())\n",
        "            return (t2, lcol)\n",
        "\n",
        "    return None\n",
        "\n",
        "TEXT_ROOT, text_sc = find_text_folders_root(WORKDIR)\n",
        "CSV_FILES = find_csv_files(WORKDIR)\n",
        "\n",
        "if TEXT_ROOT is not None:\n",
        "    MODE = \"folders_txt\"\n",
        "    DATA_DIR = TEXT_ROOT\n",
        "else:\n",
        "    for c in CSV_FILES:\n",
        "        try:\n",
        "            df0 = pd.read_csv(c)\n",
        "            out = detect_text_label_columns_textonly(df0)\n",
        "            if out is None:\n",
        "                continue\n",
        "            TEXT_COLS, LABEL_COL = out\n",
        "            MODE = \"csv_text\"\n",
        "            CSV_TEXT_PATH = c\n",
        "            break\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "if MODE is None:\n",
        "    raise ValueError(\n",
        "        \"No detect√© dataset de texto.\\n\"\n",
        "        \"- O bien carpetas por clase con .txt\\n\"\n",
        "        \"- O bien CSV con columnas texto+label\\n\"\n",
        "        f\"WORKDIR={WORKDIR}\"\n",
        "    )\n",
        "\n",
        "print(\"\\nCONFIG FINAL (TEXTO):\")\n",
        "print(\"  WORKDIR:\", WORKDIR)\n",
        "print(\"  MODE   :\", MODE)\n",
        "if MODE == \"folders_txt\":\n",
        "    print(\"  DATA_DIR:\", DATA_DIR)\n",
        "    print(\"  score (folders_con_txt, total_txt, subcarpetas):\", text_sc)\n",
        "else:\n",
        "    print(\"  CSV_TEXT_PATH:\", CSV_TEXT_PATH)\n",
        "    print(\"  TEXT_COLS:\", TEXT_COLS)\n",
        "    print(\"  LABEL_COL:\", LABEL_COL)\n",
        "print(\"  GPU:\", tf.config.list_physical_devices(\"GPU\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 1 ‚Äî CARGA + SPLIT + DESBALANCE (estilo tu CNN)\n",
        "# ==========================================================\n",
        "import numpy as np\n",
        "\n",
        "def stratified_split(labels, train_frac, val_frac, test_frac, seed=123):\n",
        "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-9\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(len(labels))\n",
        "\n",
        "    train_idx, val_idx, test_idx = [], [], []\n",
        "    for c in np.unique(labels):\n",
        "        c_idx = idx[labels == c]\n",
        "        rng.shuffle(c_idx)\n",
        "        n = len(c_idx)\n",
        "        if n == 0:\n",
        "            continue\n",
        "\n",
        "        n_train = int(round(n * train_frac))\n",
        "        n_val   = int(round(n * val_frac))\n",
        "\n",
        "        n_train = max(1, min(n_train, n))\n",
        "        n_val = min(n_val, n - n_train)\n",
        "\n",
        "        train_idx.extend(c_idx[:n_train])\n",
        "        val_idx.extend(c_idx[n_train:n_train+n_val])\n",
        "        test_idx.extend(c_idx[n_train+n_val:])\n",
        "\n",
        "    rng.shuffle(train_idx); rng.shuffle(val_idx); rng.shuffle(test_idx)\n",
        "    return np.array(train_idx), np.array(val_idx), np.array(test_idx)\n",
        "\n",
        "def compute_class_weight(train_labels, num_classes):\n",
        "    counts = np.bincount(train_labels, minlength=num_classes).astype(np.int64)\n",
        "    N = counts.sum()\n",
        "    weights = {}\n",
        "    for c in range(num_classes):\n",
        "        weights[c] = 0.0 if counts[c] == 0 else float(N) / float(num_classes * counts[c])\n",
        "    return counts, weights\n",
        "\n",
        "def bincountK(y, K):\n",
        "    return np.bincount(y, minlength=K)\n",
        "\n",
        "# -------- modo folders_txt ----------\n",
        "if MODE == \"folders_txt\":\n",
        "    import os, glob\n",
        "\n",
        "    classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
        "    if len(classes) < 2:\n",
        "        raise ValueError(\"Se requieren >=2 clases.\")\n",
        "\n",
        "    files, labels = [], []\n",
        "    per_class = []\n",
        "    for i, cls in enumerate(classes):\n",
        "        cls_dir = os.path.join(DATA_DIR, cls)\n",
        "        cls_files = sorted(glob.glob(os.path.join(cls_dir, \"*.txt\")))\n",
        "        per_class.append((cls, len(cls_files)))\n",
        "        files.extend(cls_files)\n",
        "        labels.extend([i]*len(cls_files))\n",
        "\n",
        "    all_files = np.array(files)\n",
        "    all_labels = np.array(labels, dtype=np.int32)\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    train_idx, val_idx, test_idx = stratified_split(all_labels, TRAIN_FRAC, VAL_FRAC, TEST_FRAC, SEED)\n",
        "\n",
        "    print(\"\\n[Texto por carpetas]\")\n",
        "    print(\"DATA_DIR:\", DATA_DIR)\n",
        "    print(\"Num clases:\", num_classes)\n",
        "    print(\"Total ejemplos:\", len(all_files))\n",
        "    print(\"\\nConteo por clase (primeras 20):\")\n",
        "    for cls, n in per_class[:20]:\n",
        "        print(f\"  {cls:<30s} {n}\")\n",
        "    if len(per_class) > 20:\n",
        "        print(\"  ...\")\n",
        "\n",
        "# -------- modo csv_text ----------\n",
        "else:\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(CSV_TEXT_PATH)\n",
        "\n",
        "    use_cols = list(TEXT_COLS) + [LABEL_COL]\n",
        "    df = df[use_cols].dropna()\n",
        "\n",
        "    if len(TEXT_COLS) == 1:\n",
        "        all_texts = df[TEXT_COLS[0]].astype(str).values\n",
        "    else:\n",
        "        all_texts = (df[TEXT_COLS[0]].astype(str) + \" \" + df[TEXT_COLS[1]].astype(str)).values\n",
        "\n",
        "    labels_raw = df[LABEL_COL].astype(str).values\n",
        "    classes = sorted(list(set(labels_raw.tolist())))\n",
        "    class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "    all_labels = np.array([class_to_idx[c] for c in labels_raw], dtype=np.int32)\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    # \"archivos\" dummy para que el resto sea uniforme\n",
        "    all_files = np.array(all_texts, dtype=object)\n",
        "\n",
        "    train_idx, val_idx, test_idx = stratified_split(all_labels, TRAIN_FRAC, VAL_FRAC, TEST_FRAC, SEED)\n",
        "\n",
        "    global_counts = np.bincount(all_labels, minlength=num_classes).astype(int)\n",
        "    per_class = [(classes[i], int(global_counts[i])) for i in range(num_classes)]\n",
        "\n",
        "    print(\"\\n[Texto en CSV]\")\n",
        "    print(\"CSV_TEXT_PATH:\", CSV_TEXT_PATH)\n",
        "    print(\"TEXT_COLS:\", TEXT_COLS, \"| LABEL_COL:\", LABEL_COL)\n",
        "    print(\"Num clases:\", num_classes)\n",
        "    print(\"Total ejemplos:\", len(all_files))\n",
        "    print(\"\\nConteo por clase (primeras 20):\")\n",
        "    for cls, n in per_class[:20]:\n",
        "        print(f\"  {cls:<30s} {n}\")\n",
        "    if len(per_class) > 20:\n",
        "        print(\"  ...\")\n",
        "\n",
        "# -------- validaci√≥n fuerte + desbalance estilo CNN ----------\n",
        "print(\"\\nSplit tama√±os:\", \"train\", len(train_idx), \"| val\", len(val_idx), \"| test\", len(test_idx))\n",
        "if len(train_idx) == 0 or len(val_idx) == 0 or len(test_idx) == 0:\n",
        "    raise ValueError(\"Alguno de los splits qued√≥ vac√≠o.\")\n",
        "\n",
        "train_labels = all_labels[train_idx]\n",
        "class_counts, class_weight = compute_class_weight(train_labels, num_classes)\n",
        "\n",
        "min_count = int(class_counts.min()) if len(class_counts) else 0\n",
        "max_count = int(class_counts.max()) if len(class_counts) else 0\n",
        "imbalance_ratio = (max_count / min_count) if (min_count > 0) else float(\"inf\")\n",
        "\n",
        "IMBALANCED = (imbalance_ratio >= 2.0) or (min_count <= 10)\n",
        "\n",
        "TINY_CLASS_THRESHOLD = 5\n",
        "RARE_CLASS_THRESHOLD = 10\n",
        "\n",
        "tiny_idx = np.where(class_counts <= TINY_CLASS_THRESHOLD)[0]\n",
        "rare_idx = np.where((class_counts > TINY_CLASS_THRESHOLD) & (class_counts <= RARE_CLASS_THRESHOLD))[0]\n",
        "zero_idx = np.where(class_counts == 0)[0]\n",
        "\n",
        "HAS_TINY_CLASSES = len(tiny_idx) > 0\n",
        "HAS_RARE_CLASSES = len(rare_idx) > 0\n",
        "\n",
        "USE_CLASS_WEIGHT = IMBALANCED or HAS_TINY_CLASSES or HAS_RARE_CLASSES\n",
        "MONITOR_METRIC = \"val_loss\" if (IMBALANCED or HAS_TINY_CLASSES) else \"val_accuracy\"\n",
        "\n",
        "print(\"\\nDistribuci√≥n TRAIN: min\", min_count, \"| max\", max_count, \"| ratio\", imbalance_ratio)\n",
        "print(\"IMBALANCED:\", IMBALANCED)\n",
        "print(\"HAS_TINY_CLASSES:\", HAS_TINY_CLASSES, \"| HAS_RARE_CLASSES:\", HAS_RARE_CLASSES)\n",
        "print(\"USE_CLASS_WEIGHT:\", USE_CLASS_WEIGHT)\n",
        "print(\"MONITOR_METRIC:\", MONITOR_METRIC)\n",
        "\n",
        "print(\"\\n=== SANITY CHECK SPLITS ===\")\n",
        "print(\"Labels min/max:\", int(all_labels.min()), int(all_labels.max()))\n",
        "print(\"Num clases declarado:\", num_classes)\n",
        "print(\"Train per class:\", bincountK(all_labels[train_idx], num_classes).tolist())\n",
        "print(\"Val   per class:\", bincountK(all_labels[val_idx],   num_classes).tolist())\n",
        "print(\"Test  per class:\", bincountK(all_labels[test_idx],  num_classes).tolist())\n"
      ],
      "metadata": {
        "id": "VEKM6EeeAlGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 2 ‚Äî PIPELINE tf.data (texto) + TextVectorization.adapt(train)\n",
        "# ==========================================================\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "MAX_TOKENS = 20000\n",
        "SEQ_LEN = 256\n",
        "\n",
        "text_vectorizer = layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LEN,\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    split=\"whitespace\"\n",
        ")\n",
        "\n",
        "# -------- obtener textos raw (train) ----------\n",
        "if MODE == \"folders_txt\":\n",
        "    def read_txt_tf(path):\n",
        "        x = tf.io.read_file(path)\n",
        "        x = tf.strings.unicode_decode(x, \"UTF-8\", errors=\"replace\")\n",
        "        x = tf.strings.unicode_encode(x, \"UTF-8\")\n",
        "        return x\n",
        "\n",
        "    train_text_ds = tf.data.Dataset.from_tensor_slices(all_files[train_idx]).map(read_txt_tf, num_parallel_calls=AUTOTUNE).batch(256)\n",
        "    text_vectorizer.adapt(train_text_ds)\n",
        "\n",
        "    def make_text_ds_files(files, labels, training=False, batch=32, seed=123):\n",
        "        ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
        "        if training:\n",
        "            ds = ds.shuffle(len(files), seed=seed, reshuffle_each_iteration=True)\n",
        "        ds = ds.map(lambda p,y: (text_vectorizer(read_txt_tf(p)), y), num_parallel_calls=AUTOTUNE)\n",
        "        ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
        "        return ds\n",
        "\n",
        "    BATCH = 32\n",
        "    train_ds = make_text_ds_files(all_files[train_idx], all_labels[train_idx], training=True, batch=BATCH, seed=SEED)\n",
        "    val_ds   = make_text_ds_files(all_files[val_idx],   all_labels[val_idx],   training=False, batch=BATCH)\n",
        "    test_ds  = make_text_ds_files(all_files[test_idx],  all_labels[test_idx],  training=False, batch=BATCH)\n",
        "\n",
        "else:\n",
        "    # all_files contiene los textos raw\n",
        "    all_texts = all_files.astype(str)\n",
        "\n",
        "    train_text_ds = tf.data.Dataset.from_tensor_slices(all_texts[train_idx]).batch(256)\n",
        "    text_vectorizer.adapt(train_text_ds)\n",
        "\n",
        "    def make_text_ds_texts(texts, labels, training=False, batch=32, seed=123):\n",
        "        ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "        if training:\n",
        "            ds = ds.shuffle(len(texts), seed=seed, reshuffle_each_iteration=True)\n",
        "        ds = ds.map(lambda t,y: (text_vectorizer(t), y), num_parallel_calls=AUTOTUNE)\n",
        "        ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
        "        return ds\n",
        "\n",
        "    BATCH = 32\n",
        "    train_ds = make_text_ds_texts(all_texts[train_idx], all_labels[train_idx], training=True, batch=BATCH, seed=SEED)\n",
        "    val_ds   = make_text_ds_texts(all_texts[val_idx],   all_labels[val_idx],   training=False, batch=BATCH)\n",
        "    test_ds  = make_text_ds_texts(all_texts[test_idx],  all_labels[test_idx],  training=False, batch=BATCH)\n",
        "\n",
        "print(\"Datasets listos (texto). BATCH =\", BATCH)\n"
      ],
      "metadata": {
        "id": "nmDza1hAAor1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 3 ‚Äî MODELO RNN (texto: binario o multiclase)\n",
        "# ==========================================================\n",
        "from tensorflow.keras import models, layers\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_text_rnn(num_classes, vocab_size, embed_dim=128, rnn_units=128):\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(inputs)\n",
        "    x = layers.Bidirectional(layers.GRU(rnn_units, return_sequences=False))(x)\n",
        "    x = layers.Dropout(0.30)(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.30)(x)\n",
        "\n",
        "    if int(num_classes) == 2:\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n",
        "        loss = \"binary_crossentropy\"\n",
        "        metrics = [\"accuracy\"]\n",
        "    else:\n",
        "        outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "        loss = \"sparse_categorical_crossentropy\"\n",
        "        metrics = [\"accuracy\"]\n",
        "        if int(num_classes) >= 10:\n",
        "            metrics.append(tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"top5_acc\"))\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model, loss, metrics\n",
        "\n",
        "vocab_size = text_vectorizer.vocabulary_size()\n",
        "model, loss_fn, metrics = build_text_rnn(num_classes=num_classes, vocab_size=vocab_size)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=loss_fn,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "W9QtsJSpAq_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 4 ‚Äî TRAIN (OOM-safe + class_weight + repeat + steps_per_epoch)\n",
        "# ==========================================================\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "fit_class_weight = class_weight if USE_CLASS_WEIGHT else None\n",
        "LR = 5e-4 if USE_CLASS_WEIGHT else 1e-3\n",
        "PATIENCE = 8 if USE_CLASS_WEIGHT else 5\n",
        "\n",
        "try:\n",
        "    model.optimizer.learning_rate.assign(LR)\n",
        "except Exception:\n",
        "    model.optimizer.learning_rate = LR\n",
        "\n",
        "print(\"LR usado:\", LR)\n",
        "print(\"PATIENCE usado:\", PATIENCE)\n",
        "print(\"MONITOR_METRIC:\", MONITOR_METRIC)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=MONITOR_METRIC,\n",
        "        patience=PATIENCE,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"/content/weights.best.keras\",\n",
        "        monitor=MONITOR_METRIC,\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "def batch_candidates(b0):\n",
        "    cands = [int(b0)]\n",
        "    while cands[-1] > 8:\n",
        "        cands.append(cands[-1] // 2)\n",
        "    cands = sorted(set([b for b in cands if b >= 8]), reverse=True)\n",
        "    return cands\n",
        "\n",
        "BATCH_TRIES = batch_candidates(BATCH)\n",
        "print(\"BATCH tries:\", BATCH_TRIES)\n",
        "\n",
        "history = None\n",
        "last_err = None\n",
        "\n",
        "for b_try in BATCH_TRIES:\n",
        "    try:\n",
        "        train_ds_b = train_ds.unbatch().batch(b_try).prefetch(AUTOTUNE)\n",
        "        val_ds_b   = val_ds.unbatch().batch(b_try).prefetch(AUTOTUNE)\n",
        "        test_ds_b  = test_ds.unbatch().batch(b_try).prefetch(AUTOTUNE)\n",
        "\n",
        "        steps_per_epoch   = int(math.ceil(len(train_idx) / b_try))\n",
        "        validation_steps  = int(math.ceil(len(val_idx)   / b_try))\n",
        "\n",
        "        print(f\"\\nEntrenando con BATCH={b_try} | monitor={MONITOR_METRIC} | class_weight={fit_class_weight is not None}\")\n",
        "        history = model.fit(\n",
        "            train_ds_b.repeat(),\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            validation_data=val_ds_b,\n",
        "            validation_steps=validation_steps,\n",
        "            epochs=30,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=fit_class_weight\n",
        "        )\n",
        "\n",
        "        BATCH = b_try\n",
        "        train_ds, val_ds, test_ds = train_ds_b, val_ds_b, test_ds_b\n",
        "        last_err = None\n",
        "        break\n",
        "\n",
        "    except tf.errors.ResourceExhaustedError as e:\n",
        "        last_err = e\n",
        "        print(f\"\\n‚ö†Ô∏è OOM con BATCH={b_try}. Reintentando con batch menor...\")\n",
        "        try:\n",
        "            del train_ds_b, val_ds_b, test_ds_b\n",
        "        except Exception:\n",
        "            pass\n",
        "        gc.collect()\n",
        "\n",
        "if history is None and last_err is not None:\n",
        "    raise last_err\n",
        "\n",
        "print(\"\\n‚úÖ Entrenamiento finalizado. BATCH final usado:\", BATCH)\n"
      ],
      "metadata": {
        "id": "BYEomhqKAs_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 4.5 ‚Äî RESUMEN DE ENTRENAMIENTO (BEST epoch real por MONITOR_METRIC)\n",
        "# ==========================================================\n",
        "import numpy as np\n",
        "\n",
        "hist = history.history\n",
        "mon = MONITOR_METRIC\n",
        "\n",
        "if mon in hist:\n",
        "    if \"acc\" in mon:\n",
        "        best_epoch = int(np.argmax(hist[mon]) + 1)\n",
        "        best_value = float(np.max(hist[mon]))\n",
        "        mode = \"max\"\n",
        "    else:\n",
        "        best_epoch = int(np.argmin(hist[mon]) + 1)\n",
        "        best_value = float(np.min(hist[mon]))\n",
        "        mode = \"min\"\n",
        "\n",
        "    print(\"\\nüìå RESUMEN DE ENTRENAMIENTO\")\n",
        "    print(f\"Monitor usado      : {mon} ({mode})\")\n",
        "    print(f\"Epoch seleccionado : {best_epoch}\")\n",
        "    print(f\"Mejor {mon}        : {best_value:.4f}\")\n",
        "    print(\"‚úî restore_best_weights=True ‚Üí el modelo en memoria qued√≥ en ese epoch\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No se pudo determinar el epoch final (monitor no encontrado).\")\n",
        "    print(\"Keys disponibles:\", list(hist.keys()))\n"
      ],
      "metadata": {
        "id": "WGEltp-4Au8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 4.7 ‚Äî DIAGN√ìSTICO AUTOM√ÅTICO DEL ENTRENAMIENTO (v2, texto)\n",
        "# ==========================================================\n",
        "import numpy as np\n",
        "\n",
        "def diagnose_training_v2_text(history, num_classes, monitor_metric=\"val_loss\", patience=None):\n",
        "    h = history.history\n",
        "    epochs_ran = len(next(iter(h.values()))) if len(h) else 0\n",
        "\n",
        "    def arr(key):\n",
        "        v = h.get(key, None)\n",
        "        return None if v is None else np.array(v, dtype=float)\n",
        "\n",
        "    acc   = arr(\"accuracy\")\n",
        "    vacc  = arr(\"val_accuracy\")\n",
        "    loss  = arr(\"loss\")\n",
        "    vloss = arr(\"val_loss\")\n",
        "\n",
        "    chance = 1.0 / float(num_classes) if num_classes else np.nan\n",
        "\n",
        "    mon = arr(monitor_metric)\n",
        "    if mon is None:\n",
        "        print(\"‚ö†Ô∏è No existe monitor_metric en history:\", monitor_metric)\n",
        "        print(\"Keys:\", list(h.keys()))\n",
        "        return\n",
        "\n",
        "    if \"acc\" in monitor_metric:\n",
        "        best_i = int(np.nanargmax(mon))\n",
        "        best_val = float(np.nanmax(mon))\n",
        "        mode = \"max\"\n",
        "    else:\n",
        "        best_i = int(np.nanargmin(mon))\n",
        "        best_val = float(np.nanmin(mon))\n",
        "        mode = \"min\"\n",
        "\n",
        "    def safe_get(a, i):\n",
        "        return float(a[i]) if a is not None and len(a) > i else np.nan\n",
        "\n",
        "    last_i = epochs_ran - 1\n",
        "\n",
        "    last_acc  = safe_get(acc, last_i)\n",
        "    last_vacc = safe_get(vacc, last_i)\n",
        "    last_loss = safe_get(loss, last_i)\n",
        "    last_vloss= safe_get(vloss, last_i)\n",
        "\n",
        "    best_acc  = safe_get(acc, best_i)\n",
        "    best_vacc = safe_get(vacc, best_i)\n",
        "    best_loss = safe_get(loss, best_i)\n",
        "    best_vloss= safe_get(vloss, best_i)\n",
        "\n",
        "    degrade_loss = (not np.isnan(best_vloss) and not np.isnan(last_vloss) and last_vloss > best_vloss * 1.15)\n",
        "    degrade_acc  = (not np.isnan(best_vacc) and not np.isnan(last_vacc) and last_vacc < best_vacc - 0.07)\n",
        "\n",
        "    gap_best = best_acc - best_vacc if (not np.isnan(best_acc) and not np.isnan(best_vacc)) else np.nan\n",
        "    gap_last = last_acc - last_vacc if (not np.isnan(last_acc) and not np.isnan(last_vacc)) else np.nan\n",
        "\n",
        "    def slope(a):\n",
        "        if a is None or len(a) < 6:\n",
        "            return np.nan\n",
        "        y = a[-5:]\n",
        "        x = np.arange(len(y), dtype=float)\n",
        "        return float(np.polyfit(x, y, 1)[0])\n",
        "\n",
        "    s_acc  = slope(acc)\n",
        "    s_vacc = slope(vacc)\n",
        "    s_loss = slope(loss)\n",
        "    s_vloss= slope(vloss)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DIAGN√ìSTICO ‚Äî RESUMEN (v2, TEXTO)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Clases: {num_classes} | azar‚âà {chance:.4f} | epochs corridos: {epochs_ran}\")\n",
        "    print(f\"Monitor: {monitor_metric} ({mode}) | best_epoch={best_i+1} | best={best_val:.4f}\")\n",
        "    if patience is not None:\n",
        "        print(f\"Patience: {patience}\")\n",
        "\n",
        "    print(\"\\n‚Äî En BEST epoch (lo que queda en memoria si restore_best_weights=True) ‚Äî\")\n",
        "    print(f\"  acc={best_acc:.4f} | val_acc={best_vacc:.4f} | loss={best_loss:.4f} | val_loss={best_vloss:.4f}\")\n",
        "    print(f\"  gap(train-val) en BEST: {gap_best:.4f}\")\n",
        "\n",
        "    print(\"\\n‚Äî En √öLTIMO epoch entrenado (solo para ver tendencia) ‚Äî\")\n",
        "    print(f\"  acc={last_acc:.4f} | val_acc={last_vacc:.4f} | loss={last_loss:.4f} | val_loss={last_vloss:.4f}\")\n",
        "    print(f\"  gap(train-val) en √öLTIMO: {gap_last:.4f}\")\n",
        "    print(f\"  slopes √∫ltimos 5: acc_tr={s_acc:.4f}, acc_val={s_vacc:.4f}, loss_tr={s_loss:.4f}, loss_val={s_vloss:.4f}\")\n",
        "\n",
        "    near_chance = chance + 0.03\n",
        "\n",
        "    if not np.isnan(best_vacc) and best_vacc <= near_chance:\n",
        "        print(\"\\n‚ö†Ô∏è VALIDACI√ìN CERCA DE AZAR (pipeline/labels/split sospechoso)\")\n",
        "        return\n",
        "\n",
        "    if (not np.isnan(best_acc) and best_acc < 0.60) and (not np.isnan(best_vacc) and best_vacc < 0.60):\n",
        "        print(\"\\nüü° SUBAPRENDIZAJE (UNDERFITTING)\")\n",
        "        return\n",
        "\n",
        "    if (degrade_loss or degrade_acc) and (not np.isnan(gap_best) and gap_best >= 0.12):\n",
        "        print(\"\\nüî¥ OVERFITTING (MEMORIZACI√ìN) DESPU√âS DEL BEST\")\n",
        "        return\n",
        "\n",
        "    if (not np.isnan(best_vacc) and best_vacc > chance + 0.20) and (not np.isnan(gap_best) and gap_best <= 0.12):\n",
        "        print(\"\\n‚úÖ TODO BIEN / GENERALIZA RAZONABLEMENTE\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüü¢ MIXTO (pero NO roto): aprende, con margen de mejora\")\n",
        "\n",
        "diagnose_training_v2_text(history, num_classes=num_classes, monitor_metric=MONITOR_METRIC, patience=PATIENCE)\n"
      ],
      "metadata": {
        "id": "cgf-uAmJAxU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 5 ‚Äî EVALUACI√ìN EN TEST\n",
        "# ==========================================================\n",
        "test_out = model.evaluate(test_ds, verbose=0)\n",
        "print(\"TEST metrics:\")\n",
        "for name, val in zip(model.metrics_names, test_out):\n",
        "    print(f\"  {name:>12s}: {float(val):.4f}\")\n"
      ],
      "metadata": {
        "id": "rtYPN7HHA0NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 5.5 ‚Äî REPORTE + MATRIZ DE CONFUSI√ìN\n",
        "# ==========================================================\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for x, y in test_ds:\n",
        "    p = model.predict(x, verbose=0)\n",
        "    if int(num_classes) == 2:\n",
        "        pred = (p.reshape(-1) >= 0.5).astype(int)\n",
        "    else:\n",
        "        pred = np.argmax(p, axis=1).astype(int)\n",
        "\n",
        "    y_true.extend(y.numpy().tolist())\n",
        "    y_pred.extend(pred.tolist())\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Matriz de confusi√≥n shape:\", cm.shape)\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n"
      ],
      "metadata": {
        "id": "0cjjjmoBA4eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 6 ‚Äî PERSISTENCIA EN resultados_texto.zip + DESCARGA AUTOM√ÅTICA\n",
        "# Contiene:\n",
        "#   - model.keras\n",
        "#   - weights.best.keras (si existe)\n",
        "#   - metadata.json\n",
        "#   - vocab.txt\n",
        "# ==========================================================\n",
        "import os, zipfile, shutil, time, json\n",
        "\n",
        "OUT_ZIP = \"/content/resultados_texto.zip\"\n",
        "BUNDLE_DIR = \"/content/_bundle_resultados_texto\"\n",
        "\n",
        "if os.path.isdir(BUNDLE_DIR):\n",
        "    shutil.rmtree(BUNDLE_DIR)\n",
        "os.makedirs(BUNDLE_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Guardar modelo completo\n",
        "MODEL_PATH = os.path.join(BUNDLE_DIR, \"model.keras\")\n",
        "model.save(MODEL_PATH)\n",
        "\n",
        "# 2) Copiar checkpoint best si existe\n",
        "WEIGHTS_SRC = \"/content/weights.best.keras\"\n",
        "WEIGHTS_DST = os.path.join(BUNDLE_DIR, \"weights.best.keras\")\n",
        "if os.path.isfile(WEIGHTS_SRC):\n",
        "    shutil.copy2(WEIGHTS_SRC, WEIGHTS_DST)\n",
        "\n",
        "# 3) Guardar vocab fijo\n",
        "VOCAB_PATH = os.path.join(BUNDLE_DIR, \"vocab.txt\")\n",
        "vocab = text_vectorizer.get_vocabulary()\n",
        "with open(VOCAB_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for tok in vocab:\n",
        "        f.write(tok.replace(\"\\n\", \" \") + \"\\n\")\n",
        "\n",
        "# 4) Metadata\n",
        "meta = {\n",
        "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"zip_train_source\": ZIP_DATOS_BASENAME,\n",
        "    \"mode\": MODE,\n",
        "    \"seed\": int(SEED),\n",
        "    \"train_frac\": float(TRAIN_FRAC),\n",
        "    \"val_frac\": float(VAL_FRAC),\n",
        "    \"test_frac\": float(TEST_FRAC),\n",
        "    \"batch_final\": int(BATCH),\n",
        "    \"monitor_metric\": MONITOR_METRIC,\n",
        "    \"classes\": list(classes),\n",
        "    \"num_classes\": int(num_classes),\n",
        "    \"text\": {\n",
        "        \"max_tokens\": int(MAX_TOKENS),\n",
        "        \"seq_len\": int(SEQ_LEN),\n",
        "        \"standardize\": \"lower_and_strip_punctuation\",\n",
        "        \"split\": \"whitespace\",\n",
        "        \"vocab_file\": \"vocab.txt\",\n",
        "        \"vocab_size\": int(len(vocab)),\n",
        "    },\n",
        "    \"prediction\": {\n",
        "        \"type\": \"binary\" if int(num_classes) == 2 else \"multiclass\",\n",
        "        \"label_type\": \"int index -> classes[index]\",\n",
        "    }\n",
        "}\n",
        "with open(os.path.join(BUNDLE_DIR, \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# 5) Empaquetar zip\n",
        "if os.path.isfile(OUT_ZIP):\n",
        "    os.remove(OUT_ZIP)\n",
        "\n",
        "with zipfile.ZipFile(OUT_ZIP, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    for root, _, files in os.walk(BUNDLE_DIR):\n",
        "        for fn in files:\n",
        "            abs_path = os.path.join(root, fn)\n",
        "            rel_path = os.path.relpath(abs_path, BUNDLE_DIR)\n",
        "            z.write(abs_path, rel_path)\n",
        "\n",
        "print(\"‚úÖ Creado:\", OUT_ZIP)\n",
        "!ls -lah /content/resultados_texto.zip\n",
        "\n",
        "# 6) Descarga autom√°tica (Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(OUT_ZIP)\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è No se pudo descargar autom√°ticamente:\", e)\n"
      ],
      "metadata": {
        "id": "zT2EGn2wA5-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}