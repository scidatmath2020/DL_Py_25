{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hptjXLQsE4Pw"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# NOTEBOOK / SCRIPT: INFERENCIA TEXTO DESDE 2 ZIPs\n",
        "#   - resultados_texto.zip  (bundle entrenado)\n",
        "#   - textos_nuevos.zip     (datos nuevos)\n",
        "#\n",
        "# Soporta formatos de textos_nuevos.zip:\n",
        "#   (A) Carpetas por clase con .txt (las clases pueden existir o no; aquí se ignoran)\n",
        "#       /\n",
        "#         cualquier_carpeta/*.txt\n",
        "#         ...\n",
        "#       -> toma TODOS los .txt recursivamente\n",
        "#\n",
        "#   (B) CSV con 1 o 2 columnas de texto (autodetecta), sin label\n",
        "#       /\n",
        "#         *.csv\n",
        "#       -> concatena 2 columnas si aplica\n",
        "#\n",
        "# Salida:\n",
        "#   - /content/predicciones_texto.csv\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CELDA 0 — CONFIG + descomprimir resultados_texto.zip y textos_nuevos.zip\n",
        "# =========================\n",
        "import os, glob, zipfile, shutil, json, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "RESULTS_ZIP = \"/content/resultados_texto.zip\"\n",
        "NEW_ZIP     = \"/content/textos_nuevos.zip\"\n",
        "\n",
        "assert os.path.isfile(RESULTS_ZIP), f\"No existe: {RESULTS_ZIP}\"\n",
        "assert os.path.isfile(NEW_ZIP), f\"No existe: {NEW_ZIP}\"\n",
        "\n",
        "RESULTS_DIR = \"/content/_infer_resultados_texto\"\n",
        "NEW_DIR     = \"/content/_infer_textos_nuevos\"\n",
        "\n",
        "for d in [RESULTS_DIR, NEW_DIR]:\n",
        "    if os.path.isdir(d):\n",
        "        shutil.rmtree(d)\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(RESULTS_ZIP, \"r\") as z:\n",
        "    z.extractall(RESULTS_DIR)\n",
        "\n",
        "with zipfile.ZipFile(NEW_ZIP, \"r\") as z:\n",
        "    z.extractall(NEW_DIR)\n",
        "\n",
        "print(\"✅ resultados extraído en:\", RESULTS_DIR)\n",
        "print(\"✅ nuevos textos extraído en:\", NEW_DIR)\n",
        "!ls -lah \"{RESULTS_DIR}\"\n",
        "!ls -lah \"{NEW_DIR}\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CELDA 1 — Cargar metadata + vocab + modelo (preprocesamiento aprendido)\n",
        "# =========================\n",
        "META_PATH  = os.path.join(RESULTS_DIR, \"metadata.json\")\n",
        "MODEL_PATH = os.path.join(RESULTS_DIR, \"model.keras\")\n",
        "VOCAB_PATH = os.path.join(RESULTS_DIR, \"vocab.txt\")\n",
        "\n",
        "assert os.path.isfile(META_PATH),  \"metadata.json no encontrado en resultados_texto.zip\"\n",
        "assert os.path.isfile(MODEL_PATH), \"model.keras no encontrado en resultados_texto.zip\"\n",
        "assert os.path.isfile(VOCAB_PATH), \"vocab.txt no encontrado en resultados_texto.zip\"\n",
        "\n",
        "with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "classes = meta[\"classes\"]\n",
        "num_classes = int(meta[\"num_classes\"])\n",
        "SEQ_LEN = int(meta[\"text\"][\"seq_len\"])\n",
        "MAX_TOKENS = int(meta[\"text\"][\"max_tokens\"])\n",
        "\n",
        "print(\"MODE entrenado:\", meta.get(\"mode\"))\n",
        "print(\"Num clases:\", num_classes)\n",
        "print(\"SEQ_LEN:\", SEQ_LEN, \"| MAX_TOKENS:\", MAX_TOKENS)\n",
        "print(\"Clases (primeras 20):\", classes[:20], \"...\" if len(classes) > 20 else \"\")\n",
        "\n",
        "# ---- reconstruir TextVectorization con el vocab aprendido ----\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "with open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab = [line.rstrip(\"\\n\") for line in f]\n",
        "\n",
        "text_vectorizer = layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LEN,\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    split=\"whitespace\"\n",
        ")\n",
        "text_vectorizer.set_vocabulary(vocab)\n",
        "\n",
        "# ---- cargar modelo ----\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "print(\"✅ Modelo cargado:\", MODEL_PATH)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CELDA 2 — Detectar formato de textos_nuevos.zip y listar entradas\n",
        "# =========================\n",
        "TEXT_EXTS = (\".txt\",)\n",
        "CSV_EXTS  = (\".csv\",)\n",
        "\n",
        "def walk_files(root):\n",
        "    for dirpath, _, filenames in os.walk(root):\n",
        "        for fn in filenames:\n",
        "            yield os.path.join(dirpath, fn)\n",
        "\n",
        "def list_txt_files(root):\n",
        "    return sorted([p for p in walk_files(root) if p.lower().endswith(TEXT_EXTS)])\n",
        "\n",
        "def list_csv_files(root):\n",
        "    return sorted([p for p in walk_files(root) if p.lower().endswith(CSV_EXTS)])\n",
        "\n",
        "def detect_text_columns_unlabeled(df):\n",
        "    \"\"\"\n",
        "    Autodetecta 1 o 2 columnas de texto para inferencia (sin label).\n",
        "    Criterio: dtype object/string y longitud promedio mayor.\n",
        "    Regresa: [col] o [col1, col2] o None\n",
        "    \"\"\"\n",
        "    df = df.dropna(axis=1, how=\"all\")\n",
        "    if df.shape[1] < 1:\n",
        "        return None\n",
        "\n",
        "    text_cands = [c for c in df.columns if (df[c].dtype == \"object\" or str(df[c].dtype).startswith(\"string\"))]\n",
        "    if len(text_cands) == 0:\n",
        "        return None\n",
        "\n",
        "    def text_score(col):\n",
        "        s = df[col].dropna().astype(str)\n",
        "        if len(s) == 0:\n",
        "            return -1.0\n",
        "        L = s.str.len().clip(0, 10000)\n",
        "        return float(L.mean())\n",
        "\n",
        "    text_cands = sorted(text_cands, key=text_score, reverse=True)\n",
        "    if text_score(text_cands[0]) < 2:\n",
        "        return None\n",
        "\n",
        "    # 1 texto por defecto\n",
        "    cols1 = [text_cands[0]]\n",
        "\n",
        "    # si hay segunda columna con score cercano, usar 2\n",
        "    if len(text_cands) >= 2:\n",
        "        s0 = text_score(text_cands[0])\n",
        "        s1 = text_score(text_cands[1])\n",
        "        if s1 >= 0.5 * s0 and s1 >= 2:\n",
        "            return [text_cands[0], text_cands[1]]\n",
        "\n",
        "    return cols1\n",
        "\n",
        "txt_files = list_txt_files(NEW_DIR)\n",
        "csv_files = list_csv_files(NEW_DIR)\n",
        "\n",
        "NEW_MODE = None\n",
        "CSV_PATH = None\n",
        "CSV_TEXT_COLS = None\n",
        "\n",
        "if len(txt_files) > 0:\n",
        "    NEW_MODE = \"txt_files\"\n",
        "    print(\"Detectado: TXT (archivos .txt). Total:\", len(txt_files))\n",
        "else:\n",
        "    # busca un csv con texto\n",
        "    for c in csv_files:\n",
        "        try:\n",
        "            df0 = pd.read_csv(c)\n",
        "            cols = detect_text_columns_unlabeled(df0)\n",
        "            if cols is None:\n",
        "                continue\n",
        "            NEW_MODE = \"csv_unlabeled\"\n",
        "            CSV_PATH = c\n",
        "            CSV_TEXT_COLS = cols\n",
        "            break\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "if NEW_MODE is None:\n",
        "    raise ValueError(\n",
        "        \"No detecté textos en el ZIP nuevo.\\n\"\n",
        "        \"- Debe contener .txt (cualquier carpeta)\\n\"\n",
        "        \"- o un .csv con 1 o 2 columnas de texto.\\n\"\n",
        "        f\"NEW_DIR={NEW_DIR}\"\n",
        "    )\n",
        "\n",
        "print(\"NEW_MODE:\", NEW_MODE)\n",
        "if NEW_MODE == \"csv_unlabeled\":\n",
        "    print(\"CSV_PATH:\", CSV_PATH)\n",
        "    print(\"CSV_TEXT_COLS:\", CSV_TEXT_COLS)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CELDA 3 — Construir Dataset tf.data y predecir\n",
        "# =========================\n",
        "BATCH = int(meta.get(\"batch_final\", 32))\n",
        "\n",
        "def read_txt_tf(path):\n",
        "    x = tf.io.read_file(path)\n",
        "    x = tf.strings.unicode_decode(x, \"UTF-8\", errors=\"replace\")\n",
        "    x = tf.strings.unicode_encode(x, \"UTF-8\")\n",
        "    return x\n",
        "\n",
        "def make_ds_from_txt_files(paths, batch):\n",
        "    paths = tf.constant(paths)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    ds = ds.map(lambda p: text_vectorizer(read_txt_tf(p)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "def make_ds_from_texts(texts, batch):\n",
        "    texts = tf.constant([str(t) for t in texts])\n",
        "    ds = tf.data.Dataset.from_tensor_slices(texts)\n",
        "    ds = ds.map(lambda t: text_vectorizer(t), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# --- construir inputs + ids para salida ---\n",
        "items_id = []\n",
        "ds = None\n",
        "\n",
        "if NEW_MODE == \"txt_files\":\n",
        "    items_id = txt_files[:]  # filepath real\n",
        "    ds = make_ds_from_txt_files(txt_files, batch=BATCH)\n",
        "\n",
        "else:\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    df = df.dropna(subset=CSV_TEXT_COLS)\n",
        "\n",
        "    if len(CSV_TEXT_COLS) == 1:\n",
        "        texts = df[CSV_TEXT_COLS[0]].astype(str).values\n",
        "    else:\n",
        "        texts = (df[CSV_TEXT_COLS[0]].astype(str) + \" \" + df[CSV_TEXT_COLS[1]].astype(str)).values\n",
        "\n",
        "    # id para salida: índice original del df\n",
        "    items_id = df.index.astype(int).tolist()\n",
        "    ds = make_ds_from_texts(texts, batch=BATCH)\n",
        "\n",
        "# --- predicción ---\n",
        "probs = model.predict(ds, verbose=0)\n",
        "\n",
        "# binario vs multiclase\n",
        "if int(num_classes) == 2:\n",
        "    # salida (N,1) con sigmoid\n",
        "    p = probs.reshape(-1)\n",
        "    pred_idx = (p >= 0.5).astype(int)\n",
        "    conf = np.where(pred_idx == 1, p, 1.0 - p)\n",
        "else:\n",
        "    pred_idx = np.argmax(probs, axis=1).astype(int)\n",
        "    conf = np.max(probs, axis=1)\n",
        "\n",
        "pred_class = [classes[i] for i in pred_idx]\n",
        "\n",
        "print(\"OK ✅ Predicciones:\", len(pred_class))\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CELDA 4 — Guardar CSV de salida\n",
        "# =========================\n",
        "OUT_CSV = \"/content/predicciones_texto.csv\"\n",
        "\n",
        "rows = []\n",
        "for rid, i, c, cf in zip(items_id, pred_idx.tolist(), pred_class, conf.tolist()):\n",
        "    rows.append({\n",
        "        \"id\": rid,                 # filepath (txt) o index (csv)\n",
        "        \"pred_idx\": int(i),\n",
        "        \"pred_class\": c,\n",
        "        \"confidence\": float(cf)\n",
        "    })\n",
        "\n",
        "out_df = pd.DataFrame(rows)\n",
        "out_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "print(\"✅ Guardado:\", OUT_CSV)\n",
        "!head -n 20 \"{OUT_CSV}\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CELDA 5 — Descargar CSV (Colab)\n",
        "# =========================\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(OUT_CSV)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ No se pudo descargar automáticamente:\", e)\n"
      ]
    }
  ]
}