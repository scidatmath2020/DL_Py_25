{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9nhhK7MYwj7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CLASIFICACIÓN MULTICLASE\n",
        "# Entradas:\n",
        "#   1) artifacts_preprocesamiento.zip   (contiene las tablas procesadas)\n",
        "# Salida:\n",
        "#   resultados.zip (historial de entrenamiento, metadatos y modelo final)\n",
        "# ============================================================\n",
        "\n",
        "# ============================================================\n",
        "# Diseñador de red para clasificación multiclase basado en heurísticas\n",
        "# + regularización (L2, dropout por capa, early stopping)\n",
        "# ============================================================\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DisenoRedMulticlase:\n",
        "    capas: List[int]\n",
        "    P: int\n",
        "    rho: float\n",
        "    l2: float\n",
        "    dropouts: List[float]\n",
        "    patience: int\n",
        "    min_delta: float\n",
        "    max_epochs: int\n",
        "\n",
        "\n",
        "def clip(x: float, lo: float, hi: float) -> float:\n",
        "    return max(lo, min(hi, x))\n",
        "\n",
        "\n",
        "def estimar_parametros_multiclase(n0: int, capas: List[int], K: int) -> int:\n",
        "    \"\"\"\n",
        "    Cuenta parámetros de una red densa:\n",
        "      - incluye sesgos en cada capa\n",
        "      - incluye capa de salida (K neuronas)\n",
        "    \"\"\"\n",
        "    if not capas:\n",
        "        return 0\n",
        "\n",
        "    P = (n0 + 1) * capas[0]\n",
        "    for i in range(1, len(capas)):\n",
        "        P += (capas[i - 1] + 1) * capas[i]\n",
        "    P += (capas[-1] + 1) * K\n",
        "    return int(P)\n",
        "\n",
        "\n",
        "def disenar_red_multiclase(\n",
        "    d: int,\n",
        "    n0: int,\n",
        "    K: int,\n",
        "    *,\n",
        "    k: int = 10,\n",
        "    c1: float = 2.0,\n",
        "    r: float = 0.5,\n",
        "    n_min: int = 8,\n",
        "    L_max: int = 4,\n",
        ") -> DisenoRedMulticlase:\n",
        "    \"\"\"\n",
        "    Heurística análoga a la binaria:\n",
        "      - d  : número de muestras (train)\n",
        "      - n0 : número de variables de entrada\n",
        "      - K  : número de clases (>= 3)\n",
        "    \"\"\"\n",
        "\n",
        "    # 0. Tope adaptativo de ancho (según tamaño de muestra)\n",
        "    n_max = min(1024, max(64, math.floor(0.25 * d)))\n",
        "\n",
        "    # 1. Verificación mínima de viabilidad\n",
        "    if d < 2 * n0:\n",
        "        raise ValueError(\"Dataset muy pequeño: alto riesgo de sobreajuste\")\n",
        "\n",
        "    if K < 3:\n",
        "        raise ValueError(\"K debe ser >= 3 para multiclase\")\n",
        "\n",
        "    # 2. Presupuesto total de parámetros\n",
        "    P_max = math.floor(k * d)\n",
        "\n",
        "    # 3. Primera capa oculta\n",
        "    n1_cap_presupuesto = math.floor(P_max / (n0 + 1))\n",
        "    n1 = min(math.floor(c1 * n0), n1_cap_presupuesto, n_max)\n",
        "\n",
        "    if n1 < n_min:\n",
        "        raise ValueError(\"Presupuesto insuficiente: no se puede ni una capa >= n_min\")\n",
        "\n",
        "    capas = [int(n1)]\n",
        "\n",
        "    # 4. Construcción iterativa (embudo)\n",
        "    while True:\n",
        "        if len(capas) >= L_max:\n",
        "            break\n",
        "\n",
        "        n_prev = capas[-1]\n",
        "        n_new = math.floor(r * n_prev)\n",
        "\n",
        "        if n_new < n_min:\n",
        "            break\n",
        "\n",
        "        n_new = min(n_new, n_max)\n",
        "        capas.append(int(n_new))\n",
        "\n",
        "    # 5. Parámetros totales\n",
        "    P = estimar_parametros_multiclase(n0, capas, K)\n",
        "\n",
        "    # 6. Recorte si excede presupuesto\n",
        "    while P > P_max:\n",
        "        if len(capas) > 1:\n",
        "            capas.pop()\n",
        "        else:\n",
        "            n_old = capas[0]\n",
        "            capas[0] = math.floor(0.9 * capas[0])\n",
        "            if capas[0] >= n_old:\n",
        "                capas[0] = n_old - 1\n",
        "            if capas[0] < n_min:\n",
        "                raise ValueError(\"Presupuesto insuficiente: no cabe una capa >= n_min\")\n",
        "\n",
        "        P = estimar_parametros_multiclase(n0, capas, K)\n",
        "\n",
        "    # ========================================================\n",
        "    # 7) Regularización adaptativa\n",
        "    # ========================================================\n",
        "\n",
        "    rho = P / P_max if P_max > 0 else 1.0\n",
        "\n",
        "    # Dropout base por tamaño\n",
        "    if d < 2000:\n",
        "        drop_base = 0.35\n",
        "    elif d < 20000:\n",
        "        drop_base = 0.25\n",
        "    else:\n",
        "        drop_base = 0.15\n",
        "\n",
        "    # Ajuste por rho\n",
        "    if rho >= 0.8:\n",
        "        drop = drop_base + 0.10\n",
        "    elif rho >= 0.4:\n",
        "        drop = drop_base\n",
        "    else:\n",
        "        drop = drop_base - 0.10\n",
        "    drop = clip(drop, 0.05, 0.50)\n",
        "\n",
        "    # Dropout por capa (más al inicio)\n",
        "    dropouts: List[float] = []\n",
        "    for i in range(1, len(capas) + 1):\n",
        "        di = drop * (1.0 - 0.15 * (i - 1))\n",
        "        di = clip(di, 0.05, 0.50)\n",
        "        dropouts.append(float(di))\n",
        "\n",
        "    # L2 base por tamaño\n",
        "    if d < 2000:\n",
        "        l2_base = 1e-3\n",
        "    elif d < 20000:\n",
        "        l2_base = 3e-4\n",
        "    else:\n",
        "        l2_base = 1e-4\n",
        "\n",
        "    # Ajuste por rho\n",
        "    if rho >= 0.8:\n",
        "        l2 = 3.0 * l2_base\n",
        "    elif rho >= 0.4:\n",
        "        l2 = 1.0 * l2_base\n",
        "    else:\n",
        "        l2 = 0.3 * l2_base\n",
        "    l2 = clip(l2, 1e-6, 3e-3)\n",
        "\n",
        "    # Early stopping\n",
        "    if d < 2000:\n",
        "        patience = 20\n",
        "        max_epochs = 400\n",
        "    elif d < 20000:\n",
        "        patience = 15\n",
        "        max_epochs = 200\n",
        "    else:\n",
        "        patience = 10\n",
        "        max_epochs = 100\n",
        "\n",
        "    min_delta = 1e-4\n",
        "\n",
        "    return DisenoRedMulticlase(\n",
        "        capas=capas,\n",
        "        P=int(P),\n",
        "        rho=float(rho),\n",
        "        l2=float(l2),\n",
        "        dropouts=dropouts,\n",
        "        patience=int(patience),\n",
        "        min_delta=float(min_delta),\n",
        "        max_epochs=int(max_epochs),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Reproducibilidad (opcional)\n",
        "SEED = 7\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Abrir ZIP y leer train/val/test\n",
        "# ============================================================\n",
        "\n",
        "ZIP_PATH = \"artifacts_preprocesamiento.zip\"\n",
        "\n",
        "def read_csv_from_zip(zip_path: str, csv_name: str) -> pd.DataFrame:\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        with z.open(csv_name) as f:\n",
        "            return pd.read_csv(f)\n",
        "\n",
        "train = read_csv_from_zip(ZIP_PATH, \"train_final.csv\")\n",
        "val   = read_csv_from_zip(ZIP_PATH, \"val_final.csv\")\n",
        "test  = read_csv_from_zip(ZIP_PATH, \"test_final.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 2) Definir target y armar X/y\n",
        "# ============================================================\n",
        "\n",
        "TARGET_COL = \"target\"\n",
        "\n",
        "X_train = train.drop(columns=[TARGET_COL])\n",
        "y_train_raw = train[TARGET_COL].astype(int)\n",
        "\n",
        "X_val = val.drop(columns=[TARGET_COL])\n",
        "y_val_raw = val[TARGET_COL].astype(int)\n",
        "\n",
        "X_test = test.drop(columns=[TARGET_COL])\n",
        "y_test_raw = test[TARGET_COL].astype(int)\n",
        "\n",
        "d  = X_train.shape[0]\n",
        "n0 = X_train.shape[1]\n",
        "\n",
        "# ============================================================\n",
        "# 3) Mapeo automático de etiquetas (clase_original -> idx 0..K-1)\n",
        "# ============================================================\n",
        "\n",
        "classes_original = np.sort(y_train_raw.unique())\n",
        "K = int(len(classes_original))\n",
        "\n",
        "class_to_index = {int(c): int(i) for i, c in enumerate(classes_original)}\n",
        "index_to_class = {int(i): int(c) for i, c in enumerate(classes_original)}\n",
        "\n",
        "def map_labels(y_series: pd.Series, mapping: dict) -> np.ndarray:\n",
        "    y_mapped = y_series.map(mapping)\n",
        "    if y_mapped.isna().any():\n",
        "        unseen = sorted(set(y_series.unique()) - set(mapping.keys()))\n",
        "        raise ValueError(\n",
        "            f\"Clases en val/test no vistas en train: {unseen}. \"\n",
        "            f\"Revisa el split/stratify o la construcción de datasets.\"\n",
        "        )\n",
        "    return y_mapped.astype(int).values\n",
        "\n",
        "y_train = map_labels(y_train_raw, class_to_index)\n",
        "y_val   = map_labels(y_val_raw, class_to_index)\n",
        "y_test  = map_labels(y_test_raw, class_to_index)\n",
        "\n",
        "print(\"d =\", d)\n",
        "print(\"n0 =\", n0)\n",
        "print(\"K =\", K)\n",
        "print(\"classes_original (train) =\", classes_original.tolist())\n",
        "print(\"class_to_index =\", class_to_index)\n",
        "\n",
        "# ============================================================\n",
        "# 4) Detección automática de desbalanceo + class_weight\n",
        "# ============================================================\n",
        "\n",
        "counts = pd.Series(y_train).value_counts().sort_index()\n",
        "counts = counts.reindex(range(K), fill_value=0)\n",
        "\n",
        "min_c = int(counts.min())\n",
        "max_c = int(counts.max())\n",
        "imbalance_ratio = (max_c / min_c) if min_c > 0 else float(\"inf\")\n",
        "\n",
        "IMBALANCE_THRESHOLD = 1.5\n",
        "usa_class_weight = bool(imbalance_ratio >= IMBALANCE_THRESHOLD)\n",
        "\n",
        "class_weight = None\n",
        "if usa_class_weight:\n",
        "    total = float(len(y_train))\n",
        "    class_weight = {int(i): float(total / (K * counts.loc[i])) for i in range(K)}\n",
        "\n",
        "print(\"\\n=== Desbalanceo (train, índices internos) ===\")\n",
        "print(\"counts (idx->n):\\n\", counts.to_string())\n",
        "print(\"imbalance_ratio (max/min):\", float(imbalance_ratio))\n",
        "print(\"IMBALANCE_THRESHOLD:\", float(IMBALANCE_THRESHOLD))\n",
        "print(\"¿Se usará class_weight?:\", usa_class_weight)\n",
        "if usa_class_weight:\n",
        "    print(\"class_weight (idx->w):\", class_weight)"
      ],
      "metadata": {
        "id": "IQM0yQTyYyaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3) Configuración de evaluación (simetría vs clases críticas)\n",
        "# ============================================================\n",
        "#\n",
        "# True  -> clases \"simétricas\" -> optimizamos Accuracy\n",
        "# False -> robustez ante desbalanceo -> optimizamos Macro-F1\n",
        "#\n",
        "# NOTA: En cualquier caso reportamos Accuracy, Macro-F1 y Weighted-F1.\n",
        "# ============================================================\n",
        "\n",
        "CLASES_SIMETRICAS = False"
      ],
      "metadata": {
        "id": "oBMMqxAAY-tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diseno = disenar_red_multiclase(d, n0, K)\n",
        "print(diseno)"
      ],
      "metadata": {
        "id": "-60ysWjJY_QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capas     = diseno.capas\n",
        "l2_value  = diseno.l2\n",
        "dropouts  = diseno.dropouts\n",
        "patience  = diseno.patience\n",
        "min_delta = diseno.min_delta\n",
        "max_epochs= diseno.max_epochs\n",
        "\n",
        "def build_multiclass_mlp(n0: int, K: int, capas: list, l2_value: float, dropouts: list) -> keras.Model:\n",
        "    assert len(capas) == len(dropouts), \"capas y dropouts deben tener la misma longitud\"\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Input(shape=(n0,)))\n",
        "\n",
        "    for units, dr in zip(capas, dropouts):\n",
        "        model.add(\n",
        "            keras.layers.Dense(\n",
        "                units,\n",
        "                activation=\"relu\",\n",
        "                kernel_regularizer=keras.regularizers.l2(l2_value)\n",
        "            )\n",
        "        )\n",
        "        model.add(keras.layers.Dropout(dr))\n",
        "\n",
        "    model.add(keras.layers.Dense(K, activation=\"softmax\"))\n",
        "    return model\n",
        "\n",
        "model = build_multiclass_mlp(n0=n0, K=K, capas=capas, l2_value=l2_value, dropouts=dropouts)"
      ],
      "metadata": {
        "id": "U1eRVI7lZAjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\n",
        "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OiP5x8qxZBov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5) Entrenar + validar (EarlyStopping) + class_weight si aplica\n",
        "# ============================================================\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=patience,\n",
        "        min_delta=min_delta,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "fit_kwargs = {}\n",
        "if usa_class_weight and (class_weight is not None):\n",
        "    fit_kwargs[\"class_weight\"] = class_weight  # SOLO si hay desbalanceo\n",
        "\n",
        "history = model.fit(\n",
        "    X_train.astype(np.float32),\n",
        "    y_train,\n",
        "    validation_data=(X_val.astype(np.float32), y_val),\n",
        "    epochs=max_epochs,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks,\n",
        "    **fit_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "I2-JW0rkft8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6) Evaluación + métricas robustas + ZIP final\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def confusion_matrix_multiclass(y_true: np.ndarray, y_pred: np.ndarray, K: int) -> np.ndarray:\n",
        "    cm = np.zeros((K, K), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[int(t), int(p)] += 1\n",
        "    return cm\n",
        "\n",
        "def accuracy_from_cm(cm: np.ndarray) -> float:\n",
        "    total = cm.sum()\n",
        "    return float(np.trace(cm) / total) if total else 0.0\n",
        "\n",
        "def f1_per_class_from_cm(cm: np.ndarray):\n",
        "    K = cm.shape[0]\n",
        "    f1s = np.zeros(K, dtype=float)\n",
        "    support = cm.sum(axis=1).astype(float)\n",
        "    for k in range(K):\n",
        "        TP = cm[k, k]\n",
        "        FP = cm[:, k].sum() - TP\n",
        "        FN = cm[k, :].sum() - TP\n",
        "        denom = 2*TP + FP + FN\n",
        "        f1s[k] = (2*TP / denom) if denom else 0.0\n",
        "    return f1s, support\n",
        "\n",
        "def f1_macro_from_cm(cm: np.ndarray) -> float:\n",
        "    f1s, _ = f1_per_class_from_cm(cm)\n",
        "    return float(np.mean(f1s)) if len(f1s) else 0.0\n",
        "\n",
        "def f1_weighted_from_cm(cm: np.ndarray) -> float:\n",
        "    f1s, support = f1_per_class_from_cm(cm)\n",
        "    total = float(support.sum())\n",
        "    if total <= 0:\n",
        "        return 0.0\n",
        "    w = support / total\n",
        "    return float((w * f1s).sum())\n",
        "\n",
        "def print_confusion_matrix_multiclass(cm: np.ndarray, *, title=\"Matriz de confusión\"):\n",
        "    print(f\"\\n{title}: (filas=True, columnas=Pred) [índices internos 0..K-1]\")\n",
        "    K = cm.shape[0]\n",
        "    header = \"True\\\\Pred | \" + \" \".join([f\"{j:6d}\" for j in range(K)])\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for i in range(K):\n",
        "        row = \" \".join([f\"{cm[i,j]:6d}\" for j in range(K)])\n",
        "        print(f\"{i:9d} | {row}\")\n",
        "\n",
        "# ---- VALIDACIÓN ----\n",
        "p_val = model.predict(X_val.astype(np.float32), verbose=0)\n",
        "yhat_val = np.argmax(p_val, axis=1).astype(int)\n",
        "\n",
        "cm_val = confusion_matrix_multiclass(y_val, yhat_val, K)\n",
        "acc_val = accuracy_from_cm(cm_val)\n",
        "f1m_val = f1_macro_from_cm(cm_val)\n",
        "f1w_val = f1_weighted_from_cm(cm_val)\n",
        "\n",
        "metric_principal = \"Accuracy\" if CLASES_SIMETRICAS else \"Macro-F1\"\n",
        "score_val_principal = acc_val if CLASES_SIMETRICAS else f1m_val\n",
        "\n",
        "print(\"\\n=== Configuración de decisión (multiclase) ===\")\n",
        "print(\"Métrica principal:\", metric_principal)\n",
        "print(\"¿Clases 'simétricas'?:\", bool(CLASES_SIMETRICAS))\n",
        "print(\"Regla de decisión:\", \"argmax(softmax)\")\n",
        "print(\"Score en VALIDACIÓN (métrica principal):\", float(score_val_principal))\n",
        "\n",
        "print(\"¿Se usó class_weight?:\", bool(usa_class_weight))\n",
        "if usa_class_weight:\n",
        "    print(\"IMBALANCE_THRESHOLD:\", float(IMBALANCE_THRESHOLD))\n",
        "    print(\"imbalance_ratio (train max/min):\", float(imbalance_ratio))\n",
        "\n",
        "print_confusion_matrix_multiclass(cm_val, title=\"Matriz de confusión (VALIDACIÓN)\")\n",
        "\n",
        "print(\"\\nMétricas en validación:\")\n",
        "print(\"-----------------------------------\")\n",
        "print(f\"{'Métrica':<15} | {'Valor':>10}\")\n",
        "print(\"-----------------------------------\")\n",
        "print(f\"{'Accuracy':<15} | {acc_val:10.4f}\")\n",
        "print(f\"{'Macro-F1':<15} | {f1m_val:10.4f}\")\n",
        "print(f\"{'Weighted-F1':<15} | {f1w_val:10.4f}\")\n",
        "print(\"-----------------------------------\")\n",
        "\n",
        "# ---- TEST ----\n",
        "p_test = model.predict(X_test.astype(np.float32), verbose=0)\n",
        "yhat_test = np.argmax(p_test, axis=1).astype(int)\n",
        "\n",
        "cm_test = confusion_matrix_multiclass(y_test, yhat_test, K)\n",
        "acc_test = accuracy_from_cm(cm_test)\n",
        "f1m_test = f1_macro_from_cm(cm_test)\n",
        "f1w_test = f1_weighted_from_cm(cm_test)\n",
        "\n",
        "print_confusion_matrix_multiclass(cm_test, title=\"Matriz de confusión (TEST)\")\n",
        "\n",
        "print(\"\\nMétricas en test:\")\n",
        "print(\"-----------------------------------\")\n",
        "print(f\"{'Métrica':<15} | {'Valor':>10}\")\n",
        "print(\"-----------------------------------\")\n",
        "print(f\"{'Accuracy':<15} | {acc_test:10.4f}\")\n",
        "print(f\"{'Macro-F1':<15} | {f1m_test:10.4f}\")\n",
        "print(f\"{'Weighted-F1':<15} | {f1w_test:10.4f}\")\n",
        "print(\"-----------------------------------\")\n",
        "\n",
        "# ============================================================\n",
        "# 7) Guardar resultados y empaquetar ZIP final\n",
        "# ============================================================\n",
        "\n",
        "OUT_DIR = \"salida_multiclase\"\n",
        "ZIP_NAME = \"resultados.zip\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "metadata = {\n",
        "    \"problem_type\": \"multiclass_classification\",\n",
        "\n",
        "    \"n_samples_train\": int(d),\n",
        "    \"n_features\": int(n0),\n",
        "    \"n_classes\": int(K),\n",
        "\n",
        "    # --- mapeo crítico ---\n",
        "    \"classes_original_train\": [int(c) for c in classes_original],\n",
        "    \"class_to_index\": {str(int(k)): int(v) for k, v in class_to_index.items()},\n",
        "    \"index_to_class\": {str(int(k)): int(v) for k, v in index_to_class.items()},\n",
        "\n",
        "    \"architecture\": capas,\n",
        "    \"l2\": float(l2_value),\n",
        "    \"dropouts\": dropouts,\n",
        "    \"patience\": int(patience),\n",
        "    \"min_delta\": float(min_delta),\n",
        "    \"max_epochs\": int(max_epochs),\n",
        "\n",
        "    # --- decisión / prioridad ---\n",
        "    \"clases_simetricas\": bool(CLASES_SIMETRICAS),\n",
        "    \"metrica_principal\": metric_principal,\n",
        "    \"decision_rule\": \"argmax_softmax\",\n",
        "\n",
        "    # --- desbalanceo / pesos ---\n",
        "    \"imbalance_threshold\": float(IMBALANCE_THRESHOLD),\n",
        "    \"imbalance_ratio_train_max_min\": float(imbalance_ratio),\n",
        "    \"used_class_weight\": bool(usa_class_weight),\n",
        "    \"class_counts_train_indexed\": {str(int(k)): int(v) for k, v in counts.items()},\n",
        "    \"class_weight_indexed\": None if not usa_class_weight else {str(int(k)): float(v) for k, v in class_weight.items()},\n",
        "\n",
        "    # --- resultados ---\n",
        "    \"metrics_val\": {\n",
        "        \"accuracy\": float(acc_val),\n",
        "        \"macro_f1\": float(f1m_val),\n",
        "        \"weighted_f1\": float(f1w_val),\n",
        "    },\n",
        "    \"metrics_test\": {\n",
        "        \"accuracy\": float(acc_test),\n",
        "        \"macro_f1\": float(f1m_test),\n",
        "        \"weighted_f1\": float(f1w_test),\n",
        "    },\n",
        "\n",
        "    \"confusion_matrix_val_indexed\": cm_val.tolist(),\n",
        "    \"confusion_matrix_test_indexed\": cm_test.tolist(),\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"metadata.json\"), \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "model_path = os.path.join(OUT_DIR, \"modelo.keras\")\n",
        "model.save(model_path)\n",
        "\n",
        "history_path = os.path.join(OUT_DIR, \"historial_entrenamiento.csv\")\n",
        "pd.DataFrame(history.history).to_csv(history_path, index=False)\n",
        "\n",
        "with zipfile.ZipFile(ZIP_NAME, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for file in [model_path, history_path, os.path.join(OUT_DIR, \"metadata.json\")]:\n",
        "        zipf.write(file, arcname=os.path.basename(file))\n",
        "\n",
        "print(f\"\\n✔ ZIP generado correctamente: {ZIP_NAME}\")"
      ],
      "metadata": {
        "id": "e6HLBbiyZDpm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}