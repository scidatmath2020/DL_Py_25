{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8d7a10-2941-4f36-9a87-7ba7472c5b56",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d16f88-0460-4713-9236-dcafa2c7af6e",
   "metadata": {},
   "source": [
    "# Modelo matemático\n",
    "\n",
    "Vayamos ahora a las matemáticas que trabajan en una red neuronal.\n",
    "\n",
    "## Capa de entrada\n",
    "\n",
    "Supongamos que llega un cliente potencial al antro y se forma en la fila. El cliente tiene ciertas características: edad, peso, estatura, vestimenta, actitud, hora de llegada, etc. En ese momento **nadie juzga ni aprueba nada**. El cliente solamente está formado.\n",
    "\n",
    "En una red neuronal, esta fila es la capa de entrada. Las neuronas que la conforman se encargan únicamente de recabar la información del cliente formado. Esta información es un número. De esta manera, en una neurona entra la edad; en otra neurona entra el peso; en otra neurona entra la estatura; en otra neurona entra la vestimenta; etc.\n",
    "\n",
    "Así, si la capa de entrada tiene $n$ neuronas, entonces esta espera recibir $n$ números; llamemos $x_1$, $x_2$,..., $x_n$ a estos valores y denotemos por $x$ a $$\\boldsymbol{x}=(x_1,x_2,...,x_n)$$\n",
    "\n",
    "## Capas medias\n",
    "\n",
    "### Primera capa \n",
    "\n",
    "El cliente llega a una primera revisión formada por $n_1$ porteros. Cada uno de ellos realizará un juicio sobre el cliente basado en sus características $\\boldsymbol{x}$. Aquí hay que tenerlo claro: se trata de un primer filtro formado por un conjunto de porteros. Cada uno de estos porteros es una neurona, y el conjunto de ellos forman la primera capa intermedia.\n",
    "\n",
    "Cada portero observará el valor de los $x_i$; y cada uno le da una cierta importancia a cada valor. Por ejemplo, un portero puede considerar que es más importante la edad que la hora de llegada; otro portero puede considerar que es más importante la actitud que la edad.\n",
    "\n",
    "Así, el portero $j$ de esta primera capa intermedia asigna ciertas importancias a cada característica: \n",
    "\n",
    "- $w_{1,1}^{(j)}$ es la importancia que el portero $j$ de la capa 1 asigna al valor $x_1$\n",
    "- $w_{1,2}^{(j)}$ es la importancia que el portero $j$ de la capa 1 asigna al valor $x_2$ \n",
    "- $w_{1,3}^{(j)}$ es la importancia que el portero $j$ de la capa 1 asigna al valor $x_3$\n",
    "- ...\n",
    "- $w_{1,n}^{(j)}$ es la importancia que el portero $j$ de la capa 1 asigna al valor $x_n$\n",
    "\n",
    "Y la forma en que este portero valora al cliente es $$z_{1}^{(j)}=w_{1,1}^{(j)}x_1+w_{1,2}^{(j)}x_2+w_{1,3}^{(j)}x_3+...+w_{1,n}^{(j)}x_n+b_1^{(j)}$$\n",
    "\n",
    "En la ecuación anterior, $b_1^{(j)}$ es el **sesgo** del portero $j$ de la capa 1. Este número representa un \"umbral de la personalidad del portero\". Se trata del nivel de exigencia de dicho portero, independientemente de la persona que llegue. Hay porteros naturalmente estrictos y hay porteros naturalmente flexibles. Antes de ver a nadie, ya traen una inclinación: si $b_1^{(j)}$ es grande, significa que el portero es muy exigente. En caso contrario, el portero es muy permisivo.\n",
    "\n",
    "De esta manera, $$z_{1}^{(j)}=w_{1,1}^{(j)}x_1+w_{1,2}^{(j)}x_2+w_{1,3}^{(j)}x_3+...+w_{1,n}^{(j)}x_n+b_1^{(j)}$$ significa \"Tomo lo que veo de la persona, ponderado por mis criterios, y además parto de una postura inicial\".\n",
    "\n",
    "Por simplicidad, denotemos lo anterior como $$z_{1}^{(j)}=\\boldsymbol{w}_{1}^{(j)}\\cdot x+b_{1}^{(j)}$$\n",
    "\n",
    "De alguna manera, $z_{1}^{(j)}$ es **lo que piensa el portero del cliente**. Pero ahora recordemos que hay $n_1$ porteros en esta primera capa intermedia. Por lo tanto hay $n_1$ juicios: $z_{1}^{(1)}$, $z_{1}^{(2)}$,..., $z_{1}^{(n_1)}$. Estos son solo pensamientos de los porteros. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81842c56-206c-42d8-b114-d74ea4d3ab84",
   "metadata": {},
   "source": [
    "Por lo tanto, como tenemos $n_1$ ecuaciones lineales, podemos definir la matriz $W_1$ como \n",
    "\n",
    "$$W_1=\\left(\\begin{matrix}w_{1,1}^{(1)}&w_{1,2}^{(1)}&w_{1,3}^{(1)}&\\dots&w_{1,n}^{(1)}\\\\\n",
    "w_{1,1}^{(2)}&w_{1,2}^{(2)}&w_{1,3}^{(2)}&\\dots&w_{1,n}^{(2)}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "w_{1,1}^{(n_1)}&w_{1,2}^{(n_1)}&w_{1,3}^{(n_1)}&\\dots&w_{1,n}^{(n_1)}\\\\\\end{matrix}\\right)$$\n",
    "\n",
    "y si $\\boldsymbol{b}_1=(b_1^{(1)},b_1^{(2)},...,b_{1}^{(n_1)})$ son los sesgos de cada portero de esta primera capa, entonces todos los pensamientos de esta primera capa intermedia son $$\\boldsymbol{z}_1=W_1\\boldsymbol{x}+\\boldsymbol{b}_1$$\n",
    "\n",
    "A continuación, cada uno de estos porteros enviará su valoración a los porteros del siguiente filtro (es decir, la segunda capa). Esa valoración no es el pensamiento como tal. Digamos que el gerente ha dado una regla para toda esta primera capa: dependiendo de tu valoración (es decir, $z_1^{(j)}$) **aplicarás una fórmula y el resultado se lo pasarás a la segunda capa**.\n",
    "\n",
    "Matemáticamente, esto significa que los porteros de la primera capa no pasan sus pensamientos, sino el resultado de aplicar una fórmula a sus pensamientos. Si denotamos por $f_1$ a dicha fórmula (el 1 es porque se trata de la fórmula de la primera capa intermedia) y $a_1^{(j)}$ es el resultado de aplicar la fórmula $f_1$ al pensamiento del portero $j$ de la capa 1, entonces $$a_1^{(j)}=f_1(z_1^{(j)})$$\n",
    "\n",
    "Por lo tanto obtenemos $n_1$ resultados (uno por cada portero de la primera capa). Si denotamos por $\\boldsymbol{a}_1$ estos resultados, entonces $$\\boldsymbol{a}_1=f_1(\\boldsymbol{z}_1)=f_1(W_1\\boldsymbol{x}+\\boldsymbol{b}_1)$$\n",
    "\n",
    "Esta es la información que fluirá desde la capa 1 hacia la capa 2. **Este proceso se repetirá entre todas las capas intermedias.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d6519c-7c83-4511-99e9-4e50b0b266d5",
   "metadata": {},
   "source": [
    "## Otras capas intermedias\n",
    "\n",
    "Dicho lo anterior, conservando la notación, el proceso se vuelve iterativo. Digamos que tenemos las capas $r$ y $r+1$, cada una $n_r$ y $n_{r+1}$ neuronas, respectivamente.\n",
    "\n",
    "La capa $r$ ha generado entonces los $n_r$ pensamientos $\\boldsymbol{z}_r=(z_r^{(1)},z_r^{(2)},...,z_r^{(n_r)})$. Se aplica una fórmula $f_r$ a cada uno de esos pensamientos y se obtienen $n_r$ resultados $\\boldsymbol{a}_r=(a_r^{(1)},a_r^{(2)},...,a_r^{(n_r)})$. Estos $n_r$ resultados son enviados a las $n_{r+1}$ neuronas de la siguiente capa. \n",
    "\n",
    "Cada una estas neuronas tiene sus propias \"importancias\" (pesos) para cada uno de los $n_r$ resultados que recibe: la neurona $j$ de la capa $n_{r+1}$ tiene pesos $\\boldsymbol{w}_{r+1}^{(j)}=(w_{r+1,1}^{(j)},w_{r+1,2}^{(j)},...,w_{r+1,n_r}^{(j)})$ y forma su propio pensamiento $z_{r+1}^{(j)}$ como $$z_{r+1}^{(j)}=w_{r+1,1}^{(j)}a_r^{(1)}+w_{r+1,2}^{(j)}a_r^{(2)}+...+w_{r+1,n_r}^{(j)}a_r^{(n_r)}+b_{r+1}^{(j)}=\\boldsymbol{w}_{r+1}^{(j)}\\cdot\\boldsymbol{a}_r+b_{r+1}^{(j)}$$\n",
    "\n",
    "Si los pesos de toda la capa son $$W_{r+1}=\\left(\\begin{matrix}w_{r+1,1}^{(1)}&w_{r+1,2}^{(1)}&\\dots&w_{r+1,n_r}^{(1)}\\\\\n",
    "w_{r+1,1}^{(2)}&w_{r+1,2}^{(2)}&\\dots&w_{r+1,n_r}^{(2)}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "w_{r+1,1}^{(n_{r+1})}&w_{r+1,2}^{(n_{r+1})}&\\dots&w_{r+1,n_r}^{(n_{r+1})}\\end{matrix}\\right)$$\n",
    "\n",
    "y los sesgos de cada neurona de esa capa son $\\boldsymbol{b}_{r+1}=(b_{r+1}^{(1)},b_{r+1}^{(2)},...,b_{r+1}^{(n_{r+1})})$ entonces los pensamientos en la capa $r+1$ son $$\\boldsymbol{z}_{r+1}=W_{r+1}\\boldsymbol{a}_r+\\boldsymbol{b}_{r+1}$$ y, tomando como **fórmula de activación** a $f_{r+1}$ entonces   $$\\boldsymbol{a}_{r+1}=f_{r+1}(\\boldsymbol{z}_{r+1})$$\n",
    "\n",
    "De esta manera **el Flujo de información** se puede resumir así: \n",
    "Entrada → Pesos y sesgo → Juicio crudo z → Activación a → Entrada para siguiente capa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a5802-ceca-4709-88af-7a17ebeed81d",
   "metadata": {},
   "source": [
    "## Capa de salida\n",
    "\n",
    "Supongamos que la última capa intermedia es la capa es $R$ y tiene $n_R$ neuronas. Esta capa ya produjo su vector de activaciones $\\boldsymbol{a}_{R}=(a_R^{(1)},a_R^{(2)},...,a_R^{(n_R)})$. Este vector contiene toda la información procesada del cliente: no son ya características originales, sino criterios abstractos aprendidos por la red. Ese vector entra completo a la capa de salida.\n",
    "\n",
    "Digamos que la capa de salida tiene $n_{out}$ neuronas. Cada una representa una posible decisión final:\n",
    "- $n_{out}=1$ cuando el problema es de clasificación binaria.\n",
    "- $n_{out}=k$ cuando el problema es de clasificación múltiple en $k$ categorías.\n",
    "- $n_{out}$ puede ser 1 o más cuando se trata de un problema de regresión.\n",
    "\n",
    "Cada neurona de salida funciona igual que las anteriores: forma un pensamiento lineal a partir de $\\boldsymbol{a}_R$.\n",
    "\n",
    "La neurona $j$ de la capa de salida tiene sus pesos $\\boldsymbol{w}_{out}^{(j)}$ y calcula su propio pensamiento crudo $$z_{out}^{(j)}=\\boldsymbol{w}_{out}^{(j)}\\cdot\\boldsymbol{a}_R+b_{out}^{(j)}$$\n",
    "\n",
    "Como hay $n_{out}$ neuronas en la capa de salida, entonces tenemos $n_{out}$ pensamientos crudos $\\boldsymbol{z}_{out}=(z_{out}^{(1)},z_{out}^{(2)},...,z_{out}^{(n_{out})})$ y tomando $W_{out}$ como la matriz de pesos (recuerda que la primer fila son las importancias de la primer neurona de la capa de salida; la segunda fila son las importancias de la segunda neurona de la capa de salida; etc) y $\\boldsymbol{b}_{out}$ como los sesgos, entonces $$\\boldsymbol{z}_{out}=W_{out}\\boldsymbol{a}_R+\\boldsymbol{b}_{out}$$\n",
    "\n",
    "Ahora ocurre algo importante: el gerente va a tomar una decisión. Esto lo hará basado en la fórmula con la que transforma los pensamientos crudos de la capa de salida en pensamientos procesados: $$\\boldsymbol{\\hat{y}}=\\boldsymbol{a}_{out}=f_{out}(\\boldsymbol{z}_{out})$$\n",
    "\n",
    "Este resultado $\\boldsymbol{\\hat{y}}$ es lo que finalmente la red neuronal entrega.\n",
    "\n",
    "Notemos que si vemos todo el flujo desde la llegada del cliente a la fila hasta el resultado final, tenemos:\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}}=f_{out}(W_{out}\\boldsymbol{a}_R+\\boldsymbol{b}_R)=f_{out}(W_{out}f_R(W_{R-1}\\boldsymbol{a}_{R-1}+\\boldsymbol{b}_{R-1})+\\boldsymbol{b}_R)=...=f_{out}(W_{out}f_R(W_R...f_1(W_1\\boldsymbol{x}+\\boldsymbol{b}_1)...+\\boldsymbol{b}_R)+\\boldsymbol{b}_{out})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb362c-2b8d-4685-8ed9-910fb5edfedf",
   "metadata": {},
   "source": [
    "## Ejemplo.\n",
    "\n",
    "Supongamos una arquitectura 3-2-3-4. Es decir, tres neuronas de entrada, 2 capas intermedias con 2 y 3 neuronas cada una, y 4 neuronas de capa final. Primero veamos cuántos parámetros vamos a requerir:\n",
    "\n",
    "- Como hay tres neuronas en la capa inicial, entonces cada una de las dos neuronas de la capa 1 (la primer capa intermedia) debe tener tres importancias. Además cada una tiene su propio sesgo. Esto son $3\\cdot2+2=8$ \n",
    "\n",
    "- Como hay dos neuronas en la capa 1, entonces cada una de las tres neuronas de la capa 2 (la segunda capa intermedia) debe tener dos importancias. Además cada una tiene su propio sesgo. Esto son $2\\cdot3+3=9$ \n",
    "\n",
    "- Como hay tres neuronas en la capa 2, entonces cada una de las cuatro neuronas de la capa final debe tener dos importancias. Además cada una tiene su propio sesgo. Esto son $3\\cdot4+4=16$ \n",
    "\n",
    "Por lo tanto hay $8+9+16=33$ parámetros. De hecho, en general hay $$\\sum_{k=0}^Rn_{k+1}(n_k+1)$$ parámetros donde $n_0$ es el total de neuronas de la capa de entrada, $n_1,n_2,...,n_R$ son el total de neuronas de cada capa intermedia y $n_{R+1}$ es el total de neuronas en la capa de salida, por lo que en nuestro ejemplo tenemos $2(3+1)+3(2+1)+4(3+1)=8+9+16=33$ parámetros.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc88ed-b8f1-4efb-a2dc-7ed0bd414480",
   "metadata": {},
   "source": [
    "Digamos que la CAPA 1 (Oculta 1) tiene estos 8 parámetros\n",
    "  - $\\boldsymbol{w}_{1}^{(1)}=(0.5, -0.2, 0.8)$ y sesgo $b_1^{(1)}=0.1$\n",
    "  - $\\boldsymbol{w}_{1}^{(2)}=(-0.3, 0.7, 0.4)$ y sesgo $b_1^{(2)}=-0.5$\n",
    "\n",
    "Por lo tanto $W_1=\\left(\\begin{matrix}0.5&-0.2&0.8\\\\-0.3&0.7&0.4\\end{matrix}\\right)$ y $\\boldsymbol{b}_1=(0.1,-0.5)$. Digamos que el gerente de la red neuronal le dice a los porteros de esta capa que la fórmula de transformación de pensamientos crudos es $f_1(t)=t^2$\n",
    "\n",
    "CAPA 2 (Oculta 2): 9 parámetros\n",
    "  - $\\boldsymbol{w}_{2}^{(1)}=(0.9, -0.6)$ y sesgo $b_2^{(1)}=0.2$\n",
    "  - $\\boldsymbol{w}_{2}^{(2)}=(-0.1, 0.3)$ y sesgo $b_2^{(2)}=-0.4$\n",
    "  - $\\boldsymbol{w}_{2}^{(3)}=(0.5, -0.8)$ y sesgo $b_2^{(3)}=-0.3$\n",
    "\n",
    "Por lo tanto $W_2=\\left(\\begin{matrix}0.9&-0.6\\\\-0.1&0.3\\\\0.5&-0.8\\end{matrix}\\right)$ y $\\boldsymbol{b}_2=(0.2,-0.4,-0.3)$. Digamos que el gerente de la red neuronal le dice a los porteros de esta capa que la fórmula de transformación de pensamientos crudos es $f_2(t)=\\cos(t)$\n",
    "  \n",
    "CAPA de Salida - 16 parámetros:\n",
    "  - $\\boldsymbol{w}_{out}^{(1)}=(0.7, -0.4, 0.1)$ y sesgo $b_{out}^{(1)}=0$\n",
    "  - $\\boldsymbol{w}_{out}^{(2)}=(-0.2, 0.5, -0.6)$ y sesgo $b_{out}^{(2)}=0.3$\n",
    "  - $\\boldsymbol{w}_{out}^{(3)}=(0.4, 0.8, -0.9)$ y sesgo $b_{out}^{(3)}=-0.1$\n",
    "  - $\\boldsymbol{w}_{out}^{(4)}=(-0.5, 0.2, 0.6)$ y sesgo $b_{out}^{(4)}=0.7$  \n",
    "\n",
    "Por lo tanto $W_{out}=\\left(\\begin{matrix}0.7&-0.4&0.1\\\\-0.2&0.5&-0.6\\\\0.4&0.8&-0.9\\\\-0.5&0.2&0.6\\\\\\end{matrix}\\right)$ y $\\boldsymbol{b}_{out}=(0,0.3,-0.1,0.7)$. Digamos que el gerente de la red neuronal le dice a los porteros de esta capa que la fórmula de transformación de pensamientos crudos es $f_{out}(t)=100t$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eec1d0-9c08-4a71-9028-9cc968762ce3",
   "metadata": {},
   "source": [
    "Con todo esto, ya tenemos una red neuronal completamente construida. Pensemos que llega un cliente $\\boldsymbol{x}$ con características $\\boldsymbol{x}=(2,1,5)$. ¿Cuál será el resultado de aplicarle la red neuronal? Como la capa de salida tiene 4 neuronas, esperamos 4 valores finales.\n",
    "\n",
    "- **Pensamiento crudo y procesado de la capa 1**\n",
    "\n",
    "$$\\boldsymbol{z}_1=W_1\\boldsymbol{x}+\\boldsymbol{b}_1=\\left(\\begin{matrix}0.5&-0.2&0.8\\\\-0.3&0.7&0.4\\end{matrix}\\right)\\left(\\begin{matrix}2\\\\1\\\\5\\end{matrix}\\right)+\\left(\\begin{matrix}0.1\\\\-0.5\\end{matrix}\\right)=\\left(\\begin{matrix}4.9\\\\1.6\\end{matrix}\\right)$$\n",
    "\n",
    "$$\\boldsymbol{a}_1=f_1(\\boldsymbol{z}_1)=\\left(\\begin{matrix}4.9^2\\\\1.6^2\\end{matrix}\\right)=\\left(\\begin{matrix}24.01\\\\2.56\\end{matrix}\\right)$$\n",
    "\n",
    "- **Pensamiento crudo y procesado de la capa 2**\n",
    "\n",
    "$$\\boldsymbol{z}_2=W_2\\boldsymbol{a}_1+\\boldsymbol{b}_2=\\left(\\begin{matrix}0.9&-0.6\\\\-0.1&0.3\\\\0.5&-0.8\\end{matrix}\\right)\\left(\\begin{matrix}24.01\\\\2.56\\end{matrix}\\right)+\\left(\\begin{matrix}0.2\\\\-0.4\\\\-0.3\\end{matrix}\\right)=\\left(\\begin{matrix}20.273\\\\-2.033\\\\9.657\\end{matrix}\\right)$$\n",
    "\n",
    "$$\\boldsymbol{a}_2=f_2(\\boldsymbol{z}_2)=\\left(\\begin{matrix}\\cos(20.273)\\\\\\cos(-2.033)\\\\\\cos(9.657)\\end{matrix}\\right)=\\left(\\begin{matrix}0.148\\\\-0.448\\\\-0.973\\end{matrix}\\right)$$\n",
    "\n",
    "- **Pensamiento crudo y procesado de la capa final**\n",
    "\n",
    "$$\\boldsymbol{z}_{out}=W_{out}\\boldsymbol{a}_2+\\boldsymbol{b}_{out}=\\left(\\begin{matrix}0.7&-0.4&0.1\\\\-0.2&0.5&-0.6\\\\0.4&0.8&-0.9\\\\-0.5&0.2&0.6\\\\\\end{matrix}\\right)\\left(\\begin{matrix}0.148\\\\-0.448\\\\-0.973\\end{matrix}\\right)+\\left(\\begin{matrix}0\\\\0.3\\\\-0.1\\\\0.7\\end{matrix}\\right)=\\left(\\begin{matrix}0.186\\\\0.630\\\\0.477\\\\-0.047\\end{matrix}\\right)$$ $$\\boldsymbol{\\hat{y}}=\\boldsymbol{a}_{out}=f_{out}(\\boldsymbol{z}_{out})=\\left(\\begin{matrix}100*0.186\\\\100*0.630\\\\100*0.477\\\\100*-0.047\\end{matrix}\\right)=\\left(\\begin{matrix}18.6\\\\63\\\\47.7\\\\-4.7\\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb5d32-4ca2-4d1e-b41f-c276dad1f58a",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Hasta ahora hemos descrito cómo fluye la información hacia adelante: el cliente entra, su información pasa por etapas (las capas) donde se toman juicios intermedios y finalmente el gerente da una decisión. Pero una red neuronal no es interesante solo por decidir, sino por aprender a decidir mejor con el tiempo. Supongamos que el gerente toma una decisión final sobre el cliente, como dejarlo entrar, no dejarlo entrar, o asignarle cierto valor numérico (por ejemplo, cuánto gastará).\n",
    "\n",
    "Sin embargo, existe una decisión correcta esperada. Tal vez el cliente sí debía entrar, o no, o gastar más o menos. Esa información llega después. Aquí aparece una pregunta clave: ¿Qué tan buena fue la decisión que tomó la red?\n",
    "\n",
    "### Función de pérdida\n",
    "\n",
    "Para responder eso, el gerente compara lo que la red decidió, $\\boldsymbol{\\hat{y}}$, con lo que debía haber pasado, $\\boldsymbol{y}$. A partir de esta comparación se define una función de pérdida, que denotaremos por $\\mathcal{L}$. Esto es $$\\mathcal{L}(\\boldsymbol{y},\\boldsymbol{\\hat{y}})$$\n",
    "\n",
    "Este número mide qué tan mala fue la decisión. Si es grande, la red se equivocó mucho; si es pequeño, la red lo hizo bien.\n",
    "\n",
    "### ¿Quién tuvo la culpa del error?\n",
    "\n",
    "Aquí viene la idea central del backpropagation. La decisión final fue incorrecta, pero ¿fue culpa del último portero?, ¿de alguno intermedio?, ¿de un portero muy al inicio que exageró la importancia de la estatura?, ¿de un sesgo demasiado estricto?\n",
    "\n",
    "La red necesita repartir la **responsabilidad del error** entre todos los pesos y todos los sesgos de todas las capas. Matemáticamente, pérdida $\\mathcal{L}$ depende de $\\boldsymbol{\\hat{y}}$, la cual depende de $\\boldsymbol{a}_R$ (la última capa intermedia), la cual depende de $\\boldsymbol{a}_{R-1}$ (la penúltima capa intermedia) y así sucesivamente.\n",
    "\n",
    "Es decir, \n",
    "\n",
    "$$\\mathcal{L}=\\mathcal{L}(\\boldsymbol{y},\\textcolor{red}{\\boldsymbol{\\hat{y}}})=\\mathcal{L}(\\boldsymbol{y},\\textcolor{red}{f_{out}(W_{out}f_R(W_R...f_1(W_1\\boldsymbol{x}+\\boldsymbol{b}_1)...+\\boldsymbol{b}_R)+\\boldsymbol{b}_{out})})$$\n",
    "\n",
    "Ahora se usa la regla de la cadena para responder \"si cambio un poco este peso o este sesgo, ¿cuánto cambia el error final?\"\n",
    "\n",
    "Para cada peso $w_{i,s}^{(j)}$ y cada sesgo $b_{i}^{(j)}$, se calculan $$\\frac{\\partial\\mathcal{L}}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "Puntualmente, esto representa \"¿cuánto es que la importancia $s$ de la neurona $j$ de la capa $i$ y su sesgo afectan al resultado final?\". Si el valor absoluto de la derivada es grande, estos parámetros influyen mucho en el error. Si es pequeña, su inluencia es baja.\n",
    "\n",
    "## Ajuste de pesos\n",
    "\n",
    "Una vez calculadas estas responsabilidades, cada peso y sesgo se ajusta ligeramente:\n",
    "\n",
    "$$w_{i,s}^{(j)}\\leftarrow w_{i,s}^{(j)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }b_{i}^{(j)}\\leftarrow b_{i}^{(j)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "donde $\\eta>0$ es la **tasa de aprendizaje** (qué tanto le dice el gerente a la neurona que debe mejorar).\n",
    "\n",
    "Así, si un portero exageró un criterio, se le baja un poco; si ignoró algo importante, se le sube un poco; los porteros estrictos pueden volverse más flexibles y viceversa. Este proceso se repite con muchos clientes, y poco a poco la red afina sus criterios, aprende patrones, y mejora sus decisiones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73249e8-d9d3-4750-8eee-62b73bd19a38",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "\n",
    "Teníamos $\\boldsymbol{x}=(2,1,5)$ y nos produjo $\\boldsymbol{\\hat{y}}=(18.6,63,47.7,-4.7)$. Supongamos que por alguna razón sabemos que para esta $\\boldsymbol{x}$ se debió tener $\\boldsymbol{y}=(20,60,40,0)$.\n",
    "\n",
    "Digamos que el error cometido lo medimos como $$\\mathcal{L}(\\boldsymbol{y},\\boldsymbol{\\hat{y}})=\\frac{1}{2}((y_1-\\hat{y}_1)^2+(y_2-\\hat{y}_2)^2+(y_3-\\hat{y}_3)^2+(y_4-\\hat{y}_4)^2)$$\n",
    "\n",
    "Tomemos como tasa de aprendizaje $\\eta=0.001$.\n",
    "\n",
    "Para esta pérdida, $$\\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}_i}=\\hat{y}_i-y_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b8ed6-b125-4c87-9a9d-3fce1849e89d",
   "metadata": {},
   "source": [
    "Vemamos cómo actualizar los pesos y el sesgo de la segunda neurona de la capa de salida. Para esto, como la última capa intermedia tiene 3 neuronas, entonces para $s=1,2,3$ se debe calcular\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial w_{out,s}^{(2)}}\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}}{\\partial b_{out}^{(2)}}$$\n",
    "\n",
    "Por una parte, por la Regla de la Cadena\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial w_{out,s}^{(2)}}=\\textcolor{red}{\\frac{\\partial\\mathcal{L}}{\\partial z_{out}^{(2)}}}\\,\\textcolor{blue}{\\frac{\\partial z_{out}^{(2)}}{\\partial w_{out,s}^{(2)}}}=\\textcolor{red}{\\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}_2}\\,\\frac{\\partial \\hat{y}_2}{\\partial z_{out}^{(2)}}}\\,\\textcolor{blue}{\\frac{\\partial z_{out}^{(2)}}{\\partial w_{out,s}^{(2)}}}\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}}{\\partial b_{out}^{(2)}}=\\textcolor{red}{\\frac{\\partial\\mathcal{L}}{\\partial z_{out}^{(2)}}}\\,\\textcolor{darkgreen}{\\frac{\\partial{z_{out}^{(2)}}}{\\partial b_{out}^{(2)}}}$$\n",
    "\n",
    "Por otra parte, $$\\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}_2}=\\hat{y}_2-y_2=63-60=3$$\n",
    "\n",
    "Como $\\hat{y}_2=a_{out}^{(2)}=f_{out}(z_{out}^{(2)})=100z_{out}^{(2)}$, entonces $$\\frac{\\partial\\hat{y}_2}{\\partial z_{out}^{(2)}}=100$$\n",
    "\n",
    "Con esto, $$\\textcolor{red}{\\frac{\\partial\\mathcal{L}}{\\partial z_{out}^{(2)}}}=3\\cdot100=300$$\n",
    "\n",
    "Ahora, por definición, $$z_{out}^{(2)}=w_{out,1}^{(2)}a_2^{(1)}+w_{out,2}^{(2)}a_2^{(2)}+w_{out,3}^{(2)}a_2^{(3)}+b_{out}^{(2)}$$\n",
    "\n",
    "Entonces $$\\textcolor{blue}{\\frac{\\partial z_{out}^{(2)}}{\\partial w_{out,s}^{(2)}}=a_2^{(s)}}\\quad\\mbox{ y }\\quad\\textcolor{darkgreen}{\\frac{\\partial z_{out}^{(2)}}{\\partial b_{out}^{(2)}}=1}$$ \n",
    "\n",
    "Conluimos que $$\\frac{\\partial\\mathcal{L}}{\\partial w_{out,s}^{(2)}}=3\\cdot100\\cdot a_2^{(s)}=\\left\\{\\begin{array}{l}300*0.148=44.4\\mbox{ para }s=1\\\\300*-0.448=-134.4\\mbox{ para }s=2\\\\300\\cdot-0.973=-291.9\\mbox{ para }s=3\\end{array}\\right.\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}}{\\partial b_{out}^{(2)}}=300\\cdot1=300$$\n",
    "\n",
    "Luego, los nuevos parámetros de neurona 2 de la capa de salida son \n",
    "\n",
    "$$w_{out,1}^{(2)}=-0.2-0.001*44.4=-0.2444$$\n",
    "$$w_{out,2}^{(2)}=0.5-0.001*-134.4=0.6344$$\n",
    "$$w_{out,3}^{(2)}=-0.6-0.001*-291.9=-0.3081$$\n",
    "$$b_{out}^{(2)}=0.3-0.001*300=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2aa45b-f218-4aa3-8197-28bd83dc813d",
   "metadata": {},
   "source": [
    "De hecho, la nueva matriz de pesos y nuevos sesgos de la capa de salida son\n",
    "$$\n",
    "\\mbox{nuevo }W_{out} = \n",
    "\\begin{pmatrix}\n",
    "0.720720 & -0.462720 & -0.036220 \\\\\n",
    "-0.244400 & 0.634400 & -0.308100 \\\\\n",
    "0.286040 & 1.144960 & -0.150790 \\\\\n",
    "-0.430440 & -0.010560 & 0.142690\n",
    "\\end{pmatrix}\n",
    "\\quad\\mbox{ y nuevo }\\boldsymbol{b}_{out}=\\begin{pmatrix}\n",
    "0.140 \\\\ \n",
    "0.000 \\\\ \n",
    "-0.870 \\\\ \n",
    "1.170\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b13379-da8d-4177-ab5b-f41e1ce9a41d",
   "metadata": {},
   "source": [
    "Ahora, veamos qué pasa con la primera neurona de la capa 2 (la última capa intermedia). La función de activación de esta capa es\n",
    "\n",
    "$$\n",
    "f_2(t)=\\cos(t)\n",
    "$$\n",
    "\n",
    "y queremos calcular, para $s=1,2$,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial w_{2,s}^{(1)}}\n",
    "\\qquad \\text{y} \\qquad\n",
    "\\frac{\\partial \\mathcal L}{\\partial b_2^{(1)}}\n",
    "$$\n",
    "\n",
    "Los parámetros $w_{2,s}^{(1)}$ y $b_2^{(1)}$ no influyen directamente en la pérdida. Su efecto se propaga a través del siguiente camino:\n",
    "\n",
    "$$\n",
    "(w_{2,s}^{(1)},\\, b_2^{(1)})\n",
    "\\longrightarrow\n",
    "z_2^{(1)}\n",
    "\\longrightarrow\n",
    "a_2^{(1)}\n",
    "\\longrightarrow\n",
    "z_{out}^{(j)} \\quad j=1,2,3,4\n",
    "\\longrightarrow\n",
    "\\hat y_j\n",
    "\\longrightarrow\n",
    "\\mathcal L\n",
    "$$\n",
    "\n",
    "La activación $a_2^{(1)}$ alimenta a **todas** las neuronas de salida. \n",
    "\n",
    "Para cualquier parámetro $\\theta \\in \\{ w_{2,s}^{(1)},\\, b_2^{(1)} \\}$ se tiene\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial \\theta}\n",
    "=\n",
    "\\sum_{j=1}^{4}\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(j)}}\n",
    "\\frac{\\partial z_{out}^{(j)}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "El parámetro $\\theta$ no aparece explícitamente en $z_{out}^{(j)}$, sino a través de la activación $a_2^{(1)}$.\n",
    "\n",
    "Por lo tanto\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{out}^{(j)}}{\\partial \\theta}\n",
    "=\n",
    "\\frac{\\partial z_{out}^{(j)}}{\\partial a_2^{(1)}}\n",
    "\\frac{\\partial a_2^{(1)}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Sustituyendo\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial \\theta}\n",
    "=\n",
    "\\sum_{j=1}^{4}\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(j)}}\n",
    "\\frac{\\partial z_{out}^{(j)}}{\\partial a_2^{(1)}}\n",
    "\\frac{\\partial a_2^{(1)}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "La activación de la neurona es\n",
    "\n",
    "$$\n",
    "a_2^{(1)} = f_2(z_2^{(1)}) = \\cos(z_2^{(1)})\n",
    "$$\n",
    "\n",
    "Por la Regla de la Cadena\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_2^{(1)}}{\\partial \\theta}\n",
    "=\n",
    "\\frac{\\partial a_2^{(1)}}{\\partial z_2^{(1)}}\n",
    "\\frac{\\partial z_2^{(1)}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "y como\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt}\\cos(t) = -\\sin(t)\n",
    "$$\n",
    "\n",
    "se tiene\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_2^{(1)}}{\\partial z_2^{(1)}} = -\\sin(z_2^{(1)})\n",
    "$$\n",
    "\n",
    "Sustituyendo nuevamente\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial \\theta}\n",
    "=\n",
    "\\sum_{j=1}^{4}\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(j)}}\n",
    "\\frac{\\partial z_{out}^{(j)}}{\\partial a_2^{(1)}}\n",
    "\\left(-\\sin(z_2^{(1)})\\right)\n",
    "\\frac{\\partial z_2^{(1)}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "El pensamiento crudo de la neurona $j$ de salida es\n",
    "\n",
    "$$\n",
    "z_{out}^{(j)}\n",
    "=\n",
    "\\sum_{k=1}^{3} w_{out,k}^{(j)} a_2^{(k)} + b_{out}^{(j)}\n",
    "$$\n",
    "\n",
    "Por lo tanto\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{out}^{(j)}}{\\partial a_2^{(1)}} = w_{out,1}^{(j)}\n",
    "$$\n",
    "\n",
    "El pensamiento crudo de la primera neurona de la capa 2 es\n",
    "\n",
    "$$\n",
    "z_2^{(1)}\n",
    "=\n",
    "\\sum_{r=1}^{n_1} w_{2,r}^{(1)} a_1^{(r)} + b_2^{(1)}\n",
    "$$\n",
    "\n",
    "- Para un peso $w_{2,s}^{(1)}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_2^{(1)}}{\\partial w_{2,s}^{(1)}} = a_1^{(s)}\n",
    "$$\n",
    "\n",
    "- Para el sesgo $b_2^{(1)}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_2^{(1)}}{\\partial b_2^{(1)}} = 1\n",
    "$$\n",
    "\n",
    "Gradiente respecto a cualquier peso $w_{2,s}^{(1)}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial w_{2,s}^{(1)}}\n",
    "=\n",
    "\\left(\n",
    "\\sum_{j=1}^{n_{out}}\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(j)}}\n",
    "\\, w_{out,1}^{(j)}\n",
    "\\right)\n",
    "\\left(-\\sin(z_2^{(1)})\\right)\n",
    "a_1^{(s)}\n",
    "$$\n",
    "\n",
    "Gradiente respecto al sesgo $b_2^{(1)}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial b_2^{(1)}}\n",
    "=\n",
    "\\left(\n",
    "\\sum_{j=1}^{n_{out}}\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(j)}}\n",
    "\\, w_{out,1}^{(j)}\n",
    "\\right)\n",
    "\\left(-\\sin(z_2^{(1)})\\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
