{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVxEegVlr0Sv"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# NUEVA NOTEBOOK — CELDA ÚNICA: Cargar resultados.zip y predecir un ZIP nuevo\n",
        "# Entradas (subidas manualmente a /content):\n",
        "#   - /content/resultados.zip   (bundle del modelo + metadata)\n",
        "#   - /content/nuevo.zip        (nuevo dataset para inferencia)\n",
        "# Salidas:\n",
        "#   - /content/predicciones.csv\n",
        "# ==========================================================\n",
        "import os, glob, json, zipfile, shutil, time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# -------------------------\n",
        "# 0) RUTAS\n",
        "# -------------------------\n",
        "RESULTS_ZIP = \"/content/resultados.zip\"\n",
        "NEW_ZIP     = \"/content/simpson_nuevos.zip\"  # cambia el nombre si llega con otro nombre\n",
        "\n",
        "RESULTS_DIR = \"/content/resultados\"\n",
        "NEW_WORKDIR = \"/content/new_data\"\n",
        "\n",
        "EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "assert os.path.isfile(RESULTS_ZIP), \"Falta /content/resultados.zip (súbelo manualmente)\"\n",
        "assert os.path.isfile(NEW_ZIP),     \"Falta /content/nuevo.zip (súbelo manualmente)\"\n",
        "\n",
        "# -------------------------\n",
        "# 1) DESCOMPRIMIR resultados.zip\n",
        "# -------------------------\n",
        "if os.path.isdir(RESULTS_DIR):\n",
        "    shutil.rmtree(RESULTS_DIR)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(RESULTS_ZIP, \"r\") as z:\n",
        "    z.extractall(RESULTS_DIR)\n",
        "\n",
        "print(\"✅ resultados.zip extraído en:\", RESULTS_DIR)\n",
        "print(\"Contenido:\", sorted(os.listdir(RESULTS_DIR))[:20])\n",
        "\n",
        "MODEL_PATH = os.path.join(RESULTS_DIR, \"model.keras\")\n",
        "META_PATH  = os.path.join(RESULTS_DIR, \"metadata.json\")\n",
        "\n",
        "assert os.path.isfile(MODEL_PATH), f\"No existe {MODEL_PATH}\"\n",
        "assert os.path.isfile(META_PATH),  f\"No existe {META_PATH}\"\n",
        "\n",
        "# -------------------------\n",
        "# 2) CARGAR METADATA + MODELO\n",
        "# -------------------------\n",
        "with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "IMG_SIZE  = tuple(meta[\"img_size\"])\n",
        "CHANNELS  = int(meta[\"channels\"])\n",
        "CLASSES   = list(meta[\"classes\"])\n",
        "BATCH     = int(meta.get(\"batch_final\", 32))\n",
        "\n",
        "print(\"\\nCONFIG INFERENCIA:\")\n",
        "print(\"  IMG_SIZE :\", IMG_SIZE)\n",
        "print(\"  CHANNELS :\", CHANNELS)\n",
        "print(\"  BATCH    :\", BATCH)\n",
        "print(\"  #CLASSES :\", len(CLASSES))\n",
        "\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "print(\"\\n✅ Modelo cargado:\", MODEL_PATH)\n",
        "\n",
        "# -------------------------\n",
        "# 3) DESCOMPRIMIR NUEVO ZIP\n",
        "# -------------------------\n",
        "if os.path.isdir(NEW_WORKDIR):\n",
        "    shutil.rmtree(NEW_WORKDIR)\n",
        "os.makedirs(NEW_WORKDIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(NEW_ZIP, \"r\") as z:\n",
        "    z.extractall(NEW_WORKDIR)\n",
        "\n",
        "print(\"\\n✅ nuevo.zip extraído en:\", NEW_WORKDIR)\n",
        "\n",
        "# -------------------------\n",
        "# 4) ENCONTRAR TODAS LAS IMÁGENES (sin asumir carpetas por clase)\n",
        "#    (Soporta: imágenes en raíz, o en subcarpetas a cualquier profundidad)\n",
        "# -------------------------\n",
        "def list_all_images_recursive(root_dir, exts=EXTS):\n",
        "    files = []\n",
        "    for r, _, fs in os.walk(root_dir):\n",
        "        for fn in fs:\n",
        "            if fn.lower().endswith(exts):\n",
        "                files.append(os.path.join(r, fn))\n",
        "    return sorted(files)\n",
        "\n",
        "files = list_all_images_recursive(NEW_WORKDIR)\n",
        "if len(files) == 0:\n",
        "    raise ValueError(f\"No encontré imágenes dentro de {NEW_ZIP}\")\n",
        "\n",
        "print(\"Imágenes encontradas:\", len(files))\n",
        "print(\"Ejemplo:\", files[0])\n",
        "\n",
        "# -------------------------\n",
        "# 5) PIPELINE TF.DATA (mismo preprocesamiento que entrenaste)\n",
        "# -------------------------\n",
        "def decode_image(path, img_size, channels):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_image(img, channels=channels, expand_animations=False)\n",
        "    img = tf.image.resize(img, img_size, antialias=True)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "def make_ds(paths, batch):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    ds = ds.map(lambda p: decode_image(p, IMG_SIZE, CHANNELS), num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "ds = make_ds(files, BATCH)\n",
        "\n",
        "# -------------------------\n",
        "# 6) PREDICCIÓN + CSV\n",
        "# -------------------------\n",
        "probs = model.predict(ds, verbose=0)\n",
        "pred_idx = np.argmax(probs, axis=1)\n",
        "pred_cls = [CLASSES[i] for i in pred_idx]\n",
        "conf = np.max(probs, axis=1)\n",
        "\n",
        "out_csv = \"/content/predicciones.csv\"\n",
        "import csv\n",
        "with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"filepath\", \"pred_idx\", \"pred_class\", \"confidence\"])\n",
        "    for p, i, c, cf in zip(files, pred_idx, pred_cls, conf):\n",
        "        w.writerow([p, int(i), c, float(cf)])\n",
        "\n",
        "print(\"\\n✅ Listo. CSV:\", out_csv)\n",
        "print(\"Primeras 5 predicciones:\")\n",
        "for k in range(min(5, len(files))):\n",
        "    print(f\"  {os.path.basename(files[k])} -> {pred_cls[k]} (conf={conf[k]:.3f})\")\n"
      ]
    }
  ]
}