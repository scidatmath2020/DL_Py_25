{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8d7a10-2941-4f36-9a87-7ba7472c5b56",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d16f88-0460-4713-9236-dcafa2c7af6e",
   "metadata": {},
   "source": [
    "# Modelo matemático\n",
    "\n",
    "Vayamos ahora a las matemáticas que trabajan en una red neuronal.\n",
    "\n",
    "## Capa de entrada\n",
    "\n",
    "Supongamos que llega un cliente potencial al antro y se forma en la fila. El cliente tiene ciertas características: edad, peso, estatura, vestimenta, actitud, hora de llegada, etc. En ese momento **nadie juzga ni aprueba nada**. El cliente solamente está formado.\n",
    "\n",
    "En una red neuronal, esta fila es la capa de entrada. Las neuronas que la conforman se encargan únicamente de recabar la información del cliente formado. Esta información es un número. De esta manera, en una neurona entra la edad; en otra neurona entra el peso; en otra neurona entra la estatura; en otra neurona entra la vestimenta; etc.\n",
    "\n",
    "Así, si la capa de entrada tiene $n$ neuronas, entonces esta espera recibir $n$ números; llamemos $x_1$, $x_2$,..., $x_n$ a estos valores y denotemos por $x$ a $$\\boldsymbol{x}=(x_1,x_2,...,x_n)$$\n",
    "\n",
    "## Capas medias\n",
    "\n",
    "### Primera capa \n",
    "\n",
    "El cliente llega a una primera revisión formada por $n_1$ porteros. Cada uno de ellos realizará un juicio sobre el cliente basado en sus características $\\boldsymbol{x}$. Aquí hay que tenerlo claro: se trata de un primer filtro formado por un conjunto de porteros. Cada uno de estos porteros es una neurona, y el conjunto de ellos forman la primera capa intermedia.\n",
    "\n",
    "Cada portero observará el valor de los $x_i$; y cada uno le da una cierta importancia a cada valor. Por ejemplo, un portero puede considerar que es más importante la edad que la hora de llegada; otro portero puede considerar que es más importante la actitud que la edad.\n",
    "\n",
    "Así, el portero $j$ de esta primera capa intermedia asigna ciertas importancias a cada característica: \n",
    "\n",
    "- $w_{1,1}^{(j)}$ es la importancia que el portero $j$ de la capa 1 asigna al valor $x_1$\n",
    "- $w_{1,2}^{(j)}$ es la importancia que el portero $j$ de la capa 1 asigna al valor $x_2$ \n",
    "- $w_{1,3}^{(j)}$ es la importancia que el portero $j$ de la capa 1 asigna al valor $x_3$\n",
    "- ...\n",
    "- $w_{1,n}^{(j)}$ es la importancia que el portero $j$ de la capa 1 asigna al valor $x_n$\n",
    "\n",
    "Y la forma en que este portero valora al cliente es $$z_{1}^{(j)}=w_{1,1}^{(j)}x_1+w_{1,2}^{(j)}x_2+w_{1,3}^{(j)}x_3+...+w_{1,n}^{(j)}x_n+b_1^{(j)}$$\n",
    "\n",
    "En la ecuación anterior, $b_1^{(j)}$ es el **sesgo** del portero $j$ de la capa 1. Este número representa un \"umbral de la personalidad del portero\". Se trata del nivel de exigencia de dicho portero, independientemente de la persona que llegue. Hay porteros naturalmente estrictos y hay porteros naturalmente flexibles. Antes de ver a nadie, ya traen una inclinación: si $b_1^{(j)}$ es grande, significa que el portero es muy exigente. En caso contrario, el portero es muy permisivo.\n",
    "\n",
    "De esta manera, $$z_{1}^{(j)}=w_{1,1}^{(j)}x_1+w_{1,2}^{(j)}x_2+w_{1,3}^{(j)}x_3+...+w_{1,n}^{(j)}x_n+b_1^{(j)}$$ significa \"Tomo lo que veo de la persona, ponderado por mis criterios, y además parto de una postura inicial\".\n",
    "\n",
    "Por simplicidad, denotemos lo anterior como $$z_{1}^{(j)}=\\boldsymbol{w}_{1}^{(j)}\\cdot x+b_{1}^{(j)}$$\n",
    "\n",
    "De alguna manera, $z_{1}^{(j)}$ es **lo que piensa el portero del cliente**. Pero ahora recordemos que hay $n_1$ porteros en esta primera capa intermedia. Por lo tanto hay $n_1$ juicios: $z_{1}^{(1)}$, $z_{1}^{(2)}$,..., $z_{1}^{(n_1)}$. Estos son solo pensamientos de los porteros. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81842c56-206c-42d8-b114-d74ea4d3ab84",
   "metadata": {},
   "source": [
    "Por lo tanto, como tenemos $n_1$ ecuaciones lineales, podemos definir la matriz $W_1$ como \n",
    "\n",
    "$$W_1=\\left(\\begin{matrix}w_{1,1}^{(1)}&w_{1,2}^{(1)}&w_{1,3}^{(1)}&\\dots&w_{1,n}^{(1)}\\\\\n",
    "w_{1,1}^{(2)}&w_{1,2}^{(2)}&w_{1,3}^{(2)}&\\dots&w_{1,n}^{(2)}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "w_{1,1}^{(n_1)}&w_{1,2}^{(n_1)}&w_{1,3}^{(n_1)}&\\dots&w_{1,n}^{(n_1)}\\\\\\end{matrix}\\right)$$\n",
    "\n",
    "y si $\\boldsymbol{b}_1=(b_1^{(1)},b_1^{(2)},...,b_{1}^{(n_1)})$ son los sesgos de cada portero de esta primera capa, entonces todos los pensamientos de esta primera capa intermedia son $$\\boldsymbol{z}_1=W_1\\boldsymbol{x}+\\boldsymbol{b}_1$$\n",
    "\n",
    "A continuación, cada uno de estos porteros enviará su valoración a los porteros del siguiente filtro (es decir, la segunda capa). Esa valoración no es el pensamiento como tal. Digamos que el gerente ha dado una regla para toda esta primera capa: dependiendo de tu valoración (es decir, $z_1^{(j)}$) **aplicarás una fórmula y el resultado se lo pasarás a la segunda capa**.\n",
    "\n",
    "Matemáticamente, esto significa que los porteros de la primera capa no pasan sus pensamientos, sino el resultado de aplicar una fórmula a sus pensamientos. Si denotamos por $f_1$ a dicha fórmula (el 1 es porque se trata de la fórmula de la primera capa intermedia) y $a_1^{(j)}$ es el resultado de aplicar la fórmula $f_1$ al pensamiento del portero $j$ de la capa 1, entonces $$a_1^{(j)}=f_1(z_1^{(j)})$$\n",
    "\n",
    "Por lo tanto obtenemos $n_1$ resultados (uno por cada portero de la primera capa). Si denotamos por $\\boldsymbol{a}_1$ estos resultados, entonces $$\\boldsymbol{a}_1=f_1(\\boldsymbol{z}_1)=f_1(W_1\\boldsymbol{x}+\\boldsymbol{b}_1)$$\n",
    "\n",
    "Esta es la información que fluirá desde la capa 1 hacia la capa 2. **Este proceso se repetirá entre todas las capas intermedias.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d6519c-7c83-4511-99e9-4e50b0b266d5",
   "metadata": {},
   "source": [
    "## Otras capas intermedias\n",
    "\n",
    "Dicho lo anterior, conservando la notación, el proceso se vuelve iterativo. Digamos que tenemos las capas $r$ y $r+1$, cada una $n_r$ y $n_{r+1}$ neuronas, respectivamente.\n",
    "\n",
    "La capa $r$ ha generado entonces los $n_r$ pensamientos $\\boldsymbol{z}_r=(z_r^{(1)},z_r^{(2)},...,z_r^{(n_r)})$. Se aplica una fórmula $f_r$ a cada uno de esos pensamientos y se obtienen $n_r$ resultados $\\boldsymbol{a}_r=(a_r^{(1)},a_r^{(2)},...,a_r^{(n_r)})$. Estos $n_r$ resultados son enviados a las $n_{r+1}$ neuronas de la siguiente capa. \n",
    "\n",
    "Cada una estas neuronas tiene sus propias \"importancias\" (pesos) para cada uno de los $n_r$ resultados que recibe: la neurona $j$ de la capa $n_{r+1}$ tiene pesos $\\boldsymbol{w}_{r+1}^{(j)}=(w_{r+1,1}^{(j)},w_{r+1,2}^{(j)},...,w_{r+1,n_r}^{(j)})$ y forma su propio pensamiento $z_{r+1}^{(j)}$ como $$z_{r+1}^{(j)}=w_{r+1,1}^{(j)}a_r^{(1)}+w_{r+1,2}^{(j)}a_r^{(2)}+...+w_{r+1,n_r}^{(j)}a_r^{(n_r)}+b_{r+1}^{(j)}=\\boldsymbol{w}_{r+1}^{(j)}\\cdot\\boldsymbol{a}_r+b_{r+1}^{(j)}$$\n",
    "\n",
    "Si los pesos de toda la capa son $$W_{r+1}=\\left(\\begin{matrix}w_{r+1,1}^{(1)}&w_{r+1,2}^{(1)}&\\dots&w_{r+1,n_r}^{(1)}\\\\\n",
    "w_{r+1,1}^{(2)}&w_{r+1,2}^{(2)}&\\dots&w_{r+1,n_r}^{(2)}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "w_{r+1,1}^{(n_{r+1})}&w_{r+1,2}^{(n_{r+1})}&\\dots&w_{r+1,n_r}^{(n_{r+1})}\\end{matrix}\\right)$$\n",
    "\n",
    "y los sesgos de cada neurona de esa capa son $\\boldsymbol{b}_{r+1}=(b_{r+1}^{(1)},b_{r+1}^{(2)},...,b_{r+1}^{(n_{r+1})})$ entonces los pensamientos en la capa $r+1$ son $$\\boldsymbol{z}_{r+1}=W_{r+1}\\boldsymbol{a}_r+\\boldsymbol{b}_{r+1}$$ y, tomando como **fórmula de activación** a $f_{r+1}$ entonces   $$\\boldsymbol{a}_{r+1}=f_{r+1}(\\boldsymbol{z}_{r+1})$$\n",
    "\n",
    "De esta manera **el Flujo de información** se puede resumir así: \n",
    "Entrada → Pesos y sesgo → Juicio crudo z → Activación a → Entrada para siguiente capa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a5802-ceca-4709-88af-7a17ebeed81d",
   "metadata": {},
   "source": [
    "## Capa de salida\n",
    "\n",
    "Supongamos que la última capa intermedia es la capa es $R$ y tiene $n_R$ neuronas. Esta capa ya produjo su vector de activaciones $\\boldsymbol{a}_{R}=(a_R^{(1)},a_R^{(2)},...,a_R^{(n_R)})$. Este vector contiene toda la información procesada del cliente: no son ya características originales, sino criterios abstractos aprendidos por la red. Ese vector entra completo a la capa de salida.\n",
    "\n",
    "Digamos que la capa de salida tiene $n_{out}$ neuronas. Cada una representa una posible decisión final:\n",
    "- $n_{out}=1$ cuando el problema es de clasificación binaria.\n",
    "- $n_{out}=k$ cuando el problema es de clasificación múltiple en $k$ categorías.\n",
    "- $n_{out}$ puede ser 1 o más cuando se trata de un problema de regresión.\n",
    "\n",
    "Cada neurona de salida funciona igual que las anteriores: forma un pensamiento lineal a partir de $\\boldsymbol{a}_R$.\n",
    "\n",
    "La neurona $j$ de la capa de salida tiene sus pesos $\\boldsymbol{w}_{out}^{(j)}$ y calcula su propio pensamiento crudo $$z_{out}^{(j)}=\\boldsymbol{w}_{out}^{(j)}\\cdot\\boldsymbol{a}_R+b_{out}^{(j)}$$\n",
    "\n",
    "Como hay $n_{out}$ neuronas en la capa de salida, entonces tenemos $n_{out}$ pensamientos crudos $\\boldsymbol{z}_{out}=(z_{out}^{(1)},z_{out}^{(2)},...,z_{out}^{(n_{out})})$ y tomando $W_{out}$ como la matriz de pesos (recuerda que la primer fila son las importancias de la primer neurona de la capa de salida; la segunda fila son las importancias de la segunda neurona de la capa de salida; etc) y $\\boldsymbol{b}_{out}$ como los sesgos, entonces $$\\boldsymbol{z}_{out}=W_{out}\\boldsymbol{a}_R+\\boldsymbol{b}_{out}$$\n",
    "\n",
    "Ahora ocurre algo importante: el gerente va a tomar una decisión. Esto lo hará basado en la fórmula con la que transforma los pensamientos crudos de la capa de salida en pensamientos procesados: $$\\boldsymbol{\\hat{y}}=\\boldsymbol{a}_{out}=f_{out}(\\boldsymbol{z}_{out})$$\n",
    "\n",
    "Este resultado $\\boldsymbol{\\hat{y}}$ es lo que finalmente la red neuronal entrega.\n",
    "\n",
    "Notemos que si vemos todo el flujo desde la llegada del cliente a la fila hasta el resultado final, tenemos:\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}}=f_{out}(W_{out}\\boldsymbol{a}_R+\\boldsymbol{b}_R)=f_{out}(W_{out}f_R(W_{R-1}\\boldsymbol{a}_{R-1}+\\boldsymbol{b}_{R-1})+\\boldsymbol{b}_R)=...=f_{out}(W_{out}f_R(W_R...f_1(W_1\\boldsymbol{x}+\\boldsymbol{b}_1)...+\\boldsymbol{b}_R)+\\boldsymbol{b}_{out})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb362c-2b8d-4685-8ed9-910fb5edfedf",
   "metadata": {},
   "source": [
    "## Ejemplo.\n",
    "\n",
    "Supongamos una arquitectura 3-2-3-4. Es decir, tres neuronas de entrada, 2 capas intermedias con 2 y 3 neuronas cada una, y 4 neuronas de capa final. Primero veamos cuántos parámetros vamos a requerir:\n",
    "\n",
    "- Como hay tres neuronas en la capa inicial, entonces cada una de las dos neuronas de la capa 1 (la primer capa intermedia) debe tener tres importancias. Además cada una tiene su propio sesgo. Esto son $3\\cdot2+2=8$ \n",
    "\n",
    "- Como hay dos neuronas en la capa 1, entonces cada una de las tres neuronas de la capa 2 (la segunda capa intermedia) debe tener dos importancias. Además cada una tiene su propio sesgo. Esto son $2\\cdot3+3=9$ \n",
    "\n",
    "- Como hay tres neuronas en la capa 2, entonces cada una de las cuatro neuronas de la capa final debe tener dos importancias. Además cada una tiene su propio sesgo. Esto son $3\\cdot4+4=16$ \n",
    "\n",
    "Por lo tanto hay $8+9+16=33$ parámetros. De hecho, en general hay $$\\sum_{k=0}^Rn_{k+1}(n_k+1)$$ parámetros donde $n_0$ es el total de neuronas de la capa de entrada, $n_1,n_2,...,n_R$ son el total de neuronas de cada capa intermedia y $n_{R+1}$ es el total de neuronas en la capa de salida, por lo que en nuestro ejemplo tenemos $2(3+1)+3(2+1)+4(3+1)=8+9+16=33$ parámetros.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc88ed-b8f1-4efb-a2dc-7ed0bd414480",
   "metadata": {},
   "source": [
    "Digamos que la CAPA 1 (Oculta 1) tiene estos 8 parámetros\n",
    "  - $\\boldsymbol{w}_{1}^{(1)}=(0.5, -0.2, 0.8)$ y sesgo $b_1^{(1)}=0.1$\n",
    "  - $\\boldsymbol{w}_{1}^{(2)}=(-0.3, 0.7, 0.4)$ y sesgo $b_1^{(2)}=-0.5$\n",
    "\n",
    "Por lo tanto $W_1=\\left(\\begin{matrix}0.5&-0.2&0.8\\\\-0.3&0.7&0.4\\end{matrix}\\right)$ y $\\boldsymbol{b}_1=(0.1,-0.5)$. Digamos que el gerente de la red neuronal le dice a los porteros de esta capa que la fórmula de transformación de pensamientos crudos es $f_1(t)=t^2$\n",
    "\n",
    "CAPA 2 (Oculta 2): 9 parámetros\n",
    "  - $\\boldsymbol{w}_{2}^{(1)}=(0.9, -0.6)$ y sesgo $b_2^{(1)}=0.2$\n",
    "  - $\\boldsymbol{w}_{2}^{(2)}=(-0.1, 0.3)$ y sesgo $b_2^{(2)}=-0.4$\n",
    "  - $\\boldsymbol{w}_{2}^{(3)}=(0.5, -0.8)$ y sesgo $b_2^{(3)}=-0.3$\n",
    "\n",
    "Por lo tanto $W_2=\\left(\\begin{matrix}0.9&-0.6\\\\-0.1&0.3\\\\0.5&-0.8\\end{matrix}\\right)$ y $\\boldsymbol{b}_2=(0.2,-0.4,-0.3)$. Digamos que el gerente de la red neuronal le dice a los porteros de esta capa que la fórmula de transformación de pensamientos crudos es $f_2(t)=\\cos(t)$\n",
    "  \n",
    "CAPA de Salida - 16 parámetros:\n",
    "  - $\\boldsymbol{w}_{out}^{(1)}=(0.7, -0.4, 0.1)$ y sesgo $b_{out}^{(1)}=0$\n",
    "  - $\\boldsymbol{w}_{out}^{(2)}=(-0.2, 0.5, -0.6)$ y sesgo $b_{out}^{(2)}=0.3$\n",
    "  - $\\boldsymbol{w}_{out}^{(3)}=(0.4, 0.8, -0.9)$ y sesgo $b_{out}^{(3)}=-0.1$\n",
    "  - $\\boldsymbol{w}_{out}^{(4)}=(-0.5, 0.2, 0.6)$ y sesgo $b_{out}^{(4)}=0.7$  \n",
    "\n",
    "Por lo tanto $W_{out}=\\left(\\begin{matrix}0.7&-0.4&0.1\\\\-0.2&0.5&-0.6\\\\0.4&0.8&-0.9\\\\-0.5&0.2&0.6\\\\\\end{matrix}\\right)$ y $\\boldsymbol{b}_{out}=(0,0.3,-0.1,0.7)$. Digamos que el gerente de la red neuronal le dice a los porteros de esta capa que la fórmula de transformación de pensamientos crudos es $f_{out}(t)=100t$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eec1d0-9c08-4a71-9028-9cc968762ce3",
   "metadata": {},
   "source": [
    "Con todo esto, ya tenemos una red neuronal completamente construida. Pensemos que llega un cliente $\\boldsymbol{x}$ con características $\\boldsymbol{x}=(2,1,5)$. ¿Cuál será el resultado de aplicarle la red neuronal? Como la capa de salida tiene 4 neuronas, esperamos 4 valores finales.\n",
    "\n",
    "- **Pensamiento crudo y procesado de la capa 1**\n",
    "\n",
    "$$\\boldsymbol{z}_1=W_1\\boldsymbol{x}+\\boldsymbol{b}_1=\\left(\\begin{matrix}0.5&-0.2&0.8\\\\-0.3&0.7&0.4\\end{matrix}\\right)\\left(\\begin{matrix}2\\\\1\\\\5\\end{matrix}\\right)+\\left(\\begin{matrix}0.1\\\\-0.5\\end{matrix}\\right)=\\left(\\begin{matrix}4.9\\\\1.6\\end{matrix}\\right)$$\n",
    "\n",
    "$$\\boldsymbol{a}_1=f_1(\\boldsymbol{z}_1)=\\left(\\begin{matrix}4.9^2\\\\1.6^2\\end{matrix}\\right)=\\left(\\begin{matrix}24.01\\\\2.56\\end{matrix}\\right)$$\n",
    "\n",
    "- **Pensamiento crudo y procesado de la capa 2**\n",
    "\n",
    "$$\\boldsymbol{z}_2=W_2\\boldsymbol{a}_1+\\boldsymbol{b}_2=\\left(\\begin{matrix}0.9&-0.6\\\\-0.1&0.3\\\\0.5&-0.8\\end{matrix}\\right)\\left(\\begin{matrix}24.01\\\\2.56\\end{matrix}\\right)+\\left(\\begin{matrix}0.2\\\\-0.4\\\\-0.3\\end{matrix}\\right)=\\left(\\begin{matrix}20.273\\\\-2.033\\\\9.657\\end{matrix}\\right)$$\n",
    "\n",
    "$$\\boldsymbol{a}_2=f_2(\\boldsymbol{z}_2)=\\left(\\begin{matrix}\\cos(20.273)\\\\\\cos(-2.033)\\\\\\cos(9.657)\\end{matrix}\\right)=\\left(\\begin{matrix}0.148\\\\-0.448\\\\-0.973\\end{matrix}\\right)$$\n",
    "\n",
    "- **Pensamiento crudo y procesado de la capa final**\n",
    "\n",
    "$$\\boldsymbol{z}_{out}=W_{out}\\boldsymbol{a}_2+\\boldsymbol{b}_{out}=\\left(\\begin{matrix}0.7&-0.4&0.1\\\\-0.2&0.5&-0.6\\\\0.4&0.8&-0.9\\\\-0.5&0.2&0.6\\\\\\end{matrix}\\right)\\left(\\begin{matrix}0.148\\\\-0.448\\\\-0.973\\end{matrix}\\right)+\\left(\\begin{matrix}0\\\\0.3\\\\-0.1\\\\0.7\\end{matrix}\\right)=\\left(\\begin{matrix}0.186\\\\0.630\\\\0.477\\\\-0.047\\end{matrix}\\right)$$ $$\\boldsymbol{\\hat{y}}=\\boldsymbol{a}_{out}=f_{out}(\\boldsymbol{z}_{out})=\\left(\\begin{matrix}100*0.186\\\\100*0.630\\\\100*0.477\\\\100*-0.047\\end{matrix}\\right)=\\left(\\begin{matrix}18.6\\\\63\\\\47.7\\\\-4.7\\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb5d32-4ca2-4d1e-b41f-c276dad1f58a",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Hasta ahora hemos descrito cómo fluye la información hacia adelante: el cliente entra, su información pasa por etapas (las capas) donde se toman juicios intermedios y finalmente el gerente da una decisión. Pero una red neuronal no es interesante solo por decidir, sino por aprender a decidir mejor con el tiempo. Supongamos que el gerente toma una decisión final sobre el cliente, como dejarlo entrar, no dejarlo entrar, o asignarle cierto valor numérico (por ejemplo, cuánto gastará).\n",
    "\n",
    "Sin embargo, existe una decisión correcta esperada. Tal vez el cliente sí debía entrar, o no, o gastar más o menos. Esa información llega después. Aquí aparece una pregunta clave: ¿Qué tan buena fue la decisión que tomó la red?\n",
    "\n",
    "### Función de pérdida\n",
    "\n",
    "Para responder eso, el gerente compara lo que la red decidió, $\\boldsymbol{\\hat{y}}$, con lo que debía haber pasado, $\\boldsymbol{y}$. A partir de esta comparación se define una función de pérdida, que denotaremos por $\\mathcal{L}$. Esto es $$\\mathcal{L}(\\boldsymbol{y},\\boldsymbol{\\hat{y}})$$\n",
    "\n",
    "Este número mide qué tan mala fue la decisión. Si es grande, la red se equivocó mucho; si es pequeño, la red lo hizo bien.\n",
    "\n",
    "### ¿Quién tuvo la culpa del error?\n",
    "\n",
    "Aquí viene la idea central del backpropagation. La decisión final fue incorrecta, pero ¿fue culpa del último portero?, ¿de alguno intermedio?, ¿de un portero muy al inicio que exageró la importancia de la estatura?, ¿de un sesgo demasiado estricto?\n",
    "\n",
    "La red necesita repartir la **responsabilidad del error** entre todos los pesos y todos los sesgos de todas las capas. Matemáticamente, pérdida $\\mathcal{L}$ depende de $\\boldsymbol{\\hat{y}}$, la cual depende de $\\boldsymbol{a}_R$ (la última capa intermedia), la cual depende de $\\boldsymbol{a}_{R-1}$ (la penúltima capa intermedia) y así sucesivamente.\n",
    "\n",
    "Es decir, \n",
    "\n",
    "$$\\mathcal{L}=\\mathcal{L}(\\boldsymbol{y},\\textcolor{red}{\\boldsymbol{\\hat{y}}})=\\mathcal{L}(\\boldsymbol{y},\\textcolor{red}{f_{out}(W_{out}f_R(W_R...f_1(W_1\\boldsymbol{x}+\\boldsymbol{b}_1)...+\\boldsymbol{b}_R)+\\boldsymbol{b}_{out})})$$\n",
    "\n",
    "Ahora se usa la regla de la cadena para responder \"si cambio un poco este peso o este sesgo, ¿cuánto cambia el error final?\"\n",
    "\n",
    "Para cada peso $w_{i,s}^{(j)}$ y cada sesgo $b_{i}^{(j)}$, se calculan $$\\frac{\\partial\\mathcal{L}}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "Puntualmente, esto representa \"¿cuánto es que la importancia $s$ de la neurona $j$ de la capa $i$ y su sesgo afectan al resultado final?\". Si el valor absoluto de la derivada es grande, estos parámetros influyen mucho en el error. Si es pequeña, su inluencia es baja.\n",
    "\n",
    "## Ajuste de pesos\n",
    "\n",
    "Una vez calculadas estas responsabilidades, cada peso y sesgo se ajusta ligeramente:\n",
    "\n",
    "$$w_{i,s}^{(j)}\\leftarrow w_{i,s}^{(j)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }b_{i}^{(j)}\\leftarrow b_{i}^{(j)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "donde $\\eta>0$ es la **tasa de aprendizaje** (qué tanto le dice el gerente a la neurona que debe mejorar).\n",
    "\n",
    "Así, si un portero exageró un criterio, se le baja un poco; si ignoró algo importante, se le sube un poco; los porteros estrictos pueden volverse más flexibles y viceversa. Este proceso se repite con muchos clientes, y poco a poco la red afina sus criterios, aprende patrones, y mejora sus decisiones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73249e8-d9d3-4750-8eee-62b73bd19a38",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "\n",
    "Teníamos $\\boldsymbol{x}=(2,1,5)$ y nos produjo $\\boldsymbol{\\hat{y}}=(18.6,63,47.7,-4.7)$. Supongamos que por alguna razón sabemos que para esta $\\boldsymbol{x}$ se debió tener $\\boldsymbol{y}=(20,60,40,0)$.\n",
    "\n",
    "Digamos que el error cometido lo medimos como $$\\mathcal{L}(\\boldsymbol{y},\\boldsymbol{\\hat{y}})=\\frac{1}{2}((y_1-\\hat{y}_1)^2+(y_2-\\hat{y}_2)^2+(y_3-\\hat{y}_3)^2+(y_4-\\hat{y}_4)^2)$$\n",
    "\n",
    "Tomemos como tasa de aprendizaje $\\eta=0.001$.\n",
    "\n",
    "Para esta pérdida, $$\\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}_i}=\\hat{y}_i-y_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b8ed6-b125-4c87-9a9d-3fce1849e89d",
   "metadata": {},
   "source": [
    "Vemamos cómo actualizar los pesos y el sesgo de la segunda neurona de la capa de salida. Para esto, como la última capa intermedia tiene 3 neuronas, entonces para $s=1,2,3$ se debe calcular\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial w_{out,s}^{(2)}}\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}}{\\partial b_{out}^{(2)}}$$\n",
    "\n",
    "Por una parte, por la Regla de la Cadena\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial w_{out,s}^{(2)}}=\\textcolor{red}{\\frac{\\partial\\mathcal{L}}{\\partial z_{out}^{(2)}}}\\,\\textcolor{blue}{\\frac{\\partial z_{out}^{(2)}}{\\partial w_{out,s}^{(2)}}}=\\textcolor{red}{\\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}_2}\\,\\frac{\\partial \\hat{y}_2}{\\partial z_{out}^{(2)}}}\\,\\textcolor{blue}{\\frac{\\partial z_{out}^{(2)}}{\\partial w_{out,s}^{(2)}}}\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}}{\\partial b_{out}^{(2)}}=\\textcolor{red}{\\frac{\\partial\\mathcal{L}}{\\partial z_{out}^{(2)}}}\\,\\textcolor{darkgreen}{\\frac{\\partial{z_{out}^{(2)}}}{\\partial b_{out}^{(2)}}}$$\n",
    "\n",
    "Por otra parte, $$\\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}_2}=\\hat{y}_2-y_2=63-60=3$$\n",
    "\n",
    "Como $\\hat{y}_2=a_{out}^{(2)}=f_{out}(z_{out}^{(2)})=100z_{out}^{(2)}$, entonces $$\\frac{\\partial\\hat{y}_2}{\\partial z_{out}^{(2)}}=100$$\n",
    "\n",
    "Con esto, $$\\textcolor{red}{\\frac{\\partial\\mathcal{L}}{\\partial z_{out}^{(2)}}}=3\\cdot100=300$$\n",
    "\n",
    "Ahora, por definición, $$z_{out}^{(2)}=w_{out,1}^{(2)}a_2^{(1)}+w_{out,2}^{(2)}a_2^{(2)}+w_{out,3}^{(2)}a_2^{(3)}+b_{out}^{(2)}$$\n",
    "\n",
    "Entonces $$\\textcolor{blue}{\\frac{\\partial z_{out}^{(2)}}{\\partial w_{out,s}^{(2)}}=a_2^{(s)}}\\quad\\mbox{ y }\\quad\\textcolor{darkgreen}{\\frac{\\partial z_{out}^{(2)}}{\\partial b_{out}^{(2)}}=1}$$ \n",
    "\n",
    "Conluimos que $$\\frac{\\partial\\mathcal{L}}{\\partial w_{out,s}^{(2)}}=3\\cdot100\\cdot a_2^{(s)}=\\left\\{\\begin{array}{l}300*0.148=44.4\\mbox{ para }s=1\\\\300*-0.448=-134.4\\mbox{ para }s=2\\\\300\\cdot-0.973=-291.9\\mbox{ para }s=3\\end{array}\\right.\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}}{\\partial b_{out}^{(2)}}=300\\cdot1=300$$\n",
    "\n",
    "Luego, los nuevos parámetros de neurona 2 de la capa de salida son \n",
    "\n",
    "$$w_{out,1}^{(2)}=-0.2-0.001*44.4=-0.2444$$\n",
    "$$w_{out,2}^{(2)}=0.5-0.001*-134.4=0.6344$$\n",
    "$$w_{out,3}^{(2)}=-0.6-0.001*-291.9=-0.3081$$\n",
    "$$b_{out}^{(2)}=0.3-0.001*300=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2aa45b-f218-4aa3-8197-28bd83dc813d",
   "metadata": {},
   "source": [
    "De hecho, la nueva matriz de pesos y nuevos sesgos de la capa de salida son\n",
    "$$\n",
    "\\mbox{nuevo }W_{out} = \n",
    "\\begin{pmatrix}\n",
    "0.720720 & -0.462720 & -0.036220 \\\\\n",
    "-0.244400 & 0.634400 & -0.308100 \\\\\n",
    "0.286040 & 1.144960 & -0.150790 \\\\\n",
    "-0.430440 & -0.010560 & 0.142690\n",
    "\\end{pmatrix}\n",
    "\\quad\\mbox{ y nuevo }\\boldsymbol{b}_{out}=\\begin{pmatrix}\n",
    "0.140 \\\\ \n",
    "0.000 \\\\ \n",
    "-0.870 \\\\ \n",
    "1.170\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4bcdc-2b53-4d66-8f40-74b33d619df3",
   "metadata": {},
   "source": [
    "Ahora, veamos qué pasa con la primera neurona de la capa 2 (la última capa intermedia). La función de activación de esta capa es\n",
    "\n",
    "$$\n",
    "f_2(t)=\\cos(t)\n",
    "$$\n",
    "\n",
    "Como la capa anterior a esta tiene dos neuronas, entonces queremos calcular dos derivadas respecto a los dos pesos y la derivada respecto del sesgo: para $s=1,2$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial w_{2,s}^{(1)}}\n",
    "\\qquad \\text{y} \\qquad\n",
    "\\frac{\\partial \\mathcal L}{\\partial b_2^{(1)}}\n",
    "$$\n",
    "\n",
    "Los parámetros $w_{2,s}^{(1)}$ y $b_2^{(1)}$ no influyen directamente en la pérdida. Su efecto se propaga a través del siguiente camino:\n",
    "\n",
    "$$\n",
    "(w_{2,s}^{(1)},\\, b_2^{(1)})\n",
    "\\longrightarrow\n",
    "z_2^{(1)}\n",
    "\\longrightarrow\n",
    "a_2^{(1)}\n",
    "\\longrightarrow\n",
    "z_{out}^{(j)} \\quad j=1,2,3,4\n",
    "\\longrightarrow\n",
    "\\hat y_j\n",
    "\\longrightarrow\n",
    "\\mathcal L\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad378d-7eda-454d-8c29-7a7cc9757ea2",
   "metadata": {},
   "source": [
    "De esta manera:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal L}{\\partial w_{2,s}^{(1)}}=\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(1)}}\\,\\frac{\\partial \\mathcal z_{out}^{(1)}}{\\partial w_{2,s}^{(1)}}+\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(2)}}\\,\\frac{\\partial \\mathcal z_{out}^{(2)}}{\\partial w_{2,s}^{(1)}}+\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(3)}}\\,\\frac{\\partial \\mathcal z_{out}^{(3)}}{\\partial w_{2,s}^{(1)}}+\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(4)}}\\,\\frac{\\partial \\mathcal z_{out}^{(4)}}{\\partial w_{2,s}^{(1)}}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal L}{\\partial b_{2}^{(1)}}=\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(1)}}\\,\\frac{\\partial \\mathcal z_{out}^{(1)}}{\\partial b_{2}^{(1)}}+\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(2)}}\\,\\frac{\\partial \\mathcal z_{out}^{(2)}}{\\partial b_{2}^{(1)}}+\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(3)}}\\,\\frac{\\partial \\mathcal z_{out}^{(3)}}{\\partial b_{2}^{(1)}}+\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(4)}}\\,\\frac{\\partial \\mathcal z_{out}^{(4)}}{\\partial b_{2}^{(1)}}$$\n",
    "\n",
    "Recordemos que $$\\mathcal{L}=\\frac{1}{2}((y_1-\\hat{y}_1)^2+(y_2-\\hat{y}_2)^2+(y_3-\\hat{y}_3)^2+(y_4-\\hat{y}_4)^2)$$\n",
    "\n",
    "Como $\\hat{y}_j=f_{out}(z_{out}^{(j)})=100z_{out}^{(j)}$ para $j=1,2,3,4$, entonces $$\\frac{\\partial \\mathcal L}{\\partial z_{out}^{(j)}}=\\frac{\\partial \\mathcal{L}}{\\partial\\hat{y}_j}\\,\\frac{\\partial \\hat{y}_j}{\\partial z_{out}^{(j)}}=100(\\hat{y}_j-y_j)$$\n",
    "\n",
    "Los parámetros $w_{2,s}^{(1)}$ y $b_{2}^{(1)}$ no aparecen explícitamente en $z_{out}^{(j)}$, sino a través de la activación $a_2^{(1)}$.\n",
    "\n",
    "$$\\frac{\\partial z_{out}^{(j)}}{\\partial w_{2,s}^{(1)}}=\n",
    "\\frac{\\partial z_{out}^{(j)}}{\\partial a_2^{(1)}}\n",
    "\\frac{\\partial a_2^{(1)}}{\\partial w_{2,s}^{(1)}}\\quad\\mbox{ y }\\quad\n",
    "\\frac{\\partial z_{out}^{(j)}}{\\partial b_{2}^{(1)}}=\n",
    "\\frac{\\partial z_{out}^{(j)}}{\\partial a_2^{(1)}}\n",
    "\\frac{\\partial a_2^{(1)}}{\\partial b_{2}^{(1)}}\n",
    "$$\n",
    "\n",
    "Como $$z_{out}^{(j)}=w_{out,1}^{(j)}a_2^{(1)}+w_{out,2}^{(j)}a_2^{(2)}+w_{out,3}^{(j)}a_2^{(3)}+b_{out}^{(j)}$$ entonces $$\\frac{\\partial z_{out}^{(j)}}{\\partial a_2^{(1)}}=w_{out,1}^{(j)}$$\n",
    "\n",
    "Como $a_2^{(1)}=f_2(z_2^{(1)})=\\cos(z_2^{(1)})=\\cos(w_{2,1}^{(1)}a_1^{(1)}+w_{2,2}^{(1)}a_1^{(2)}+b_2^{(1)})$ entonces $$\\frac{\\partial a_2^{(1)}}{\\partial w_{2,s}^{(1)}}=-a_1^{(s)}\\sin(z_2^{(1)})\\quad\\mbox{ y }\\frac{\\partial a_2^{(1)}}{\\partial b_{2}^{(1)}}=-\\sin(z_2^{(1)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e53ce-439b-46b5-80a8-bd480a2f1f97",
   "metadata": {},
   "source": [
    "Con esto en mente\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\mathcal L}{\\partial w_{2,s}^{(1)}}&=&\n",
    "100(\\hat{y}_1-y_1)w_{out,1}^{(1)}(-a_1^{(s)}\\sin(z_2^{(1)}))+\n",
    "100(\\hat{y}_2-y_2)w_{out,1}^{(2)}(-a_1^{(s)}\\sin(z_2^{(1)}))+\n",
    "100(\\hat{y}_3-y_3)w_{out,1}^{(3)}(-a_1^{(s)}\\sin(z_2^{(1)}))+\n",
    "100(\\hat{y}_4-y_4)w_{out,1}^{(4)}(-a_1^{(s)}\\sin(z_2^{(1)}))\\\\\n",
    "&=&-100a_1^{(s)}\\sin(z_2^{(1)})\\left(w_{out,1}^{(1)}(\\hat{y}_1-y_1)+w_{out,1}^{(2)}(\\hat{y}_2-y_2)+w_{out,1}^{(3)}(\\hat{y}_3-y_3)+w_{out,1}^{(4)}(\\hat{y}_4-y_4)\\right)\\\\\n",
    "\\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial b_{2}^{(1)}}&=& 100(\\hat{y}_1-y_1)w_{out,1}^{(1)}(-\\sin(z_2^{(1)}))+ 100(\\hat{y}_2-y_2)w_{out,1}^{(2)}(-\\sin(z_2^{(1)}))+ 100(\\hat{y}_3-y_3)w_{out,1}^{(3)}(-\\sin(z_2^{(1)}))+ 100(\\hat{y}_4-y_4)w_{out,1}^{(4)}(-\\sin(z_2^{(1)}))\\\\ &=&-100\\sin(z_2^{(1)})\\left( w_{out,1}^{(1)}(\\hat{y}_1-y_1)+ w_{out,1}^{(2)}(\\hat{y}_2-y_2)+ w_{out,1}^{(3)}(\\hat{y}_3-y_3)+ w_{out,1}^{(4)}(\\hat{y}_4-y_4) \\right) \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e6cd8-0d05-4d66-a4a4-0793b385b44d",
   "metadata": {},
   "source": [
    "Finalmente, con  $\\hat{y} = (18.6,\\ 63,\\ 47.7,\\ -4.7)$ y $y = (20,\\ 60,\\ 40,\\ 0)$ se tiene\n",
    "$$\n",
    "\\hat{y} - y = (-1.4,\\ 3,\\ 7.7,\\ -4.7),\n",
    "$$\n",
    "\n",
    "Además \n",
    "$$z_2^{(1)}=20.273\\quad\\mbox{ y }\\quad a_1^{(1)}=24.01,\\,a_1^{(2)}=2.56$$\n",
    "\n",
    "y la primera columna de $W_{\\text{out}}$ es $(0.7,\\ -0.2,\\ 0.4,\\ -0.5)$. Entonces\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\mathcal L}{\\partial w_{2,1}^{(1)}}&=&-100*24.01*0.989\\left(0.7(-1.4)+(-0.2)3+0.4(7.7)+(-0.5)(-4.7)\\right)\\\\\n",
    "&=&-2374.589*3.85\\\\\n",
    "&=&-9142.16765\\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial w_{2,2}^{(1)}}&=&-100*2.56*0.989\\left(0.7(-1.4)+(-0.2)3+0.4(7.7)+(-0.5)(-4.7)\\right)\\\\\n",
    "&=&-253.184*3.85\\\\\n",
    "&=&-974.7584\\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial b_{2}^{(1)}}&=&-100*0.989\\left(0.7(-1.4)+(-0.2)3+0.4(7.7)+(-0.5)(-4.7)\\right)\\\\\n",
    "&=&-9.89*3.85\\\\\n",
    "&=&-380.765\n",
    "\\end{eqnarray*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc3ce03-0342-4c3b-aee7-27f6adb286c9",
   "metadata": {},
   "source": [
    "Concluimos que las nuevas actualizaciones son \n",
    "$$\n",
    "\\begin{array}{ccccc}\n",
    "w_{2,1}^{(1)}&=&0.9-0.001\\cdot-9142.167&=&10.042\\\\\n",
    "w_{2,2}^{(1)}&=&-0.6-0.001\\cdot-974.758&=&0.3747\\\\\n",
    "b_{2}^{(1)}&=&0.2-0.001\\cdot-380.765&=&0.5807\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b48902-2b31-4122-9124-e2e7a6548829",
   "metadata": {},
   "source": [
    "De hecho, después de hacer **todas** las actualizaciones, la red neuronal se transforma en\n",
    "\n",
    "1) Nuevos pesos y sesgos de la capa 1\n",
    "\n",
    "$$\n",
    "W_{1, \\text{nuevo}} = \n",
    "\\begin{pmatrix}\n",
    "10.903335 & 5.001667 & 26.808337 \\\\\n",
    "-4.384645 & -1.342322 & -9.811612\n",
    "\\end{pmatrix}, \\quad b_{1, \\text{nuevo}} = \n",
    "\\begin{pmatrix}\n",
    "5.301667 \\\\\n",
    "-2.542322\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "2) Nuevos pesos y sesgos de la capa 2\n",
    "\n",
    "$$\n",
    "W_{2, \\text{nuevo}} = \n",
    "\\begin{pmatrix}\n",
    "9.626123 & 0.330399 \\\\\n",
    "-16.265773 & -1.423631 \\\\\n",
    "7.047994 & -0.101838\n",
    "\\end{pmatrix}, \\quad b_{2, \\text{nuevo}} = \n",
    "\\begin{pmatrix}\n",
    "0.563437 \\\\\n",
    "-1.073293 \\\\\n",
    "-0.027281\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "3) Nuevos pesos y sesgos de la capa de salida\n",
    "\n",
    "$$\n",
    "W_{\\text{out, nuevo}} =\n",
    "\\begin{pmatrix}\n",
    "0.723746 & -0.472121 & -0.057392 \\\\\n",
    "-0.246351 & 0.640776 & -0.292777 \\\\\n",
    "0.285727 & 1.147071 & -0.142570 \\\\\n",
    "-0.431746 & -0.007303 & 0.147593\n",
    "\\end{pmatrix}, \\quad b_{\\text{out, nuevo}} = \n",
    "\\begin{pmatrix}\n",
    "0.161734 \\\\\n",
    "-0.015697 \\\\\n",
    "-0.878322 \\\\\n",
    "1.164886\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb132a-0efa-42d5-99e9-9bb00fd172fe",
   "metadata": {},
   "source": [
    "## Más de un cliente\n",
    "\n",
    "Hasta este momento hemos trabajado con un único cliente en la fila. ¿Cómo es el aprendizaje de la red neuronal cuando se reciben varios clientes?\n",
    "\n",
    "Tomemos una red con $n_0$ neuronas en la capa inicial.\n",
    "\n",
    "Supongamos que tenemos $n$ clientes. Cada cliente se puede ver como una pareja de información: los datos y la realidad. Es decir, tenemos los clientes $(\\boldsymbol{x}^{(1)},\\boldsymbol{y}^{(1)})$, $(\\boldsymbol{x}^{(2)},\\boldsymbol{y}^{(2)})$, $(\\boldsymbol{x}^{(3)},\\boldsymbol{y}^{(3)})$,..., $(\\boldsymbol{x}^{(n)},\\boldsymbol{y}^{(n)})$. Como la red tiene tres neuronas en la capa de origen, entonces $\\boldsymbol{x}^{(k)}=(x_1^{(k)},x_2^{(k)},...,x_{n_0}^{(k)})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e85f30-8c06-4fc3-a5dc-cee5d9b69330",
   "metadata": {},
   "source": [
    "Supongamos que la red neuronal tiene inicialmente matrices de pesos $W_1$, $W_2$,...,$W_R$, $W_{out}$ y vectores de sesgo $\\boldsymbol{b}_1$, $\\boldsymbol{b}_2$,...,$\\boldsymbol{b}_R$, $\\boldsymbol{b}_{out}$ y hace las predicciones $\\boldsymbol{\\hat{y}}^{(1)}$, $\\boldsymbol{\\hat{y}}^{(2)}$,...,$\\boldsymbol{\\hat{y}}^{(n)}$. \n",
    "\n",
    "Por lo tanto tenemos **$n$ errores**:\n",
    "\n",
    "$\\mathcal{L}^{(1)}=\\mathcal{L}(\\boldsymbol{y}^{(1)},\\boldsymbol{\\hat{y}}^{(1)})$ error cometido con el primer cliente\n",
    "\n",
    "$\\mathcal{L}^{(2)}=\\mathcal{L}(\\boldsymbol{y}^{(2)},\\boldsymbol{\\hat{y}}^{(2)})$ error cometido con el segundo cliente\n",
    "\n",
    "...\n",
    "\n",
    "$\\mathcal{L}^{(n)}=\\mathcal{L}(\\boldsymbol{y}^{(n)},\\boldsymbol{\\hat{y}}^{(n)})$ error cometido con el $n$-ésimo cliente\n",
    "\n",
    "Definimos el **error promedio de la red** como el promedio de los errores:\n",
    "\n",
    "$$\\mathcal{L}_{prom}=\\frac{\\mathcal{L}^{(1)}+\\mathcal{L}^{(2)}+...+\\mathcal{L}^{(n)}}{n}$$ y se busca minimizar este número.\n",
    "\n",
    "Recordemos que el proceso consiste en actualizar pesos y sesgos de las neuronas. En este caso, **las actualizaciones se hacen hasta que todos los clientes hallan pasado por la red**. Es decir, hasta que se han calculado todos los $\\boldsymbol{\\hat{y}}^{(k)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cda578-4713-46a7-acb7-43b8866a88a4",
   "metadata": {},
   "source": [
    "Existen tres métodos clásicos para minimizar el error promedio. La diferencia entre ellos **NO** está en las derivadas, sino en cuántos gradientes promedias antes de actualizar.\n",
    "\n",
    "### Batch gradiente descendente\n",
    "\n",
    "En este caso, la actualización se hace de la siguiente manera:\n",
    "$$w_{i,s}^{(j)}\\leftarrow w_{i,s}^{(j)}-\\eta\\frac{\\partial \\mathcal{L}_{prom}}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }\\quad b_{i}^{(j)}\\leftarrow b_{i}^{(j)}-\\eta\\frac{\\partial \\mathcal{L}_{prom}}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "Matemáticamente es el método más limpio. Observa que se utilizan **todos los $n$ clientes**, pues aparecen implícitamente en la definición de $\\mathcal{L}_{prom}$. Es un método estable, pero suele ser caro y lento.\n",
    "\n",
    "### SGD Gradiente descendente estocástico\n",
    "\n",
    "En este caso, se elige al azar un índice $k\\in\\{1,2,...,n\\}$ y la actualización es\n",
    "$$w_{i,s}^{(j)}\\leftarrow w_{i,s}^{(j)}-\\eta\\frac{\\partial \\mathcal{L}^{(k)}}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }\\quad b_{i}^{(j)}\\leftarrow b_{i}^{(j)}-\\eta\\frac{\\partial \\mathcal{L}^{(k)}}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "Por lo tanto usas un solo cliente al azar. Es un método útilmente ruidoso y muy barato. Su problema es que no garantiza disminuir la pérdida promedio en cada paso.\n",
    "\n",
    "### Minibatch estándar\n",
    "\n",
    "Tomas al azar un subconjunto de clientes $B\\subset\\{1,2,...,n\\}$ y la actualización es\n",
    "$$w_{i,s}^{(j)}\\leftarrow w_{i,s}^{(j)}-\\eta\\frac{1}{|B|}\\sum_{k\\in B}\\frac{\\partial \\mathcal{L}^{(k)}}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }\\quad b_{i}^{(j)}\\leftarrow b_{i}^{(j)}-\\eta\\frac{1}{|B|}\\sum_{k\\in B}\\frac{\\partial \\mathcal{L}^{(k)}}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "Es un método estable, eficiente y el que se usa en la práctica. Y se suele tomar $|B|\\in\\{16,32,64,128,256\\}$, aunque 32 y 64 son los más comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bfd7c-34f0-4d05-99e3-8eb798c6b7c2",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "\n",
    "Supongamos una arquitectura 3-4-1 con los siguientes parámetros:\n",
    "\n",
    "- **Capa 1.** Cuatro neuronas\n",
    "\n",
    "matriz de pesos $\n",
    "W_1 =\n",
    "\\begin{pmatrix}\n",
    " 0.2 & -0.1 &  0 \\\\\n",
    "-0.3 &  0.1 &  0.2 \\\\\n",
    " 0   &  0.2 & -0.2 \\\\\n",
    " 0.1 &  0   &  0.3\n",
    "\\end{pmatrix}$ y sesgos $b_1 =\n",
    "\\begin{pmatrix}\n",
    " 0 \\\\\n",
    " 0.1 \\\\\n",
    "-0.1 \\\\\n",
    " 0.05\n",
    "\\end{pmatrix}\n",
    "$ con función de activación $f_1(t)=\\tanh(t)$\n",
    "\n",
    "- **Capa 2 (capa de salida).** Una neurona\n",
    "\n",
    "Matriz de pesos $\n",
    "W_{out} =\n",
    "\\begin{pmatrix}\n",
    " 0.1 & -0.2 & 0   &  0.3 \n",
    "\\end{pmatrix}$ y sesgos $b_{out} =\n",
    "\\begin{pmatrix}\n",
    " 0.05\n",
    "\\end{pmatrix}\n",
    "$ con función de activación $f_{out}(t)=t$\n",
    "\n",
    "Tomemos $\\eta=0.1$ y función de pérdida $$\\mathcal{L}(\\boldsymbol{y},\\boldsymbol{\\hat{y}})=\\frac{(y_1-\\hat{y}_1)^2}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82d5a4-60bf-4085-bd93-69278afac727",
   "metadata": {},
   "source": [
    "En este caso, cada cliente tiene tres datos de características, pues la capa inicial tiene tres neuronas, y su vector real de resultado solo es un número (porque la capa de salida solo tiene una neurona). Digamos que hay 4 clientes en la fila:\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{x}^{(1)}, \\boldsymbol{y}^{(1)}) = ((1, 0, 1), 0.5), \\qquad\n",
    "(\\boldsymbol{x}^{(2)}, \\boldsymbol{y}^{(2)}) = ((0, 1, 1), 0),\n",
    "$$\n",
    "$$\n",
    "(\\boldsymbol{x}^{(3)}, \\boldsymbol{y}^{(3)}) = ((1, 1, 0), 0.4), \\qquad\n",
    "(\\boldsymbol{x}^{(4)}, \\boldsymbol{y}^{(4)}) = ((-1, 1, 0.5), -0.3).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96315d4c-c5b7-4156-92c9-b4b35c236a7f",
   "metadata": {},
   "source": [
    "Veamos el camino para llegar a $\\boldsymbol{\\hat{y}}^{(3)}$. Es decir, la predicción de la red para el tercer cliente.\n",
    "\n",
    "$$\\boldsymbol{z}_1=W_1\\boldsymbol{x}^{(3)}+\\boldsymbol{b}_1=\n",
    "\\begin{pmatrix}\n",
    " 0.2 & -0.1 &  0 \\\\\n",
    "-0.3 &  0.1 &  0.2 \\\\\n",
    " 0   &  0.2 & -0.2 \\\\\n",
    " 0.1 &  0   &  0.3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}+\n",
    "\\begin{pmatrix}\n",
    " 0 \\\\\n",
    " 0.1 \\\\\n",
    "-0.1 \\\\\n",
    " 0.05\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    " 0.1 \\\\\n",
    " -0.1 \\\\\n",
    "0.1 \\\\\n",
    " 0.15\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\\boldsymbol{a}_1=f_1(\\boldsymbol{z}_1)=\\tanh(\\boldsymbol{z}_1)=\\begin{pmatrix}\n",
    " 0.0996679946 \\\\\n",
    " -0.0996679946 \\\\\n",
    "0.0996679946 \\\\\n",
    " 0.1488850334\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$$\\boldsymbol{z}_{out}=W_{out}\\boldsymbol{a}_1+\\boldsymbol{b}_{out}=\\begin{pmatrix}\n",
    " 0.1 & -0.2 & 0   &  0.3 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " 0.0996679946 \\\\\n",
    " -0.0996679946 \\\\\n",
    "0.0996679946 \\\\\n",
    " 0.1488850334\n",
    "\\end{pmatrix}+\\begin{pmatrix}\n",
    " 0.05\n",
    "\\end{pmatrix}=0.1245659084$$\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}}^{(3)}=\\boldsymbol{a}_{out}=f_{out}(\\boldsymbol{z}_{out})=\\boldsymbol{z}_{out}=0.1245659084$$\n",
    "\n",
    "Por lo tanto $$\\mathcal{L}^{(3)}=\\mathcal{L}(\\boldsymbol{y}^{(3)},\\boldsymbol{\\hat{y}}^{(3)})=\\frac{(0.4-0.1245659084)^2}{2}=0.03793196939$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd51e4a-c898-4919-8447-a8eb4b050970",
   "metadata": {},
   "source": [
    "De hecho, si hacemos el proceso para cada uno de los cuatro clientes obtenemos\n",
    "\n",
    "| cliente | $y^{(i)}$ | $\\hat{y}^{(i)}$ |\n",
    "|-----|-----------|-----------------|\n",
    "| 1   | 0.5       | 0.1963072336    |\n",
    "| 2   | 0         | 0.06495607139   |\n",
    "| 3   | 0.4       | 0.1245659085    |\n",
    "| 4   | -0.3      | -0.05664077626  |\n",
    "\n",
    "y pérdidas \n",
    "$$\\mathcal{L}^{(1)}=0.04611464818$$\n",
    "$$\\mathcal{L}^{(2)}=0.002109645605$$\n",
    "$$\\mathcal{L}^{(3)}=0.03793196939$$\n",
    "$$\\mathcal{L}^{(4)}=0.02961185589$$\n",
    "\n",
    "Por lo tanto $$\\mathcal{L}_{prom}=\\frac{0.04611464818+0.002109645605+0.03793196939+0.02961185589}{4}=0.02894202977$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdf1dc-6e89-4831-8800-d3c58ca23153",
   "metadata": {},
   "source": [
    "Ahora hagamos las actualizaciones con cada uno de los tres métodos comentados arriba.\n",
    "\n",
    "Primero, notemos que hay 21 parámetros, que inicialmente son 16 números $w_{i,s}^{(j)}$ y 5 números $b_i^{(j)}$. \n",
    "\n",
    "Cada vez que ingresamos un cliente $k$ a la red se hace la propagación hacia adelante; se obtiene $\\mathcal{L}^{(k)}$ y en la retropropagación se calcula $$\\frac{\\partial {\\cal L}^{(k)}}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }\\quad\\frac{\\partial {\\cal L}^{(k)}}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "Es decir, 21 derivadas. Como hay 4 clientes, entonces se calcularán 84 derivadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba38e6-e5c5-4434-b20b-3238d589afb8",
   "metadata": {},
   "source": [
    "Estas son:\n",
    "\n",
    "**Cliente 1**\n",
    "\n",
    "    - Capa 1\n",
    "\n",
    "        - Neurona 1\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,1}^{(1)}}=-0.0291861802129,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,2}^{(1)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,3}^{(1)}}=-0.0291861802129,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial b_{1}^{(1)}}=-0.0291861802129\n",
    "$$\n",
    "\n",
    "        - Neurona 2\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,1}^{(2)}}=0.0607385532805,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,2}^{(2)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,3}^{(2)}}=0.0607385532805,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial b_{1}^{(2)}}=0.0607385532805\n",
    "$$\n",
    "\n",
    "        - Neurona 3\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,1}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,2}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,3}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial b_{1}^{(3)}}=0\n",
    "$$\n",
    "\n",
    "        - Neurona 4\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,1}^{(4)}}=-0.0186338238498,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,2}^{(4)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{1,3}^{(4)}}=-0.0186338238498,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial b_{1}^{(4)}}=-0.0186338238498\n",
    "$$\n",
    "\n",
    "    - Capa 2 (capa de salida)\n",
    "\n",
    "        - Neurona 1\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{out,1}^{(1)}}=0.0908323914476,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{out,2}^{(1)}}=0.0710228754759,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{out,3}^{(1)}}=0.0884695331634,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial w_{out,4}^{(1)}}=-0.128127676047,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(1)}}{\\partial b_{out}^{(1)}}=-0.303692766403\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Cliente 2**\n",
    "\n",
    "    - Capa 1\n",
    "\n",
    "          - Neurona 1\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,1}^{(1)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,2}^{(1)}}=0.00224514853531,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,3}^{(1)}}=0.00224514853531,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial b_{1}^{(1)}}=0.00224514853531\n",
    "$$\n",
    "\n",
    "        - Neurona 2\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,1}^{(2)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,2}^{(2)}}=-0.0111157868148,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,3}^{(2)}}=-0.0111157868148,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial b_{1}^{(2)}}=-0.0111157868148\n",
    "$$\n",
    "\n",
    "        - Neurona 3\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,1}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,2}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,3}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial b_{1}^{(3)}}=0\n",
    "$$\n",
    "\n",
    "        - Neurona 4\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,1}^{(4)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,2}^{(4)}}=0.00663486051116,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{1,3}^{(4)}}=0.00663486051116,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial b_{1}^{(4)}}=0.00663486051116\n",
    "$$\n",
    "\n",
    "    - Capa 2 (capa final)\n",
    "\n",
    "        - Neurona 1\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{out,1}^{(1)}}=0.0108627420084,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{out,2}^{(1)}}=0.0246808747211,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{out,3}^{(1)}}=-0.00649258498951,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial w_{out,4}^{(1)}}=0.0180999896883,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(2)}}{\\partial b_{out}^{(1)}}=0.0649560713874\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Cliente 3**\n",
    "\n",
    "    - Capa 1\n",
    "\n",
    "        - Neurona 1\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,1}^{(1)}}=0.00158583420316,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,2}^{(1)}}=0.00158583420316,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,3}^{(1)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial b_{1}^{(1)}}=0.00158583420316\n",
    "$$\n",
    "\n",
    "        - Neurona 2    \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,1}^{(2)}}=0.0545396018694,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,2}^{(2)}}=0.0545396018694,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,3}^{(2)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial b_{1}^{(2)}}=0.0545396018694\n",
    "$$\n",
    "\n",
    "        - Neurona 3\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,1}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,2}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,3}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial b_{1}^{(3)}}=0\n",
    "$$\n",
    "\n",
    "        - Neurona 4\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,1}^{(4)}}=-0.0119949414528,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,2}^{(4)}}=-0.0119949414528,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{1,3}^{(4)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial b_{1}^{(4)}}=-0.0119949414528\n",
    "$$\n",
    "\n",
    "    - Capa 2\n",
    "\n",
    "        - Neurona 1\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{out,1}^{(1)}}=-0.00280102359729,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{out,2}^{(1)}}=-0.0274583404496,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{out,3}^{(1)}}=-0.0136358820646,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial w_{out,4}^{(1)}}=-0.0209537313689,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(3)}}{\\partial b_{out}^{(1)}}=-0.275434091526\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Cliente 4**\n",
    "\n",
    "    - Capa 1\n",
    "\n",
    "        - Neurona 1\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,1}^{(1)}}=-0.0513125608429,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,2}^{(1)}}=0.0513125608429,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,3}^{(1)}}=0.0256562804214,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial b_{1}^{(1)}}=-0.0513125608429\n",
    "$$\n",
    "\n",
    "        - Neurona 2\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,1}^{(2)}}=0.0346338023897,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,2}^{(2)}}=-0.0346338023897,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,3}^{(2)}}=-0.0173169011949,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial b_{1}^{(2)}}=-0.0346338023897\n",
    "$$\n",
    "\n",
    "        - Neurona 3\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,1}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,2}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,3}^{(3)}}=0,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial b_{1}^{(3)}}=0\n",
    "$$\n",
    "\n",
    "        - Neurona 4\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,1}^{(4)}}=-0.0284171191225,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,2}^{(4)}}=0.0284171191225,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{1,3}^{(4)}}=0.0142085595612,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial b_{1}^{(4)}}=-0.0284171191225\n",
    "$$\n",
    "\n",
    "    - Capa 2\n",
    "\n",
    "        - Neurona 1\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{out,1}^{(1)}}=-0.0375110369997,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{out,2}^{(1)}}=-0.116418487119,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{out,3}^{(1)}}=-0.0344155331836,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial w_{out,4}^{(1)}}=0.00895048735605,\\quad\n",
    "\\frac{\\partial \\mathcal L^{(4)}}{\\partial b_{out}^{(1)}}=0.243359223743\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a627ea-ab10-45f7-a00c-c39f643bd4fa",
   "metadata": {},
   "source": [
    "**Batch gradiente descendente**\n",
    "\n",
    "Para cada neurona de la red, debemos calcular $$\\frac{\\partial\\mathcal{L}_{prom}}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}_{prom}}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "Como $\\mathcal{L}_{prom}=\\frac{\\mathcal{L}^{(1)}+\\mathcal{L}^{(2)}+\\mathcal{L}^{(3)}+\\mathcal{L}^{(4)}}{4}$ se tiene\n",
    "\n",
    "$$\\frac{\\partial {\\cal L}_{\\text{prom}}}{\\partial w_{i,s}^{(j)}}=\\frac{1}{4}\\sum_{k=1}^{4}\\frac{\\partial {\\cal L}^{(k)}}{\\partial w_{i,s}^{(j)}},\\qquad\\frac{\\partial {\\cal L}_{\\text{prom}}}{\\partial b_{i}^{(j)}}=\\frac{1}{4}\\sum_{k=1}^{4}\\frac{\\partial {\\cal L}^{(k)}}{\\partial b_{i}^{(j)}}.$$\n",
    "\n",
    "Por ejemplo, para la neurona 2 de la capa 1 se tiene:\n",
    "\n",
    "- Derivadas de los pesos\n",
    "$$\\frac{\\partial \\mathcal{L}_{prom}}{\\partial w_{1,1}^{(2)}} = \\frac{0.0607385532805 + 0 + 0.0545396018694 + 0.0346338023897}{4} = 0.0374779893849$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}_{prom}}{\\partial w_{1,2}^{(2)}} = \\frac{0 - 0.0111157868148 + 0.0545396018694 - 0.0346338023897}{4} = 0.002197503166225$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}_{prom}}{\\partial w_{1,3}^{(2)}} = \\frac{0.0607385532805 - 0.0111157868148 + 0 - 0.0173169011949}{4} = 0.0080764663177$$\n",
    "\n",
    "- Derivada del sesgo\n",
    "  \n",
    "$$\\frac{\\partial \\mathcal{L}_{prom}}{\\partial b_{1}^{(2)}} = \\frac{0.0607385532805 - 0.0111157868148 + 0.0545396018694 - 0.0346338023897}{4} = 0.01738214148635$$\n",
    "\n",
    "Como $\\eta=0.1$ entonces las actualizaciones para esta neurona son\n",
    "\n",
    "$$w^{(2)}_{1,1}\\gets w^{(2)}_{1,1}-\\eta\\frac{\\partial\\mathcal{L}_{prom}}{\\partial w^{(2)}_{1,1}}=-0.3-0.1(0.0374779893849)=-0.30374779893849$$\n",
    "\n",
    "$$w^{(2)}_{1,2}\\gets w^{(2)}_{1,2}-\\eta\\frac{\\partial\\mathcal{L}_{prom}}{\\partial w^{(2)}_{1,2}}=0.1-0.1(0.002197503166225)=0.0997802496833775$$\n",
    "\n",
    "$$w^{(2)}_{1,3}\\gets w^{(2)}_{1,3}-\\eta\\frac{\\partial\\mathcal{L}_{prom}}{\\partial w^{(2)}_{1,3}}=0.2-0.1(0.0080764663177)=0.19919235336823$$\n",
    "\n",
    "$$b^{(2)}_1\\gets b^{(2)}_1-\\eta\\frac{\\partial\\mathcal{L}_{prom}}{\\partial b^{(2)}_1}=0.1-0.1(0.01738214148635)=0.098261785851365$$\n",
    "\n",
    "De hecho, utilizando batch gradiente descendente, las nuevas matrices de pesos y vectores de sesgos son\n",
    "\n",
    "**Capa 1**\n",
    "\n",
    "$$W^{\\text{nuevo}}_1 = \\begin{pmatrix} 0.20196816708 & -0.10003579957 & 0.000290493688 \\\\ -0.303747798939 & 0.099780249683 & 0.199192353368 \\\\ 0 & 0.2 & -0.2 \\\\ 0.105699296525 & -0.000219146557 & 0.300536689173 \\end{pmatrix}\\quad\\mbox{ y }b^{\\text{nuevo}}_1 = \\begin{pmatrix} 0.000693854935 \\\\ 0.098261785851 \\\\ -0.1 \\\\ 0.051653122148 \\end{pmatrix}$$\n",
    "\n",
    "**Capa de salida**\n",
    "$$W^{\\text{nuevo}}_{out} = (0.104119026829 \\quad -0.20457069803 \\quad -0.001363588206 \\quad 0.303075773259)\\quad\\mbox{ y }b^{\\text{nuevo}}_{out} = 0.05677028907$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f8ff4-6397-44cb-b68b-637bd6ac0d1a",
   "metadata": {},
   "source": [
    "**Gradiente estocástico**\n",
    "\n",
    "Ahora primero seleccionamos un cliente al azar. Digamos al 1. Las derivadas de este cliente son las que se utilizarán para actualizar toda la red. Por ejemplo, para la neurona 2 de la capa 1 se usa\n",
    "\n",
    "- Derivada de los pesos\n",
    "$$\\frac{\\partial\\mathcal{L}^{(1)}}{\\partial w_{1,1}^{(2)}}=0.0607385532805,\\quad\\frac{\\partial\\mathcal{L}^{(1)}}{\\partial w_{1,2}^{(2)}}=0,\\quad\\frac{\\partial\\mathcal{L}^{(1)}}{\\partial w_{1,3}^{(2)}}=0.0607385532805$$\n",
    "\n",
    "- Derivada del sesgo\n",
    "$$\\frac{\\partial\\mathcal{L}^{(1)}}{\\partial b_{1}^{(2)}}=0.0607385532805$$\n",
    "\n",
    "Como $\\eta=0.1$ entonces las actualizaciones para esta neurona son\n",
    "\n",
    "$$\n",
    "w_{1,1}^{(2)} \\leftarrow w_{1,1}^{(2)} - \\eta \\frac{\\partial \\mathcal{L}^{(1)}}{\\partial w_{1,1}^{(2)}}\n",
    "= -0.3 - 0.1(0.0607385532805) = -0.30607385532805\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{1,2}^{(2)} \\leftarrow w_{1,2}^{(2)} - \\eta \\frac{\\partial \\mathcal{L}^{(1)}}{\\partial w_{1,2}^{(2)}}\n",
    "= 0.1 - 0.1(0) = 0.1\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{1,3}^{(2)} \\leftarrow w_{1,3}^{(2)} - \\eta \\frac{\\partial \\mathcal{L}^{(1)}}{\\partial w_{1,3}^{(2)}}\n",
    "= 0.2 - 0.1(0.0607385532805) = 0.19392614467195\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_1^{(2)} \\leftarrow b_1^{(2)} - \\eta \\frac{\\partial \\mathcal{L}^{(1)}}{\\partial b_1^{(2)}}\n",
    "= 0.1 - 0.1(0.0607385532805) = 0.09392614467195\n",
    "$$\n",
    "\n",
    "De hecho, utilizando gradiente estocástico (con el cliente $k=1$), las nuevas matrices de pesos y vectores de sesgos son\n",
    "\n",
    "**Capa 1**\n",
    "$$\n",
    "W_1^{\\text{nuevo}} =\n",
    "\\begin{pmatrix}\n",
    " 0.20291861802129 & -0.1 & 0.00291861802129 \\\\\n",
    "-0.30607385532805 &  0.1 & 0.19392614467195 \\\\\n",
    " 0                &  0.2 & -0.2 \\\\\n",
    " 0.10186338238498 &  0   & 0.30186338238498\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "b_1^{\\text{nuevo}} =\n",
    "\\begin{pmatrix}\n",
    " 0.00291861802129 \\\\\n",
    " 0.09392614467195 \\\\\n",
    "-0.1 \\\\\n",
    " 0.05186338238498\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Capa 2**\n",
    "\n",
    "$$\n",
    "W_{\\text{out}}^{\\text{nuevo}} =\n",
    "\\begin{pmatrix}\n",
    " 0.09091676085524 & -0.20710228754759 & -0.00884695331634 & 0.3128127676047\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "b_{\\text{out}}^{\\text{nuevo}} = 0.0803692766403\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234f1b4-3848-4b55-8937-0e8f63e65506",
   "metadata": {},
   "source": [
    "**Minibatch gradiente descendente**\n",
    "\n",
    "En minibatch, en lugar de usar todos los clientes (batch) o uno solo (estocástico), elegimos un subconjunto $B$ de clientes y actualizamos con el promedio de sus derivadas.\n",
    "\n",
    "Tomemos el minibatch $B=\\{1,3\\}$ (primer y tercer clientes) y definamos la pérdida del minibatch como $$\\mathcal{L}_B=\\frac{\\mathcal{L}^{(1)}+\\mathcal{L}^{(3)}}{2}$$\n",
    "\n",
    "Entonces, para cada neurona de la red, debemos calcular $$\\frac{\\partial\\mathcal{L}_B}{\\partial w_{i,s}^{(j)}}\\quad\\mbox{ y }\\quad\\frac{\\partial\\mathcal{L}_B}{\\partial b_{i}^{(j)}}$$\n",
    "\n",
    "Y como $\\mathcal{L}_B = \\frac{\\mathcal{L}^{(1)} + \\mathcal{L}^{(3)}}{2}$ se tiene $$\n",
    "\\frac{\\partial \\mathcal{L}_B}{\\partial w_{i,s}^{(j)}}\n",
    "= \\frac{1}{2}\\left(\n",
    "\\frac{\\partial \\mathcal{L}^{(1)}}{\\partial w_{i,s}^{(j)}}+\\frac{\\partial \\mathcal{L}^{(3)}}{\\partial w_{i,s}^{(j)}}\\right),\n",
    "\\qquad\n",
    "\\frac{\\partial \\mathcal{L}_B}{\\partial b_i^{(j)}}\n",
    "= \\frac{1}{2}\\left(\n",
    "\\frac{\\partial \\mathcal{L}^{(1)}}{\\partial b_i^{(j)}}+\\frac{\\partial \\mathcal{L}^{(3)}}{\\partial b_i^{(j)}}\\right)\n",
    "$$\n",
    "\n",
    "Por ejemplo, para la neurona 2 de la capa 1 se tiene\n",
    "\n",
    "- Derivada de los pesos\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_B}{\\partial w_{1,1}^{(2)}}\n",
    "=\n",
    "\\frac{0.0607385532805 + 0.0545396018694}{2}\n",
    "=\n",
    "0.05763907757495\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_B}{\\partial w_{1,2}^{(2)}}\n",
    "=\n",
    "\\frac{0 + 0.0545396018694}{2}\n",
    "=\n",
    "0.0272698009347\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_B}{\\partial w_{1,3}^{(2)}}\n",
    "=\n",
    "\\frac{0.0607385532805 + 0}{2}\n",
    "=\n",
    "0.03036927664025\n",
    "$$\n",
    "\n",
    "- Derivada del sesgo\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_B}{\\partial b_1^{(2)}}\n",
    "=\n",
    "\\frac{0.0607385532805 + 0.0545396018694}{2}\n",
    "=\n",
    "0.05763907757495\n",
    "$$\n",
    "\n",
    "Como $\\eta=0.1$ entonces las actualizaciones para esta neurona son\n",
    "\n",
    "$$w^{(2)}_{1,1}\\gets w^{(2)}_{1,1}-\\eta\\frac{\\partial\\mathcal{L}_B}{\\partial w^{(2)}_{1,1}}=-0.3-0.1(0.05763907757495)=-0.305763907757495$$\n",
    "\n",
    "$$w^{(2)}_{1,2}\\gets w^{(2)}_{1,2}-\\eta\\frac{\\partial\\mathcal{L}_B}{\\partial w^{(2)}_{1,2}}=0.1-0.1(0.0272698009347)=0.09727301990653$$\n",
    "\n",
    "$$w^{(2)}_{1,3}\\gets w^{(2)}_{1,3}-\\eta\\frac{\\partial\\mathcal{L}_B}{\\partial w^{(2)}_{1,3}}=0.2-0.1(0.03036927664025)=0.196963072335975$$\n",
    "\n",
    "$$b^{(2)}_1\\gets b^{(2)}_1-\\eta\\frac{\\partial\\mathcal{L}_B}{\\partial b^{(2)}_1}=0.1-0.1(0.05763907757495)=0.094236092242505$$\n",
    "\n",
    "De hecho, utilizando minibatch gradiente descendente con $B={1,3}$, las nuevas matrices de pesos y vectores de sesgos son\n",
    "\n",
    "**Capa 1**\n",
    "\n",
    "$$\n",
    "W_1^{\\text{nuevo}} =\n",
    "\\begin{pmatrix}\n",
    " 0.201380017300487 & -0.100079291710158 & 0.00145309010645 \\\\\n",
    "-0.305763907757495 &  0.09727301990653  & 0.196963072335975 \\\\\n",
    " 0                 &  0.2               & -0.2 \\\\\n",
    " 0.10153143826513  &  0.00059974707264  & 0.30093169119249\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "b_1^{\\text{nuevo}} =\n",
    "\\begin{pmatrix}\n",
    " 0.001380017300487 \\\\\n",
    " 0.094236092242505 \\\\\n",
    "-0.1 \\\\\n",
    " 0.05153143826513\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Capa de salida**\n",
    "\n",
    "$$\n",
    "W_{\\text{out}}^{\\text{nuevo}} =\n",
    "\\begin{pmatrix}\n",
    " 0.0955984316074845 & -0.202178226751315 & -0.00374168255494 & 0.307454070370795\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "b_{\\text{out}}^{\\text{nuevo}} = 0.07895634289645\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598d68b-2ca9-4f16-b3ca-45adfeb4199a",
   "metadata": {},
   "source": [
    "**Batch GD (usando $W_1^{\\text{nuevo}},b_1^{\\text{nuevo}},W_{out}^{\\text{nuevo}},b_{out}^{\\text{nuevo}}$ del batch)**\n",
    "\n",
    "Cliente 1: $\\hat y^{(1)}=0.209127001022271,\\quad \\mathcal L^{(1)}=0.042303550767149$\n",
    "\n",
    "Cliente 2: $\\hat y^{(2)}=0.07186143124441,\\quad \\mathcal L^{(2)}=0.002582032650248$\n",
    "\n",
    "Cliente 3: $\\hat y^{(3)}=0.136061655831731,\\quad \\mathcal L^{(3)}=0.034831724761144$\n",
    "\n",
    "Cliente 4: $\\hat y^{(4)}=-0.054730934552725,\\quad \\mathcal L^{(4)}=0.03007845723269$\n",
    "\n",
    "Pérdida promedio: 0.02744894135280775\n",
    "\n",
    "---\n",
    "\n",
    "**SGD (un paso con el cliente $k=1$, usando tus $W_1^{\\text{nuevo}},b_1^{\\text{nuevo}},W_{out}^{\\text{nuevo}},b_{out}^{\\text{nuevo}}$ del estocástico)**\n",
    "\n",
    "Cliente 1: $\\hat y^{(1)}=0.238837650431852,\\quad \\mathcal L^{(1)}=0.034102886415978$\n",
    "\n",
    "Cliente 2: $\\hat y^{(2)}=0.10244462079014,\\quad \\mathcal L^{(2)}=0.005247450164418$\n",
    "\n",
    "Cliente 3: $\\hat y^{(3)}=0.159915731730028,\\quad \\mathcal L^{(3)}=0.028820227935364$\n",
    "\n",
    "Cliente 4: $\\hat y^{(4)}=-0.025304424342302,\\quad \\mathcal L^{(4)}=0.037728829642957$\n",
    "\n",
    "Pérdida promedio: 0.02744894135280775\n",
    "\n",
    "---\n",
    "\n",
    "**Minibatch GD (con $B={1,3}$, usando tus $W_1^{\\text{nuevo}},b_1^{\\text{nuevo}},W_{out}^{\\text{nuevo}},b_{out}^{\\text{nuevo}}$ del minibatch)**\n",
    "\n",
    "Cliente 1: $\\hat y^{(1)}=0.232969303173548,\\quad \\mathcal L^{(1)}=0.03565269652381$\n",
    "\n",
    "Cliente 2: $\\hat y^{(2)}=0.099502271761945,\\quad \\mathcal L^{(2)}=0.004950351042894$\n",
    "\n",
    "Cliente 3: $\\hat y^{(3)}=0.158240850698111,\\quad \\mathcal L^{(3)}=0.029223743135587$\n",
    "\n",
    "Cliente 4: $\\hat y^{(4)}=-0.025835679416952,\\quad \\mathcal L^{(4)}=0.037583037340382$\n",
    "\n",
    "Pérdida promedio: 0.02647484853967925"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417d25d-3b25-44d8-8c77-3aa668ba0265",
   "metadata": {},
   "source": [
    "## La intimidad de los tres métodos de actualización\n",
    "\n",
    "Ya hemos dicho que las actualizaciones se hacen hasta que pasan todos los clientes por la red. Ahora, independientemente de las cuándo se actualicen los pesos, una **época** es el intervalo en el que cada dato de la fila se usa una vez para aprender.\n",
    "\n",
    "Veamos el proceso de forma general. Digamos que hay $p$ parámetros en la red; $n$ clientes en la fila y elegimos un batch de tamaño $m$. Por lo general, $n=m*q+r$, con $r\\in\\{0,1,...,m-1\\}$.\n",
    "\n",
    "- **Época 1**: Se reordena la fila de clientes;\n",
    "    - **Minibatch 1**\n",
    "        - Se toman los clientes del 1 al $m$;  \n",
    "        - Se calculan las $m$ predicciones;\n",
    "        - Se calculan las $m$ funciones de pérdida;\n",
    "        - Usando solo esos $m$ clientes, se calculan las $p$ derivadas parciales.\n",
    "        - Se actualizan los $p$ parámetros promediando las derivadas respectivas.\n",
    "\n",
    "    - **Minibatch 2**:\n",
    "        - Se toman los clientes del $m+1$ al $2m$;\n",
    "        - Se calculan las $m$ predicciones;\n",
    "        - Se calculan las $m$ funciones de pérdida;\n",
    "        - Usando solo esos $m$ clientes, se calculan las $p$ derivadas parciales.\n",
    "        - Se actualizan los $p$ parámetros promediando las derivadas respectivas. \n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "    - **Minibatch $q$**:\n",
    "        - Se toman los clientes del $(q-1)m+1$ al $qm$;\n",
    "        - Se calculan las $m$ predicciones;\n",
    "        - Se calculan las $m$ funciones de pérdida;\n",
    "        - Usando solo esos $m$ clientes, se calculan las $p$ derivadas parciales.\n",
    "        - Se actualizan los $p$ parámetros promediando las derivadas respectivas.\n",
    "    \n",
    "\n",
    "    - **Minibatch r (si $r\\neq 0$)**, también llamado **minibatch parcial**.\n",
    "        - Se toman los clientes del $qm+1$ al $qm+r=n$;\n",
    "        - Se calculan las $r$ predicciones;\n",
    "        - Se calculan las $r$ funciones de pérdida;\n",
    "        - Usando solo esos $r$ clientes, se calculan las $p$ derivadas parciales.\n",
    "        - Se actualizan los $p$ parámetros promediando las derivadas respectivas.\n",
    "\n",
    "---\n",
    "\n",
    "- **Época 2**: Se reordena la fila de clientes y se sigue exactamente el mismo recorrido que describimos para la época 1.   \n",
    "\n",
    "---\n",
    "\n",
    "En el caso en que $m=n$ estamos en el batch gradiente descendente; en el caso en que $m=1$ estamos en el gradiente estocástico.\n",
    "\n",
    "Esto termina toda la arquitectura de la red."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
