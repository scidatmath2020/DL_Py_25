{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NOTEBOOK: Clasificaci√≥n de IM√ÅGENES + CNN\n",
        "# Robustez: split por archivos, class_weight autom√°tico, OOM-safe (T4),\n",
        "#           persistencia (weights.best.keras)\n",
        "#\n",
        "# ENTRADA:\n",
        "#   1) Subes un ZIP manualmente al runtime de Colab (/content/*.zip)\n",
        "#   2) Estructura interna del ZIP (recomendado):\n",
        "#        <raiz>/\n",
        "#          clase_0/   (jpg/png/...)\n",
        "#          clase_1/\n",
        "#          ...\n",
        "#\n",
        "# SALIDAS:\n",
        "#   - weights.best.keras\n",
        "# ============================================================\n",
        "\n",
        "# =========================\n",
        "# CELDA 0 ‚Äî CONFIG GLOBAL + ZIP ‚Üí WORKDIR + autodetecci√≥n DATA_DIR\n",
        "# Supuesto: SIEMPRE subes un .zip manualmente al runtime (/content)\n",
        "# =========================\n",
        "import os, glob, zipfile, shutil, random, math, time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# -------------------------\n",
        "# 1) CONFIGURACI√ìN GENERAL (SOLO IMAGEN)\n",
        "# -------------------------\n",
        "WORKDIR = \"/content/dataset\"\n",
        "CLEAN_WORKDIR = True\n",
        "\n",
        "SEED = 123\n",
        "TRAIN_FRAC = 0.70\n",
        "VAL_FRAC   = 0.15\n",
        "TEST_FRAC  = 0.15\n",
        "\n",
        "AUTO = True\n",
        "IMG_SIZE_MANUAL = (128, 128)   # si AUTO=False\n",
        "BATCH_MANUAL    = 32           # si AUTO=False\n",
        "\n",
        "# -------------------------\n",
        "# 2) OPTIMIZACI√ìN GPU T4\n",
        "# -------------------------\n",
        "USE_MIXED_PRECISION = True\n",
        "if USE_MIXED_PRECISION:\n",
        "    try:\n",
        "        from tensorflow.keras import mixed_precision\n",
        "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "        print(\"Mixed precision activada:\", mixed_precision.global_policy())\n",
        "    except Exception as e:\n",
        "        print(\"No se pudo activar mixed precision:\", e)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# 3) DETECTAR ZIP SUBIDO\n",
        "# -------------------------\n",
        "zips = glob.glob(\"/content/*.zip\")\n",
        "assert len(zips) > 0, \"No se encontr√≥ ning√∫n .zip en /content. S√∫belo manualmente al runtime.\"\n",
        "zip_name = max(zips, key=os.path.getmtime)\n",
        "\n",
        "print(\"ZIP detectado:\", os.path.basename(zip_name))\n",
        "print(\"√öltima modificaci√≥n:\", time.ctime(os.path.getmtime(zip_name)))\n",
        "\n",
        "# -------------------------\n",
        "# 4) PREPARAR WORKDIR Y DESCOMPRIMIR\n",
        "# -------------------------\n",
        "if CLEAN_WORKDIR and os.path.isdir(WORKDIR):\n",
        "    shutil.rmtree(WORKDIR)\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_name, \"r\") as z:\n",
        "    z.extractall(WORKDIR)\n",
        "\n",
        "print(\"Dataset extra√≠do en:\", WORKDIR)\n",
        "!ls -lah \"{WORKDIR}\"\n",
        "\n",
        "# -------------------------\n",
        "# 5) AUTODETECTAR DATA_DIR REAL\n",
        "#   buscamos una carpeta que contenga subcarpetas con archivos v√°lidos\n",
        "# -------------------------\n",
        "def find_data_root_images(workdir):\n",
        "    exts = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n",
        "\n",
        "    candidates = [workdir]\n",
        "    candidates += [os.path.join(workdir, d) for d in os.listdir(workdir) if os.path.isdir(os.path.join(workdir, d))]\n",
        "\n",
        "    def score_dir(d):\n",
        "        if not os.path.isdir(d):\n",
        "            return -1, 0, 0\n",
        "        subdirs = [os.path.join(d, s) for s in os.listdir(d) if os.path.isdir(os.path.join(d, s))]\n",
        "        if not subdirs:\n",
        "            return -1, 0, 0\n",
        "\n",
        "        good_folders = 0\n",
        "        total_files = 0\n",
        "        for sd in subdirs:\n",
        "            n = 0\n",
        "            for ext in exts:\n",
        "                n += len(glob.glob(os.path.join(sd, f\"*{ext}\")))\n",
        "            if n > 0:\n",
        "                good_folders += 1\n",
        "                total_files += n\n",
        "        return good_folders, total_files, len(subdirs)\n",
        "\n",
        "    best = None\n",
        "    best_score = (-1, -1, -1)\n",
        "    for c in candidates:\n",
        "        sc = score_dir(c)\n",
        "        if sc > best_score:\n",
        "            best_score = sc\n",
        "            best = c\n",
        "\n",
        "    if best is None or best_score[0] < 2:\n",
        "        raise ValueError(\n",
        "            \"No pude detectar una ra√≠z de dataset v√°lida.\\n\"\n",
        "            \"Aseg√∫rate de que el ZIP contenga una carpeta con subcarpetas y archivos de imagen.\\n\"\n",
        "            f\"WORKDIR={workdir}\"\n",
        "        )\n",
        "\n",
        "    return best, best_score\n",
        "\n",
        "DATA_DIR, sc = find_data_root_images(WORKDIR)\n",
        "\n",
        "print(\"\\nCONFIG FINAL:\")\n",
        "print(\"  WORKDIR   :\", WORKDIR)\n",
        "print(\"  DATA_DIR  :\", DATA_DIR)\n",
        "print(\"  (folders_con_archivos, total_archivos, subcarpetas):\", sc)\n",
        "print(\"  GPU       :\", tf.config.list_physical_devices(\"GPU\"))\n"
      ],
      "metadata": {
        "id": "ugkPYVSui6KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# CELDA 1 ‚Äî UTILIDADES: CLASES + SPLIT + DESBALANCE + VALIDACI√ìN FUERTE (IMAGEN)\n",
        "# ======================================\n",
        "import numpy as np\n",
        "import os, glob\n",
        "\n",
        "def list_class_folders(data_dir):\n",
        "    classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "    if len(classes) < 2:\n",
        "        raise ValueError(f\"Se requieren >=2 subcarpetas en: {data_dir}. Encontr√©: {classes}\")\n",
        "    return classes\n",
        "\n",
        "def list_files_by_class_image(data_dir):\n",
        "    classes = list_class_folders(data_dir)\n",
        "    exts = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n",
        "\n",
        "    files = []\n",
        "    labels = []\n",
        "    per_class = []\n",
        "\n",
        "    for i, cls in enumerate(classes):\n",
        "        cls_dir = os.path.join(data_dir, cls)\n",
        "        cls_files = []\n",
        "        for ext in exts:\n",
        "            cls_files.extend(glob.glob(os.path.join(cls_dir, f\"*{ext}\")))\n",
        "        cls_files = sorted(cls_files)\n",
        "\n",
        "        per_class.append((cls, len(cls_files)))\n",
        "        files.extend(cls_files)\n",
        "        labels.extend([i]*len(cls_files))\n",
        "\n",
        "    return classes, np.array(files), np.array(labels, dtype=np.int32), per_class\n",
        "\n",
        "def stratified_split(files, labels, train_frac, val_frac, test_frac, seed=123):\n",
        "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-9\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(len(files))\n",
        "\n",
        "    train_idx, val_idx, test_idx = [], [], []\n",
        "    for c in np.unique(labels):\n",
        "        c_idx = idx[labels == c]\n",
        "        rng.shuffle(c_idx)\n",
        "        n = len(c_idx)\n",
        "        if n == 0:\n",
        "            continue\n",
        "\n",
        "        n_train = int(round(n * train_frac))\n",
        "        n_val   = int(round(n * val_frac))\n",
        "\n",
        "        n_train = max(1, min(n_train, n))\n",
        "        n_val = min(n_val, n - n_train)\n",
        "\n",
        "        train_idx.extend(c_idx[:n_train])\n",
        "        val_idx.extend(c_idx[n_train:n_train+n_val])\n",
        "        test_idx.extend(c_idx[n_train+n_val:])\n",
        "\n",
        "    rng.shuffle(train_idx); rng.shuffle(val_idx); rng.shuffle(test_idx)\n",
        "    return np.array(train_idx), np.array(val_idx), np.array(test_idx)\n",
        "\n",
        "def compute_class_weight(train_labels, num_classes):\n",
        "    counts = np.bincount(train_labels, minlength=num_classes).astype(np.int64)\n",
        "    N = counts.sum()\n",
        "    weights = {}\n",
        "    for c in range(num_classes):\n",
        "        weights[c] = 0.0 if counts[c] == 0 else float(N) / float(num_classes * counts[c])\n",
        "    return counts, weights\n",
        "\n",
        "# -------------------------\n",
        "# CARGA (IMAGEN)\n",
        "# -------------------------\n",
        "classes, all_files, all_labels, per_class = list_files_by_class_image(DATA_DIR)\n",
        "num_classes = len(classes)\n",
        "\n",
        "train_idx, val_idx, test_idx = stratified_split(all_files, all_labels, TRAIN_FRAC, VAL_FRAC, TEST_FRAC, SEED)\n",
        "\n",
        "# -------------------------\n",
        "# PRINTS + VALIDACI√ìN FUERTE\n",
        "# -------------------------\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"Num clases:\", num_classes)\n",
        "print(\"Total ejemplos:\", len(all_files))\n",
        "print(\"\\nConteo por clase (primeras 20):\")\n",
        "for cls, n in per_class[:20]:\n",
        "    print(f\"  {cls:<30s} {n}\")\n",
        "if len(per_class) > 20:\n",
        "    print(\"  ...\")\n",
        "\n",
        "if len(all_files) == 0:\n",
        "    raise ValueError(\n",
        "        \"No se encontr√≥ ning√∫n archivo de imagen v√°lido en las carpetas.\\n\"\n",
        "        f\"DATA_DIR={DATA_DIR}\"\n",
        "    )\n",
        "\n",
        "print(\"\\nSplit tama√±os:\", \"train\", len(train_idx), \"| val\", len(val_idx), \"| test\", len(test_idx))\n",
        "if len(train_idx) == 0 or len(val_idx) == 0 or len(test_idx) == 0:\n",
        "    raise ValueError(\n",
        "        \"Alguno de los splits qued√≥ vac√≠o. Revisa que haya suficientes ejemplos.\\n\"\n",
        "        f\"train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\"\n",
        "    )\n",
        "\n",
        "# Desbalance (usa TRAIN)\n",
        "train_labels = all_labels[train_idx]\n",
        "class_counts, class_weight = compute_class_weight(train_labels, num_classes)\n",
        "\n",
        "min_count = int(class_counts.min()) if len(class_counts) else 0\n",
        "max_count = int(class_counts.max()) if len(class_counts) else 0\n",
        "imbalance_ratio = (max_count / min_count) if (min_count > 0) else float(\"inf\")\n",
        "\n",
        "IMBALANCED = (imbalance_ratio >= 2.0) or (min_count <= 10)\n",
        "\n",
        "TINY_CLASS_THRESHOLD = 5\n",
        "RARE_CLASS_THRESHOLD = 10\n",
        "\n",
        "tiny_idx = np.where(class_counts <= TINY_CLASS_THRESHOLD)[0]\n",
        "rare_idx = np.where((class_counts > TINY_CLASS_THRESHOLD) & (class_counts <= RARE_CLASS_THRESHOLD))[0]\n",
        "zero_idx = np.where(class_counts == 0)[0]\n",
        "\n",
        "HAS_TINY_CLASSES = len(tiny_idx) > 0\n",
        "HAS_RARE_CLASSES = len(rare_idx) > 0\n",
        "\n",
        "USE_CLASS_WEIGHT = IMBALANCED or HAS_TINY_CLASSES or HAS_RARE_CLASSES\n",
        "MONITOR_METRIC = \"val_loss\" if (IMBALANCED or HAS_TINY_CLASSES) else \"val_accuracy\"\n",
        "\n",
        "print(\"\\nDistribuci√≥n TRAIN: min\", min_count, \"| max\", max_count, \"| ratio\", imbalance_ratio)\n",
        "print(\"IMBALANCED:\", IMBALANCED)\n",
        "print(\"HAS_TINY_CLASSES:\", HAS_TINY_CLASSES, \"| HAS_RARE_CLASSES:\", HAS_RARE_CLASSES)\n",
        "print(\"USE_CLASS_WEIGHT:\", USE_CLASS_WEIGHT)\n",
        "print(\"MONITOR_METRIC:\", MONITOR_METRIC)\n",
        "\n",
        "print(\"\\n=== SANITY CHECK SPLITS ===\")\n",
        "print(\"Labels min/max:\", int(all_labels.min()), int(all_labels.max()))\n",
        "print(\"Num clases declarado:\", num_classes)\n",
        "\n",
        "def bincountK(y, K):\n",
        "    return np.bincount(y, minlength=K)\n",
        "\n",
        "print(\"Train per class:\", bincountK(all_labels[train_idx], num_classes).tolist())\n",
        "print(\"Val   per class:\", bincountK(all_labels[val_idx],   num_classes).tolist())\n",
        "print(\"Test  per class:\", bincountK(all_labels[test_idx],  num_classes).tolist())\n"
      ],
      "metadata": {
        "id": "hOByzwWQjFVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 2 ‚Äî AUTO-CONFIG (IMG_SIZE + BATCH) + CHANNELS (IMAGEN)\n",
        "# ==========================================================\n",
        "import math\n",
        "import PIL.Image\n",
        "\n",
        "def choose_img_size_and_batch(sample_shape, t4=True):\n",
        "    H, W, C = sample_shape\n",
        "    m = min(H, W)\n",
        "    if m <= 32:\n",
        "        img = (32, 32);  batch = 256\n",
        "    elif m <= 96:\n",
        "        img = (96, 96);  batch = 64\n",
        "    else:\n",
        "        img = (128, 128); batch = 32\n",
        "    return img, batch, C\n",
        "\n",
        "# ---- Inferir shape/canales con una muestra ----\n",
        "p0 = all_files[0]\n",
        "im = np.array(PIL.Image.open(p0))\n",
        "if im.ndim == 2:\n",
        "    H, W = im.shape\n",
        "    C = 1\n",
        "else:\n",
        "    H, W, C = im.shape\n",
        "sample_shape = (H, W, C)\n",
        "\n",
        "if AUTO:\n",
        "    IMG_SIZE, BATCH, CHANNELS = choose_img_size_and_batch(sample_shape, t4=True)\n",
        "else:\n",
        "    IMG_SIZE = IMG_SIZE_MANUAL\n",
        "    BATCH = BATCH_MANUAL\n",
        "    CHANNELS = 3  # si tus im√°genes son RGB; si son grises, cambia a 1\n",
        "\n",
        "print(\"AUTO:\", AUTO)\n",
        "print(\"IMG_SIZE:\", IMG_SIZE)\n",
        "print(\"BATCH:\", BATCH)\n",
        "print(\"CHANNELS:\", CHANNELS)\n"
      ],
      "metadata": {
        "id": "gVmukbHEjIRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 3 ‚Äî PIPELINE IMAGEN (tf.data) + build_datasets(batch)\n",
        "# ==========================================================\n",
        "import tensorflow as tf\n",
        "\n",
        "def decode_image(path, label, img_size, channels):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_image(img, channels=channels, expand_animations=False)\n",
        "    img = tf.image.resize(img, img_size, antialias=True)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img, label\n",
        "\n",
        "def make_image_ds(files, labels, img_size, channels, batch, training=False, seed=123):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
        "    if training:\n",
        "        ds = ds.shuffle(len(files), seed=seed, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(lambda p,y: decode_image(p,y,img_size,channels), num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "def build_datasets(batch):\n",
        "    train_ds = make_image_ds(all_files[train_idx], all_labels[train_idx], IMG_SIZE, CHANNELS, batch, training=True, seed=SEED)\n",
        "    val_ds   = make_image_ds(all_files[val_idx],   all_labels[val_idx],   IMG_SIZE, CHANNELS, batch, training=False)\n",
        "    test_ds  = make_image_ds(all_files[test_idx],  all_labels[test_idx],  IMG_SIZE, CHANNELS, batch, training=False)\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "train_ds, val_ds, test_ds = build_datasets(BATCH)\n",
        "print(\"Datasets listos (imagen). BATCH =\", BATCH)\n"
      ],
      "metadata": {
        "id": "gBGCDVXgjCFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 5 ‚Äî AUGMENT ROBUSTO (IMAGEN)\n",
        "# ==========================================================\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "augment = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.10),\n",
        "], name=\"augment_image\")\n"
      ],
      "metadata": {
        "id": "i6kqduwRjPIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 6 ‚Äî MODELO CNN ROBUSTO (GAP + BN + Dropout)\n",
        "# ==========================================================\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_cnn(img_size, channels, num_classes, augment_layer):\n",
        "    inputs = layers.Input(shape=(img_size[0], img_size[1], channels))\n",
        "    x = augment_layer(inputs)\n",
        "\n",
        "    x = layers.Conv2D(32, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "\n",
        "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.30)(x)\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "model = build_cnn(IMG_SIZE, CHANNELS, num_classes, augment)\n",
        "\n",
        "metrics = [\"accuracy\"]\n",
        "if num_classes >= 10:\n",
        "    metrics.append(tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"top5_acc\"))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "xFeUbKiEjRaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 7 ‚Äî TRAIN (OOM-safe + class_weight + pol√≠tica robusta)\n",
        "# ==========================================================\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "if HAS_TINY_CLASSES:\n",
        "    LR = 5e-4\n",
        "    PATIENCE = 8\n",
        "else:\n",
        "    LR = 1e-3\n",
        "    PATIENCE = 5\n",
        "\n",
        "try:\n",
        "    model.optimizer.learning_rate.assign(LR)\n",
        "except Exception:\n",
        "    model.optimizer.learning_rate = LR\n",
        "\n",
        "print(\"LR usado:\", LR)\n",
        "print(\"PATIENCE usado:\", PATIENCE)\n",
        "print(\"MONITOR_METRIC:\", MONITOR_METRIC)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=MONITOR_METRIC,\n",
        "        patience=PATIENCE,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"/content/weights.best.keras\",\n",
        "        monitor=MONITOR_METRIC,\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "fit_class_weight = class_weight if USE_CLASS_WEIGHT else None\n",
        "\n",
        "def batch_candidates(b0):\n",
        "    cands = [int(b0)]\n",
        "    while cands[-1] > 8:\n",
        "        cands.append(cands[-1] // 2)\n",
        "    cands = sorted(set([b for b in cands if b >= 8]), reverse=True)\n",
        "    return cands\n",
        "\n",
        "BATCH_TRIES = batch_candidates(BATCH)\n",
        "print(\"BATCH tries:\", BATCH_TRIES)\n",
        "\n",
        "history = None\n",
        "last_err = None\n",
        "\n",
        "for b_try in BATCH_TRIES:\n",
        "    try:\n",
        "        train_ds, val_ds, test_ds = build_datasets(b_try)\n",
        "\n",
        "        print(f\"\\nEntrenando con BATCH={b_try} | monitor={MONITOR_METRIC} | class_weight={USE_CLASS_WEIGHT}\")\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=30,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=fit_class_weight\n",
        "        )\n",
        "        BATCH = b_try\n",
        "        last_err = None\n",
        "        break\n",
        "\n",
        "    except tf.errors.ResourceExhaustedError as e:\n",
        "        last_err = e\n",
        "        print(f\"\\n‚ö†Ô∏è OOM con BATCH={b_try}. Reintentando con batch menor...\")\n",
        "        try:\n",
        "            del train_ds, val_ds, test_ds\n",
        "        except Exception:\n",
        "            pass\n",
        "        gc.collect()\n",
        "\n",
        "if history is None and last_err is not None:\n",
        "    raise last_err\n",
        "\n",
        "print(\"\\n‚úÖ Entrenamiento finalizado. BATCH final usado:\", BATCH)\n",
        "\n",
        "# -------------------------\n",
        "# INFORME DE EPOCH FINAL (REAL)\n",
        "# -------------------------\n",
        "hist = history.history\n",
        "mon = MONITOR_METRIC\n",
        "\n",
        "if mon in hist:\n",
        "    if \"acc\" in mon:\n",
        "        best_epoch = int(np.argmax(hist[mon]) + 1)\n",
        "        best_value = float(np.max(hist[mon]))\n",
        "        mode = \"max\"\n",
        "    else:\n",
        "        best_epoch = int(np.argmin(hist[mon]) + 1)\n",
        "        best_value = float(np.min(hist[mon]))\n",
        "        mode = \"min\"\n",
        "\n",
        "    print(\"\\nüìå RESUMEN DE ENTRENAMIENTO\")\n",
        "    print(f\"Monitor usado      : {mon} ({mode})\")\n",
        "    print(f\"Epoch seleccionado : {best_epoch}\")\n",
        "    print(f\"Mejor {mon}        : {best_value:.4f}\")\n",
        "    print(\"‚úî restore_best_weights=True ‚Üí el modelo en memoria qued√≥ en ese epoch\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No se pudo determinar el epoch final (monitor no encontrado).\")\n",
        "    print(\"Keys disponibles:\", list(hist.keys()))\n"
      ],
      "metadata": {
        "id": "WZTWdIO2jTvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 7.5 ‚Äî DIAGN√ìSTICO AUTOM√ÅTICO DEL ENTRENAMIENTO (mejorado)\n",
        "# ==========================================================\n",
        "import numpy as np\n",
        "\n",
        "def diagnose_training_v2(history, num_classes, monitor_metric=\"val_loss\", patience=None):\n",
        "    h = history.history\n",
        "    epochs_ran = len(next(iter(h.values()))) if len(h) else 0\n",
        "\n",
        "    def arr(key):\n",
        "        v = h.get(key, None)\n",
        "        return None if v is None else np.array(v, dtype=float)\n",
        "\n",
        "    acc   = arr(\"accuracy\")\n",
        "    vacc  = arr(\"val_accuracy\")\n",
        "    loss  = arr(\"loss\")\n",
        "    vloss = arr(\"val_loss\")\n",
        "\n",
        "    chance = 1.0 / float(num_classes) if num_classes else np.nan\n",
        "\n",
        "    mon = arr(monitor_metric)\n",
        "    if mon is None:\n",
        "        print(\"‚ö†Ô∏è No existe monitor_metric en history:\", monitor_metric)\n",
        "        print(\"Keys:\", list(h.keys()))\n",
        "        return\n",
        "\n",
        "    if \"acc\" in monitor_metric:\n",
        "        best_i = int(np.nanargmax(mon))\n",
        "        best_val = float(np.nanmax(mon))\n",
        "        mode = \"max\"\n",
        "    else:\n",
        "        best_i = int(np.nanargmin(mon))\n",
        "        best_val = float(np.nanmin(mon))\n",
        "        mode = \"min\"\n",
        "\n",
        "    def safe_get(a, i):\n",
        "        return float(a[i]) if a is not None and len(a) > i else np.nan\n",
        "\n",
        "    last_i = epochs_ran - 1\n",
        "\n",
        "    last_acc  = safe_get(acc, last_i)\n",
        "    last_vacc = safe_get(vacc, last_i)\n",
        "    last_loss = safe_get(loss, last_i)\n",
        "    last_vloss= safe_get(vloss, last_i)\n",
        "\n",
        "    best_acc  = safe_get(acc, best_i)\n",
        "    best_vacc = safe_get(vacc, best_i)\n",
        "    best_loss = safe_get(loss, best_i)\n",
        "    best_vloss= safe_get(vloss, best_i)\n",
        "\n",
        "    degrade_loss = (not np.isnan(best_vloss) and not np.isnan(last_vloss) and last_vloss > best_vloss * 1.15)\n",
        "    degrade_acc  = (not np.isnan(best_vacc) and not np.isnan(last_vacc) and last_vacc < best_vacc - 0.07)\n",
        "\n",
        "    gap_best = best_acc - best_vacc if (not np.isnan(best_acc) and not np.isnan(best_vacc)) else np.nan\n",
        "    gap_last = last_acc - last_vacc if (not np.isnan(last_acc) and not np.isnan(last_vacc)) else np.nan\n",
        "\n",
        "    def slope(a):\n",
        "        if a is None or len(a) < 6:\n",
        "            return np.nan\n",
        "        y = a[-5:]\n",
        "        x = np.arange(len(y), dtype=float)\n",
        "        return float(np.polyfit(x, y, 1)[0])\n",
        "\n",
        "    s_acc  = slope(acc)\n",
        "    s_vacc = slope(vacc)\n",
        "    s_loss = slope(loss)\n",
        "    s_vloss= slope(vloss)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DIAGN√ìSTICO 7.5 ‚Äî RESUMEN (v2)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Clases: {num_classes} | azar‚âà {chance:.4f} | epochs corridos: {epochs_ran}\")\n",
        "    print(f\"Monitor: {monitor_metric} ({mode}) | best_epoch={best_i+1} | best={best_val:.4f}\")\n",
        "    if patience is not None:\n",
        "        print(f\"Patience: {patience}\")\n",
        "\n",
        "    print(\"\\n‚Äî En BEST epoch (lo que queda en memoria si restore_best_weights=True) ‚Äî\")\n",
        "    print(f\"  acc={best_acc:.4f} | val_acc={best_vacc:.4f} | loss={best_loss:.4f} | val_loss={best_vloss:.4f}\")\n",
        "    print(f\"  gap(train-val) en BEST: {gap_best:.4f}\")\n",
        "\n",
        "    print(\"\\n‚Äî En √öLTIMO epoch entrenado (solo para ver tendencia) ‚Äî\")\n",
        "    print(f\"  acc={last_acc:.4f} | val_acc={last_vacc:.4f} | loss={last_loss:.4f} | val_loss={last_vloss:.4f}\")\n",
        "    print(f\"  gap(train-val) en √öLTIMO: {gap_last:.4f}\")\n",
        "    print(f\"  slopes √∫ltimos 5: acc_tr={s_acc:.4f}, acc_val={s_vacc:.4f}, loss_tr={s_loss:.4f}, loss_val={s_vloss:.4f}\")\n",
        "\n",
        "    near_chance = chance + 0.03\n",
        "\n",
        "    if not np.isnan(best_vacc) and best_vacc <= near_chance:\n",
        "        print(\"\\n‚ö†Ô∏è VALIDACI√ìN CERCA DE AZAR (pipeline/labels/split sospechoso)\")\n",
        "        print(\"Acciones: revisar DATA_DIR, clases, etiquetas, y que train/val/test tengan todas las clases.\")\n",
        "        return\n",
        "\n",
        "    if (not np.isnan(best_acc) and best_acc < 0.60) and (not np.isnan(best_vacc) and best_vacc < 0.60):\n",
        "        print(\"\\nüü° SUBAPRENDIZAJE (UNDERFITTING)\")\n",
        "        print(\"Acciones: m√°s capacidad, m√°s epochs, revisar representaci√≥n/IMG_SIZE, LR, etc.\")\n",
        "        return\n",
        "\n",
        "    if (degrade_loss or degrade_acc) and (not np.isnan(gap_best) and gap_best >= 0.12):\n",
        "        print(\"\\nüî¥ OVERFITTING (MEMORIZACI√ìN) DESPU√âS DEL BEST\")\n",
        "        print(\"Se√±al: el modelo mejor√≥ y luego empeor√≥ en validaci√≥n.\")\n",
        "        print(\"Recomendaciones:\")\n",
        "        print(\"  - Qu√©date con el modelo del best_epoch (ya queda restaurado si restore_best_weights=True).\")\n",
        "        print(\"  - Baja patience (p.ej. 3‚Äì4) o limita epochs.\")\n",
        "        print(\"  - Aumenta augment y/o sube dropout.\")\n",
        "        print(\"  - Si hay duplicados muy parecidos: split por grupo (escena/personaje/objeto).\")\n",
        "        return\n",
        "\n",
        "    if (not np.isnan(best_vacc) and best_vacc > chance + 0.20) and (not np.isnan(gap_best) and gap_best <= 0.12):\n",
        "        print(\"\\n‚úÖ TODO BIEN / GENERALIZA RAZONABLEMENTE\")\n",
        "        print(\"Recomendaciones leves: afinar LR, scheduler, o un poco m√°s de capacidad.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüü¢ MIXTO (pero NO roto): aprende, con margen de mejora\")\n",
        "    print(\"Sugerencias:\")\n",
        "    print(\"  - Si gap es alto: m√°s regularizaci√≥n/augment o stopping m√°s agresivo.\")\n",
        "    print(\"  - Si val se estanca: LR menor o scheduler.\")\n",
        "    print(\"Nota: si restore_best_weights=True, el modelo final es el del best_epoch.\")\n",
        "\n",
        "diagnose_training_v2(history, num_classes=num_classes, monitor_metric=MONITOR_METRIC, patience=PATIENCE)\n"
      ],
      "metadata": {
        "id": "KPvT_JrdmvT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 8 ‚Äî EVALUACI√ìN EN TEST\n",
        "# ==========================================================\n",
        "test_out = model.evaluate(test_ds, verbose=0)\n",
        "print(\"TEST metrics:\")\n",
        "for name, val in zip(model.metrics_names, test_out):\n",
        "    print(f\"  {name:>12s}: {val:.4f}\")\n"
      ],
      "metadata": {
        "id": "lbVPmtNgmzGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 9 ‚Äî REPORTE + MATRIZ DE CONFUSI√ìN\n",
        "# ==========================================================\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for x, y in test_ds:\n",
        "    p = model.predict(x, verbose=0)\n",
        "    y_true.extend(y.numpy().tolist())\n",
        "    y_pred.extend(np.argmax(p, axis=1).tolist())\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Matriz de confusi√≥n shape:\", cm.shape)\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
        ""
      ],
      "metadata": {
        "id": "GfvJcBxKm31d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELDA 10 ‚Äî EXPORTAR \"resultados.zip\" (inferencia reproducible)\n",
        "# Contiene:\n",
        "#   - model.keras (modelo completo)\n",
        "#   - weights.best.keras (pesos del mejor checkpoint, si existe)\n",
        "#   - metadata.json (config + clases + par√°metros)\n",
        "#   - infer_from_zip.py (script para predecir un ZIP nuevo)\n",
        "#   - README_INFERENCIA.txt (gu√≠a r√°pida)\n",
        "# Salida: /content/resultados.zip\n",
        "# ==========================================================\n",
        "import os, json, zipfile, shutil, time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "OUT_ZIP = \"/content/resultados.zip\"\n",
        "BUNDLE_DIR = \"/content/_bundle_resultados\"\n",
        "\n",
        "# Limpia bundle\n",
        "if os.path.isdir(BUNDLE_DIR):\n",
        "    shutil.rmtree(BUNDLE_DIR)\n",
        "os.makedirs(BUNDLE_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Guardar modelo completo (incluye arquitectura + pesos actuales en memoria)\n",
        "MODEL_PATH = os.path.join(BUNDLE_DIR, \"model.keras\")\n",
        "model.save(MODEL_PATH)\n",
        "\n",
        "# 2) Copiar checkpoint best si existe\n",
        "WEIGHTS_SRC = \"/content/weights.best.keras\"\n",
        "WEIGHTS_DST = os.path.join(BUNDLE_DIR, \"weights.best.keras\")\n",
        "if os.path.isfile(WEIGHTS_SRC):\n",
        "    shutil.copy2(WEIGHTS_SRC, WEIGHTS_DST)\n",
        "\n",
        "# 3) Guardar metadata\n",
        "meta = {\n",
        "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"zip_train_source\": os.path.basename(zip_name) if \"zip_name\" in globals() else None,\n",
        "    \"data_dir_train\": DATA_DIR if \"DATA_DIR\" in globals() else None,\n",
        "    \"seed\": int(SEED),\n",
        "    \"train_frac\": float(TRAIN_FRAC),\n",
        "    \"val_frac\": float(VAL_FRAC),\n",
        "    \"test_frac\": float(TEST_FRAC),\n",
        "    \"auto\": bool(AUTO),\n",
        "    \"img_size\": [int(IMG_SIZE[0]), int(IMG_SIZE[1])],\n",
        "    \"channels\": int(CHANNELS),\n",
        "    \"batch_final\": int(BATCH),\n",
        "    \"num_classes\": int(num_classes),\n",
        "    \"classes\": list(classes),\n",
        "    \"normalize\": \"x/255.0\",\n",
        "    \"decoder\": \"tf.io.decode_image(channels=CHANNELS, expand_animations=False)\",\n",
        "    \"resize\": \"tf.image.resize(img_size, antialias=True)\",\n",
        "    \"prediction\": {\n",
        "        \"type\": \"multiclass\",\n",
        "        \"activation\": \"softmax\",\n",
        "        \"label_type\": \"int index -> classes[index]\"\n",
        "    }\n",
        "}\n",
        "with open(os.path.join(BUNDLE_DIR, \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# 4) Script de inferencia desde un ZIP nuevo\n",
        "infer_py = r'''\n",
        "import os, glob, zipfile, shutil, json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "WORKDIR = \"/content/new_zip_workdir\"\n",
        "EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n",
        "\n",
        "def find_images_root(workdir):\n",
        "    # Permite: ra√≠z con im√°genes, o subcarpetas con im√°genes (cualquier profundidad 1)\n",
        "    candidates = [workdir] + [os.path.join(workdir, d) for d in os.listdir(workdir) if os.path.isdir(os.path.join(workdir, d))]\n",
        "\n",
        "    def score_dir(d):\n",
        "        if not os.path.isdir(d): return (-1, -1)\n",
        "        n = 0\n",
        "        for ext in EXTS:\n",
        "            n += len(glob.glob(os.path.join(d, f\"*{ext}\")))\n",
        "        # si tiene subcarpetas, suma tambi√©n im√°genes dentro de cada subcarpeta (1 nivel)\n",
        "        subdirs = [os.path.join(d, s) for s in os.listdir(d) if os.path.isdir(os.path.join(d, s))]\n",
        "        n2 = 0\n",
        "        for sd in subdirs:\n",
        "            for ext in EXTS:\n",
        "                n2 += len(glob.glob(os.path.join(sd, f\"*{ext}\")))\n",
        "        return (n, n2)\n",
        "\n",
        "    best = None\n",
        "    best_sc = (-1, -1)\n",
        "    for c in candidates:\n",
        "        sc = score_dir(c)\n",
        "        if sc > best_sc:\n",
        "            best_sc = sc\n",
        "            best = c\n",
        "    if best is None or (best_sc[0] + best_sc[1]) == 0:\n",
        "        raise ValueError(f\"No se encontraron im√°genes en {workdir}\")\n",
        "    return best\n",
        "\n",
        "def list_all_images(root_dir):\n",
        "    # Si root_dir tiene im√°genes directas, usa esas; si no, usa todas las subcarpetas (1 nivel)\n",
        "    direct = []\n",
        "    for ext in EXTS:\n",
        "        direct += glob.glob(os.path.join(root_dir, f\"*{ext}\"))\n",
        "    direct = sorted(direct)\n",
        "    if len(direct) > 0:\n",
        "        return direct\n",
        "\n",
        "    files = []\n",
        "    subdirs = [os.path.join(root_dir, s) for s in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, s))]\n",
        "    for sd in sorted(subdirs):\n",
        "        for ext in EXTS:\n",
        "            files += glob.glob(os.path.join(sd, f\"*{ext}\"))\n",
        "    return sorted(files)\n",
        "\n",
        "def decode_image(path, img_size, channels):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_image(img, channels=channels, expand_animations=False)\n",
        "    img = tf.image.resize(img, img_size, antialias=True)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "def make_ds(files, img_size, channels, batch):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "    ds = ds.map(lambda p: decode_image(p, img_size, channels), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "def predict_zip(zip_path, model_path=\"model.keras\", metadata_path=\"metadata.json\", out_csv=\"/content/predicciones.csv\"):\n",
        "    # Cargar metadata\n",
        "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        meta = json.load(f)\n",
        "    img_size = tuple(meta[\"img_size\"])\n",
        "    channels = int(meta[\"channels\"])\n",
        "    classes = meta[\"classes\"]\n",
        "    batch = int(meta.get(\"batch_final\", 32))\n",
        "\n",
        "    # Preparar workdir\n",
        "    if os.path.isdir(WORKDIR):\n",
        "        shutil.rmtree(WORKDIR)\n",
        "    os.makedirs(WORKDIR, exist_ok=True)\n",
        "\n",
        "    # Extraer zip\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(WORKDIR)\n",
        "\n",
        "    # Detectar ra√≠z con im√°genes\n",
        "    root = find_images_root(WORKDIR)\n",
        "    files = list_all_images(root)\n",
        "    if len(files) == 0:\n",
        "        raise ValueError(\"No se encontraron im√°genes para predecir.\")\n",
        "\n",
        "    # Cargar modelo\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    # Predicci√≥n\n",
        "    ds = make_ds(files, img_size, channels, batch)\n",
        "    probs = model.predict(ds, verbose=0)\n",
        "    pred_idx = np.argmax(probs, axis=1)\n",
        "    pred_name = [classes[i] for i in pred_idx]\n",
        "    conf = np.max(probs, axis=1)\n",
        "\n",
        "    # Guardar CSV\n",
        "    import csv\n",
        "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"filepath\", \"pred_idx\", \"pred_class\", \"confidence\"])\n",
        "        for p, i, c, cf in zip(files, pred_idx, pred_name, conf):\n",
        "            w.writerow([p, int(i), c, float(cf)])\n",
        "\n",
        "    print(\"OK ‚úÖ\")\n",
        "    print(\"ZIP:\", zip_path)\n",
        "    print(\"Im√°genes:\", len(files))\n",
        "    print(\"Salida CSV:\", out_csv)\n",
        "    return out_csv\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Usa el ZIP m√°s reciente en /content por defecto\n",
        "    zips = sorted(glob.glob(\"/content/*.zip\"), key=os.path.getmtime)\n",
        "    assert len(zips) > 0, \"No hay ZIP en /content\"\n",
        "    zip_path = zips[-1]\n",
        "    predict_zip(zip_path)\n",
        "'''\n",
        "with open(os.path.join(BUNDLE_DIR, \"infer_from_zip.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(infer_py.strip() + \"\\n\")\n",
        "\n",
        "# 5) README m√≠nimo\n",
        "readme = f\"\"\"\\\n",
        "RESULTADOS ‚Äî INFERENCIA DESDE UN ZIP NUEVO (IM√ÅGENES)\n",
        "\n",
        "Archivos:\n",
        "- model.keras            : modelo completo (arquitectura + pesos en memoria)\n",
        "- weights.best.keras     : checkpoint best (si existe)\n",
        "- metadata.json          : IMG_SIZE, CHANNELS, classes, etc.\n",
        "- infer_from_zip.py      : script de predicci√≥n para un ZIP nuevo\n",
        "\n",
        "USO EN COLAB:\n",
        "1) Sube resultados.zip a /content y descompr√≠melo:\n",
        "   !unzip -o /content/resultados.zip -d /content/resultados\n",
        "\n",
        "2) Sube un ZIP NUEVO de im√°genes a /content (ej: nuevo.zip)\n",
        "\n",
        "3) Corre:\n",
        "   %cd /content/resultados\n",
        "   !python infer_from_zip.py\n",
        "\n",
        "Salida:\n",
        "- /content/predicciones.csv\n",
        "\"\"\"\n",
        "with open(os.path.join(BUNDLE_DIR, \"README_INFERENCIA.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(readme)\n",
        "\n",
        "# 6) Empaquetar resultados.zip\n",
        "if os.path.isfile(OUT_ZIP):\n",
        "    os.remove(OUT_ZIP)\n",
        "\n",
        "with zipfile.ZipFile(OUT_ZIP, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    for root, _, files in os.walk(BUNDLE_DIR):\n",
        "        for fn in files:\n",
        "            abs_path = os.path.join(root, fn)\n",
        "            rel_path = os.path.relpath(abs_path, BUNDLE_DIR)\n",
        "            z.write(abs_path, rel_path)\n",
        "\n",
        "print(\"‚úÖ Creado:\", OUT_ZIP)\n",
        "!ls -lah /content/resultados.zip\n"
      ],
      "metadata": {
        "id": "xf-xGlGhpZ-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}