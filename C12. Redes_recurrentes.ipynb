{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e61300-79bf-442f-bc37-650e9bcc6b32",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd830161-9b46-4c4f-a380-b993fe333a7b",
   "metadata": {},
   "source": [
    "# Redes recurrentes\n",
    "\n",
    "En una red neuronal estándar (densa), se asume que cada dato es un vector completo y estático. Todas las características están disponibles simultáneamente y no existe noción de orden. El modelo recibe al cliente de una sola vez y produce una salida sin memoria del pasado.\n",
    "\n",
    "Este supuesto es adecuado para datos tabulares, pero falla cuando los datos presentan:\n",
    "\n",
    "- **orden temporal**,  \n",
    "- **dependencia entre observaciones**,  \n",
    "- **efectos acumulativos**,  \n",
    "- **evolución en el tiempo**.\n",
    "\n",
    "Las redes neuronales recurrentes (RNN) surgen para modelar este tipo de datos.\n",
    "\n",
    "Una RNN está diseñada para situaciones en las que el cliente **no es un objeto fijo**, sino una **historia que se desarrolla en el tiempo**. Ya no estamos frente a un edificio de información, sino ante una **secuencia ordenada de eventos**.\n",
    "\n",
    "Imagina que un adolescente llega a consultar al experto ligador más sabio del barrio. El adolescente no llega en blanco, sino que trae toda su historia emocional ordenada en el tiempo: cada mensaje enviado, cada \"visto\" sin respuesta, cada cita que salió bien (o desastrosamente mal), cada señal ambigua, cada pelea, cada reconciliación. Nada ocurre aislado, pues todo es una secuencia que se va contando paso a paso.\n",
    "\n",
    "El experto no ve al adolescente como una foto fija. Lo ve como una historia en movimiento: \"Lo que pasó en cierta ocasión cambia cómo interpreta lo de del presente, y eso condiciona lo que probablemente pase mañana\". Eso es exactamente una red neuronal recurrente (RNN).\n",
    "\n",
    "## El adolescente como serie de tiempo\n",
    "\n",
    "Cada instante en la vida amorosa del adolescente es un estado observado dentro de una secuencia:\n",
    "\n",
    "- $x_1$: el primer “hola” en Instagram  \n",
    "- $x_2$: la primera respuesta (rápida y con muchos emojis)  \n",
    "- $x_3$: el primer “ok” después de 4 horas  \n",
    "- $x_4$: la cita donde todo fluyó increíble  \n",
    "- ...\n",
    "- $x_T$: el último mensaje que acaba de llegar (“estoy ocupado” seco)\n",
    "\n",
    "Por sí solo, un “ok” no dice casi nada. Pero un “ok” después de una pelea + ghosting de 3 días + like en storie es una señal completamente distinta. Hagamos hincapié en esto: **El presente solo cobra sentido a través del pasado.**\n",
    "\n",
    "Formalmente, el adolescente no es un vector aislado, sino una secuencia ordenada $$(x_1, x_2, \\dots, x_T)$$\n",
    "\n",
    "En el marco de RNN, cada cliente es un adolescente que llega al experto con una historia emocional ordenada en el tiempo. No se presenta como una fotografía aislada, sino como una sucesión de estados observados:\n",
    "\n",
    "$$\n",
    "(x_1, x_2, \\dots, x_T)\n",
    "$$\n",
    "\n",
    "donde cada $x_t \\in \\mathbb{R}^d$ representa lo ocurrido en el instante $t$: un mensaje, una respuesta tardía, una cita, un silencio.\n",
    "\n",
    "El orden es fundamental. Intercambiar dos instantes altera completamente el significado. El presente **solo cobra sentido a través del pasado**.\n",
    "\n",
    "\n",
    "\n",
    "## El experto ligador como RNN\n",
    "\n",
    "El experto escucha la historia completa de golpe. No necesita verte después, pues procesa la secuencia paso a paso. En cada instante $t$:\n",
    "\n",
    "1. Recibe el evento nuevo $x_t$  \n",
    "2. Lo combina con todo lo que ya sabe hasta ese momento  \n",
    "3. Actualiza su diagnóstico interno\n",
    "\n",
    "Ese diagnóstico interno es el **estado oculto** $h_t$. Es decir, $h_t$ es lo que se opina del evento $x_t$ + la opinión de los eventos anteriores $h_{t-1}$. Es decir, $$h_t = f(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2002f2-5c8a-4917-bba5-421cc1efea87",
   "metadata": {},
   "source": [
    "## Ecuación de una RNN básica\n",
    "\n",
    "En una RNN elemental, la actualización del estado oculto se expresa como\n",
    "\n",
    "$$\n",
    "h_t = \\phi(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "con:\n",
    "\n",
    "- $W_x \\in \\mathbb{R}^{m \\times d}$,\n",
    "- $W_h \\in \\mathbb{R}^{m \\times m}$,\n",
    "- $b \\in \\mathbb{R}^m$,\n",
    "- $\\phi$: función no lineal (tanh o ReLU).\n",
    "\n",
    "Un punto esencial: **los mismos pesos se utilizan para todos los instantes**. No hay un experto distinto en cada tiempo; hay **uno solo que evoluciona**.\n",
    "\n",
    "## Esquemas de salida\n",
    "\n",
    "Dependiendo del problema, se consideran dos configuraciones principales:\n",
    "\n",
    "### Many-to-one\n",
    "\n",
    "Se utiliza únicamente el último estado oculto:\n",
    "\n",
    "$$\n",
    "y = g(h_T)\n",
    "$$\n",
    "\n",
    "El experto escucha toda la historia y emite un diagnóstico final.\n",
    "\n",
    "### Many-to-many\n",
    "\n",
    "Se produce una salida en cada instante:\n",
    "\n",
    "$$\n",
    "y_t = g(h_t), \\quad t = 1, \\dots, T\n",
    "$$\n",
    "\n",
    "El experto actualiza su opinión tras cada nuevo evento.\n",
    "\n",
    "## Profundidad temporal\n",
    "\n",
    "A diferencia de una CNN, donde la profundidad es espacial, en una RNN la profundidad es **temporal**.\n",
    "\n",
    "Aunque la red tenga una sola capa recurrente, al desplegarla en el tiempo se obtiene una red profunda de profundidad \\(T\\):\n",
    "\n",
    "$$\n",
    "(h_0, x_1) \\rightarrow h_1,\\;\n",
    "(h_1, x_2) \\rightarrow h_2,\\;\n",
    "\\dots,\\;\n",
    "(h_{T-1}, x_T) \\rightarrow h_T\n",
    "$$\n",
    "\n",
    "Cada paso aplica la misma transformación, acumulando información histórica.\n",
    "\n",
    "Durante el entrenamiento, el gradiente del error respecto a estados lejanos se propaga como\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial h_t}=\\frac{\\partial \\mathcal{L}}{\\partial h_T}\\prod_{k=t+1}^{T}\\frac{\\partial h_k}{\\partial h_{k-1}}$$\n",
    "\n",
    "y, aproximadamente,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_k}{\\partial h_{k-1}} \\approx W_h\n",
    "$$\n",
    "\n",
    "Si los valores propios de $W_h$ son menores que 1, el producto tiende a cero:\n",
    "\n",
    "$$\n",
    "\\prod_{k=t}^{T} W_h \\rightarrow 0\n",
    "$$\n",
    "\n",
    "Este fenómeno se conoce como **desvanecimiento del gradiente** (*vanishing gradient*): la información lejana se pierde.\n",
    "\n",
    "Narrativamente: el experto comienza a olvidar los primeros eventos conforme la historia se alarga.\n",
    "\n",
    "### El experto premium: LSTM y GRU\n",
    "\n",
    "Los expertos de alto nivel no confían en memoria volátil, pues toman notas inteligentes.\n",
    "\n",
    "### LSTM: memoria controlada\n",
    "\n",
    "**El meticuloso.** La arquitectura LSTM introduce un **estado de memoria** $c_t$, además del estado oculto $h_t$.\n",
    "\n",
    "Las ecuaciones completas son:\n",
    "\n",
    "$$f_t = \\sigma(W_f [h_{t-1}, x_t])\\mbox{ forget gate }$$ $$i_t = \\sigma(W_i [h_{t-1}, x_t])\\mbox{ input gate }$$ $$\\tilde{c}_t = \\tanh(W_c [h_{t-1}, x_t])\\mbox{ nuevo contenido}$$ $$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$ $$o_t = \\sigma(W_o [h_{t-1}, x_t])\\mbox{ output gate}$$ $$h_t = o_t \\odot \\tanh(c_t)$$ \n",
    "\n",
    "Donde  \n",
    "- Forget: “esto fue drama pasajero, bórralo”  \n",
    "- Input: “esto sí importa, apúntalo”  \n",
    "- Output: “usa esta parte del recuerdo para opinar ahora”\n",
    "\n",
    "El cell state fluye casi intacto → memoria de largo plazo.\n",
    "\n",
    "### GRU: memoria simplificada\n",
    "\n",
    "**El premium eficiente.** La GRU fusiona estado y memoria en una sola variable:\n",
    "\n",
    "$$z_t = \\sigma(W_z [h_{t-1}, x_t])\\mbox{ update}$$\n",
    "\n",
    "$$r_t = \\sigma(W_r [h_{t-1}, x_t])\\mbox{ reset}$$\n",
    "\n",
    "$$\\tilde{h}_t = \\tanh(W_h [r_t \\odot h_{t-1}, x_t])$$\n",
    "\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "Decide cuánto olvidar y cuánto actualizar, sin separar explícitamente estado y memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce141f8-51c5-4778-8d0a-95f8c87439be",
   "metadata": {},
   "source": [
    "## Si el experto es muy experimentado: capas múltiples (Deep RNN)\n",
    "\n",
    "El experto no se conforma con una sola lectura. Cada capa recurrente es un día extra de reflexión.\n",
    "\n",
    "- **Capa 1:** interpreta eventos crudos  \n",
    "- **Capa 2:** interpreta interpretaciones  \n",
    "- **Capa 3:** detecta patrones psicológicos profundos\n",
    "\n",
    "Formalmente, en la capa $\\ell$\n",
    "\n",
    "$$h_t^{(\\ell)} = RNN^{(\\ell)} (h_t^{(\\ell-1)}, h_{t-1}^{(\\ell)})$$\n",
    "\n",
    "Las capas inferiores capturan dependencias temporales locales; las superiores, patrones temporales más abstractos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c30d7-6977-48d0-b607-bf914c42837e",
   "metadata": {},
   "source": [
    "## Salida de la capa \\(i+1\\) en una red recurrente\n",
    "\n",
    "Supongamos una red neuronal recurrente con **capas apiladas**, de manera análoga a una CNN.\n",
    "\n",
    "### Entrada a la capa \\(i+1\\)\n",
    "\n",
    "Supongamos que la capa $i$ produce como salida una **secuencia de tamaño $T$ de vectores** $$\n",
    "A_i = (a_{i,1}, a_{i,2}, \\dots, a_{i,T}),\n",
    "\\qquad a_{i,t} \\in \\mathbb{R}^{m_i}\n",
    "$$\n",
    "\n",
    "Esta secuencia representa la historia ya procesada por las capas anteriores. Diremos que la capa $i+1$ es **recurrente** si existe un conjunto de parámetros\n",
    "\n",
    "$$\n",
    "\\Theta_{i+1} = (W_x^{(i+1)}, W_h^{(i+1)}, b^{(i+1)})\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $W_x^{(i+1)} \\in \\mathbb{R}^{m_{i+1} \\times m_i}$\n",
    "- $W_h^{(i+1)} \\in \\mathbb{R}^{m_{i+1} \\times m_{i+1}}$\n",
    "- $b^{(i+1)} \\in \\mathbb{R}^{m_{i+1}}$\n",
    "\n",
    "Estos parámetros se **comparten en el tiempo**.\n",
    "\n",
    "Para cada instante $t = 1,\\dots,T$, la capa $i+1$ produce un estado\n",
    "\n",
    "$$h_{i+1,t}=\n",
    "\\phi\\!\\left(\n",
    "W_x^{(i+1)} a_{i,t}\n",
    "+\n",
    "W_h^{(i+1)} h_{i+1,t-1}\n",
    "+\n",
    "b^{(i+1)}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "con $h_{i+1,0}$ inicializado (típicamente en cero).\n",
    "\n",
    "Al finalizar el recorrido temporal, la salida natural de la capa $i+1$ es la secuencia\n",
    "\n",
    "$$A_{i+1}=(h_{i+1,1}, h_{i+1,2}, \\dots, h_{i+1,T})$$\n",
    "\n",
    "donde cada elemento vive en $\\mathbb{R}^{m_{i+1}}$.\n",
    "\n",
    "La salida de la capa $i+1$ puede organizarse como un tensor\n",
    "\n",
    "$$\n",
    "A_{i+1} \\in \\mathbb{R}^{T \\times m_{i+1}}\n",
    "$$\n",
    "\n",
    "Este es el análogo estructural del edificio en CNN:\n",
    "\n",
    "- en CNN: base espacial $\\times$ altura de filtros  \n",
    "- en RNN: eje temporal $\\times$ dimensión del estado oculto  \n",
    "\n",
    "## Comparación capa a capa: CNN vs RNN\n",
    "\n",
    "### CNN\n",
    "\n",
    "La capa $i$ produce un edificio\n",
    "\n",
    "$$\n",
    "A_i \\in \\mathbb{R}^{d_1 \\times d_2 \\times m_i}\n",
    "$$\n",
    "\n",
    "La capa $i+1$, con $m_{i+1}$ filtros, produce\n",
    "\n",
    "$$\n",
    "A_{i+1} \\in \\mathbb{R}^{d_1' \\times d_2' \\times m_{i+1}}\n",
    "$$\n",
    "\n",
    "### RNN\n",
    "\n",
    "La capa $i$ produce una historia\n",
    "\n",
    "$$\n",
    "A_i = (a_{i,1}, \\dots, a_{i,T}),\n",
    "\\qquad a_{i,t} \\in \\mathbb{R}^{m_i}\n",
    "$$\n",
    "\n",
    "La capa $i+1$ produce **otra historia de la misma longitud temporal**\n",
    "\n",
    "$$A_{i+1}=(h_{i+1,1}, \\dots, h_{i+1,T}),\\qquad h_{i+1,t} \\in \\mathbb{R}^{m_{i+1}}\n",
    "$$\n",
    "\n",
    "\n",
    "## Punto clave\n",
    "\n",
    "**Una capa recurrente no cambia la longitud temporal de la historia.**  Cambia la representación interna en cada instante.\n",
    "\n",
    "Así como en CNN la altura del edificio cambia con el número de filtros, en RNN la dimensión del estado oculto $m_{i+1}$ cambia de capa a capa,  \n",
    "mientras que el tiempo $T$ se conserva. Y así como cada capa convolucional transforma un edificio en otro edificio más abstracto, cada capa recurrente transforma **una historia** en otra historia de la misma duración, pero con **estados internos de mayor nivel de abstracción** en cada instante."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
